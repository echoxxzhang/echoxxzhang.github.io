<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬虫Day6-Beautiful介绍]]></title>
    <url>%2F2019%2F07%2F24%2F%E7%88%AC%E8%99%ABDay6-BeautifulSoup%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[之前的urllib实战中我们谈到了BeautifulSoup这个解析库，那么…是不是应该来一篇详细介绍呢？ 今天，他来了！！ BeautifulSoup的基本使用：拿到网页数据：一般来说，我们使用解析库的前提，已经是通过请求库拿到了网页的数据，假设现在由下图的 html 变量存储的数据正是我们请求的文本内容，下面我们将使用BeautifulSoup进行加工解析。 获取Soup对象：使用BeautifulSoup方法，第一个参数传入 HTML字符串文本，第二个参数传入的是解析器的类型，在这里我们使用lxml。不指定会使用默认的解析器，但是会发出警告 123from bs4 import Beautifulsoupsoup = BeautifulSoup(html,"lxml")print(soup) 运行后，发现内容如下（发现不完整的HTML字符串被补齐了）： 属性选择调用 soup 的各个方法和属性（选择器）对这串HTML代码解析。这个soup对象支持 “.” 获取节点操作，也可以通过”.”表达嵌套关系。 我们直接拿到的soup打印不美观，使用prettify（）方法可以打印出树形结构的 html 文本 这里我们打印 title 的文本内容，以及 head 节点信息 上面就是BeautifulSoup的使用流程，有时候，我们需要的内容存放于某些节点中，这些节点有它的特征，我们可以通过选择器获取这些节点，最后拿到我们需要的结果。 节点选择器：选择元素： 元素选择—— 直接通过调用节点的名称就可以选择节点元素了，这种选择方式速度非常快，如果单个节点结构话层次非常清晰，可以选用这种方式来解析。 注意，我们的选择器是基于soup这个对象的 输出了soup.head的类型，是 bs4.element.Tag 类型.这是 BeautifulSoup 中的一个重要的数据结构，经过选择器选择之后，选择结果都是这种 Tag 类型，它具有着一些属性，比如 string 属性（输出文本信息），可以进行获取属性、嵌套等操作。 不过这种选择方式只会选择到第一个匹配的节点，其他的后面的节点都会忽略——比如我们打印的p节点，只呈现一个信息（html字符串中有三个p节点信息） 获取属性：每个节点可能有多个属性，比如id，class 等等，我们选择到这个节点元素之后，可以调用 attrs 获取所有属性。 可以看到 attrs 的返回结果是字典形式，把选择的节点的所有属性和属性值组合成一个字典。因此，可以通过键值形式进行取值传入节点的属性，获取该节点的属性值 contents 属性： 选取到了一个节点元素之后，如果想要获取它的直接子节点的信息 （不包含子孙节点），可以调用contents 属性（和直接选择不同的是，contents返回的列表包含了所有子节点和所有文本信息） 关联选择：我们在做选择的时候有时候不能做到一步就可以选择到想要的节点元素，有时候在选择的时候需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等等。这里列出了获取节点的各个方法： Children 查询某个节点的直接子节点（返回一个迭代器） Descendants 查询某个节点的所有子孙节点（返回生成器） parent 查询某个节点的元素的父节点 Parents 查询某个节点的元素的祖父节点（返回生成器） soup.a.next_sibling 查询某个节点的下一个节点 previous_sibling 查询某个节点的上一个节点 next_siblings 和 previous_siblings 分别返回所有前面和后面的兄弟节点的生成器。 需要注意的是：上面这些节点选择，但凡涉及到多个节点内容，就会返回一个生成器类型。 关于这些节点选择，我们要注意 html 树型结构的排列关系，例如，我们不能跨越父级节点，去拿到另一个子孙节点的内容。 方法选择器前面我们所讲的选择方法都是通过属性来选择元素的，这种选择方法非常快，但是如果要进行比较复杂的选择的话则会比较繁琐，不够灵活。所以 BeautifulSoup 还为我们提供了一些查询的方法，这里介绍非常常用的 find_all()、find() 等方法，我们可以调用方法然后传入相应等参数就可以灵活地进行查询。 find_all( )查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，并使用一个列表进行返回。 我们接下来针对它的 API 下的属性进行介绍 ①name：根据标签名选择节点 方法选择器会返回一个Tag标签的类型，也就是说，其支持嵌套获取元素、层层迭代使用find_all等等操作。 当name=True时，匹配所有标签。 ② attrs / id ： 可以通过这些attrs、id 这两个关键字对我们需要的节点传入进行匹配 ③recursive：是否对子孙全部检索，默认为True，改为False则忽略孙节点 ④ text：用来匹配节点的文本，文本参数 text是用标签的文本内容去匹配，而不是用标签的属性。传入的形式可以是文本字符串，也可以是正则表达式对象（compile） 也可以使用text = “Hello，this is a link” 这样的形式进行匹配节点。 find（）find() 方法返回的是单个元素，也就是第一个匹配的元素。类型依然是 Tag 类型。（当某个标签只有一个是使用find比较好，因为find返回一个Tag类型，而find_all返回一个列表，这是最大的区别了，有时候我们需要拿到Tag类型的节点，这样对我们的后续操作更为便利），跟find_all很接近，API都是相同. 获取文本经过节点选择，我们已经匹配到我们需要的节点了，接下来应该是获取文本内容了，通过Tag 类型的string 属性，我们可以拿到文本内容，有一个方法那就是 get_text（），同样可以获取文本值。 只需基于 Tag类型，调用 get_text（） 方法即可。不过需要注意的是： .get_text（）会把你正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如你正在处理一个包含许多超链接、段落和标签的大段源代码，那么 .get_text() 会把这些超链接、段落和标签都清除掉，只剩下一串不带标签的文字。 所以，get_text（）方法一般留到最后面使用，不然会影响我们获取节点 写在最后： 说一下节点选择的建议：页面布局总是不断变化的。一个标签这次是在表格中第一行的位置，没准儿哪天就在第二行或第三行了。如果想让你的爬虫更稳定，最好还是让标签的选择更加具体。如果有属性，就利用标签的属性。 解析库呢，就像一把锤子，使用的是我们，我们学会如何使用一把锤子的同时呢，也要关注什么时候去使用它，不要手上握着一把锤子，看任何问题都是钉子。使用适当的解析库去拿到我们需要的内容才是我们应该做的，使用什么样的锤子并不重要。 不经意间，爬虫专栏已经走到的Day6了，过阵子应该会更新 爬取网站的实战。还是那句话，学理论归理论，但只有实战才能使我们将其内化，成为我们真正的技能。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day5-requests介绍]]></title>
    <url>%2F2019%2F07%2F19%2F%E7%88%AC%E8%99%ABDay5-requests%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Requests的基本使用我们知道在 Urllib 库中有 urlopen() 的方法，实际上它是以 GET 方式请求了一个网页。那么在 Requests 中，相应的方法就是 get() 方法，相比较方便（使用什么请求方法就用对应的方法名封装的API） GET 请求下面是使用requests库发出的简单get请求，是不是相对于urllib，步骤变简单了很多呢？嘿嘿~ 响应内容的类型是type，我们可以基于这个对象，使用text方法获取文本，也可以调整编码格式等等。 响应状态码封装在status_code这个属性中，200表示正常访问。 模拟其他请求接下来，在这里分别用post()、put()、delete() 等方法实现了 POST、PUT、DELETE 等请求。requests这些方法时是直接使用的（不像urllib那么隐晦） 附加信息我们知道，使用百度这样的搜素引擎搜索关键字时，会在url生成类似这样的url，比如我们现在搜索的关键词是python，会生成下面这条url——“http://www.baidu.com/s?wd=python” 那么，在requests中，如何模拟这样的关键词查询呢？ 回答：将信息数据存储到字典中，然后利用params这个参数进行传递。 我们可以看到，字典的键值被传递到url中。 注意字典里值为None 的键都不会被添加到 URL 的查询字符串里。 定制请求头 注意：定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。 如果被重定向到别的主机，授权 header 就会被删除。 代理授权 header 会被 URL 中提供的代理身份覆盖掉。 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。 更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 二进制流数据上面例子中，实际上它返回的是一个 HTML 文档，那么如果我们想抓去图片、音频、视频等文件的话应该怎么办呢？我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，所以就需要拿到他们的二进制码。 需要记住的是：（用什么保存格式编码就要用对应的解析方式解码） 前两行便是 r.text 的结果，最后一行是 r.content 的结果。 前者出现了乱码，由于图片是二进制数据，所以前者在打印时转化为 str 类型（默认的编码格式），也就是图片直接转化为字符串，理所当然会出现乱码； 而后者结果前面带有一个 b，代表这是 bytes 类型的数据。 得出结论： r.test返回的是字符串类型，如果返回结果是文本文件，那么用这种方式直接获取其内容即可。 如果返回结果是图片、音频、视频等文件，应该使用r.content，Requests 会为我们自动解码成 bytes 类型，即获取字节流数据。此时，我们就可以通过写入文件（注意编码格式是wb），将其保存为照片格式。 关于编码：编码可谓是蛰伏在黑暗中的精灵，如果你不搞懂他，它就会偶尔出来耍一下脾气… Requests 会自动解码来自服务器的内容 。 大多数 unicode 字符集都能被无缝地解码 。 请求发出后 ， Requests 会基于 HTTP 头部对响应的编码作出有根据的推测 。 当你访问 r.text 时 ， Requests 会使用其推测的文本编码 。 不过你也可以找出 Requests 使 用 了 什 么 编 码 ， 并 且 能 够 使 用 r.encoding来更改编码方式。如果你改变了编码，每当你访问r.text，Request都会使用r.encoding的新值。 不过每一次设置r.encoding都需要去浏览器中查看相应字符集，这很麻烦。我们可以这样写： 1r.encoding = r.apprent_encoding # apprent_encoding是在响应报文的头部获取字符集，这样设置比较灵活 post 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典（也可以是个元组）给 data 参数。你的数据字典在发出请求时会自动编码为表单形式 是不是觉得好简单？哈哈~ 这就是requests向我们提供的简单接口，然后我们直接拿来调用就行了！ Request的高级用法文件上传我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传。使用files关键字传递文件，就可以达到我们想要的效果。 content-type的值是multipart/from-data，表示是以表单数据提交的，证明请求方法为post 发送到网页，需要以字节流形式传输，所以我们打开文件时必须指定为“wb” 最后使用files关键字指定传入文件，文件需以字典形式传入（键名是什么不重要） 设置Cookies在前面我们使用了 Urllib 处理过 Cookies，写法比较复杂，而有了 Requests，获取和设置Cookies 就比较方便快捷了。而且requests有两种设置方法，我们一起来看看吧~ ① 在请求报文头中设置 ② 通过cookies参数进行传递 代理设置对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会提出登录验证、验证码，甚至直接把IP给封禁掉。（我就曾使用selenium对淘宝网进行爬取，结果没爬几页，发现被淘宝反爬虫给识别出来了，这就很尴尬了…） 那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在 Requests 中需要用到 proxies 这个参数。 超时设置在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能会收到一个响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，我们应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错，或者谁用异常处理来处理这类事情。 设置超时时间需要用到 timeout 参数（默认是None）。这个时间的计算是发出 Request 到服务器返回Response 的时间。 上面就是requests的简单介绍了，是不是发现功能都被封装称为各个API中，我们只需要进行调用就好了，有没有像我当初一样，被requests这个库的魅力所感染到了呢？]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识扫盲篇]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2%E7%AF%87%2F</url>
    <content type="text"><![CDATA[计算机组成基础谈谈汇编语言：机器语言——“计算机运行的基础” 机器语言是机器指令的集合，机器指令展开来讲就是一台机器可以正确执行的命令。电子计算机的机器指令是一列二进制数字。计算机将之转变为一列高低电平，以使计算机的电子器件受到驱动，进行运算。 汇编语言的诞生： 汇编语言是直接在硬件上工作的编程语言，用来替代机器语言麻烦的编程问题（难以辨别和记忆）。 汇编语言的主题是汇编指令，汇编指令和机器指令的差别在于指令的表示方法上，汇编指令是机器指令便于记忆的书写格式 编译器： 但是计算机能读懂的只有机器指令，那么如何让计算机执行程序员用汇编语言编写的程序呢？ 这时，一种能够将汇编语言转成机器指令的翻译程序——编译器 产生了。 #### 汇编语言的组成： 汇编指令：有对应的机器码，决定了汇编语言的特性 伪指令：没有对应的机器码，由编译器执行，计算机并不执行 其他符号：如+、-、*、/ 等，由编译器识别，也没有对应的机器码 计算机组成的各个部分 存储器： CPU是计算机的核心部件，它控制整个计算机的运作，要想让CPU工作，就必须向它提供指令和数据。指令和数据在存储器中存放，也就是平时说的内存。 离开了内存，CPU也无法工作。 磁盘是计算机永久性存储数据的空间所在，磁盘上的数据要想被**CPU**执行调用，必须先读到内存当中。 指令和数据： 指令和数据是应用上的概念，在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。 CPU在工作时，把有的信息看做指令，有的信息看做数据。（有特殊意义的二进制信息视为指令） 存储单元： 存储器又被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127。 微型机存储器的每一个存储单元可以存储一个字节，即8个bit（电子计算机的最小信息单位）。 也就是说，一个存储器有128个存储单元，它可以存储128个字节。 微机存储器的容量是以字节为最小单位来计算的。 CPU对存储器的读写： 存储器被划分成多个存储单元，存储单元从零开始顺序编号，这些编号可以看作存储单元在存储器中的地址，就像每个房子的门牌号 CPU要从内容中读取数据，首先要指定存储单元的地址，也就是说它要先确定读取哪一个存储单元中的数据。 另外，在微机中，不当当只有存储器这一种器件，CPU在进行读写数据操作时，需要指明它要对哪一个器件进行操作、进行哪种操作、是从中读出数据，还是写进数据。 可见，CPU要想进行数据的读写，必须和外部器件（可以说成芯片）进行三类信息的交互： 存储单元的地址（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据） 那么CPU是通过什么介质将这些信息传到存储器芯片中呢？ 电子计算机能处理、传输的信息都是电信号，电信号当然要用导线传送。在计算机中，专门连接CPU和其他芯片的导线，称为总线。 从物理上来讲，总线是一根根导线的集合，根据传送信息的不同，又从逻辑上分为三类、即地址总线、控制总线和数据总线。 总线 地址总线： CPU是通过地址总线来指定存储器单元的。地址总线上能传送多少个不同的信息，CPU就可以对多少个存储单元进行寻址。 在电子计算机中，一根导线可以传送的稳定状态有两种，高电平或是低电平。用二进制表示就是1或者0。 有 n 根导线就能表示 2的n次方种信息。假设一个CPU有N跟地址线，则可以说这个CPU的地址总线的宽度为N，这样的CPU最多可以寻找2的N次方个内存单元。 数据总线： CPU与内存或其他器件之间的数据交互是通过数据总线进行传递的。 数据总线的宽度决定了CPU与外界的数据传输速度。8跟数据总线一次可传送一个8位二进制数据（即一个字节）。 控制总线： CPU对外部器件的控制是通过控制总线来进行的。 控制总线是一些不同控制线的集合，有多少根控制总线，就意味着CPU提供了对外部器件的多少种控制，所以，控制总线的宽度决定了CPU对外部器件的控制能力。 内存地址空间： 举例来说，一个CPU的地址宽度为10，那么可寻址1024个内存单元，这1024个可寻址的内存单元就构成了这个CPU的内存地址空间。CPU在操纵和控制各类存储器时，都把他们当作内存来对待，把它们总的看做一个由若干存储单元组成的逻辑存储器，这个逻辑存储器就是我们所说的内存地址空间。 主板： 每一台PC机中，都会有一个主板，主板上的器件通过总线相连。 这些器件有：CPU、内存、外围芯片组、扩展插槽等。扩展插槽一般插有RAM内存条和各类接口卡. 接口卡： 计算机系统中，所有可用程序控制其工作的设备，都归CPU控制。 CPU对于外部设备如显示器、音箱等，是通过控制这些设备直接相连的接口卡，从而实现间接的控制这些外部设备的。简单来说，CPU通过总线向接口卡发送命令，接口卡根据CPU的命令控制外设进行工作。 各类存储器芯片： 一台PC机中，装有多个存储器芯片。不同的器件，从读写属性上分为两大类： 随机存储器（RAM）和只读存储器（ROM） 随机存储器可读可写，但必须带电存储，关机后存储的内容丢失； 只读存储器只能读取不能写入，关机后内容也不会消失。 这些存储器从功能和连接上又可分为以下几类： 随机存储器：用于存放供CPU使用的绝大部分程序和数据，主随机存储器一般由两个位置上的RAM组成，装在主板上的RAM和插在插槽上的RAM 装有BIOS（基本输入输出系统）的ROM：BIOS是由主板和给某些接口卡（如：显卡、网卡等）厂商提供的软件系统。 接口卡上的RAM：某些接口卡需要大批量输入输出数据进行暂时存储，因此会在其上装有RAM。最为经典的是显示卡上的RAM，一般称为显存。我们将需要显示的内容写入显存，就会出现在显示器上。 **** 以上就是总结的部分计算机基础知识~]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程新手的必修课]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BC%96%E7%A8%8B%E6%96%B0%E6%89%8B%E7%9A%84%E5%BF%85%E4%BF%AE%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[论一名计算机专业学生的修养从事计算机行业，私以为，是最为公平且投入风险小的一件事。不像当今社会同样红火的金融行业，需要人脉与资金等铺垫。我们听过不少这样的话——“只要你们用心学，不要放弃，就一定能学会编程”。 刚开学，学习安排的课程，发几本计算机专业的教材，按时上课，老师在那念念PPT，你若认真听了你可能懂了但是发现不会写，你若没听玩手机了你是既不懂，也不会写，蒙在鼓里的人出不来，水平就一直这样龟速增长。 有的学生学的一头雾水开始怀疑自己准备转专业放弃计算机；有的学生学懂写程序一直报错开始怀疑自己；有的学生觉得教材写得不好去书店转了一圈，买几本语言的书但最后厚厚的灰坐盖在了书上，再也没翻过。 以上这几种学生，若没有领悟到计算机的真正奥妙，最终都要凉凉。 为什么会这样呢？ 是啊，我们有没有反省过，究竟是环境问题，还是自驱动力的不足？ 努力：计算机虽然是目前最火的专业，但若人人都想分一杯羹，不付出实际行动，一切都是空谈。 视野：视野决定高度，有句话说，站在风口上，猪都能飞来。 职业规划： 计算机虽然被统称为计算机专业，但其包含的方向数不胜数，方向选择是很重要的。拥有一份明确的职业规划，绝对让你披荆斩棘，一路高歌。 接下来我们围绕这些观点，展开描述： 投入努力：在计算机专业，有一点是不容质疑的，那就是—— “付出 = 实际回报”。有的同学，因为上课听不懂，就觉得计算机很难，那我更要打击一下他了。课堂所学，只是计算机最最基础的东西，就算你课堂上听懂了，那些知识也无法支撑你拿到满意的工作。你之所以觉得听不懂，你要清楚是因为天赋不行还是投入时间不足，这一点自己最为清楚。 这是一门全靠自学为主的学科，只要你用心付出，就会领先于其他人。 视野的打开网上资源一抓一大把，学会如何科学上网，等于坐拥了这些资源，对于计算机专业的人来说，更是一笔最为宝贵的财富，让遇到问题Google，而不是百度成为习惯，进入这个世界编程大社区，哪怕什么都不懂，你也能保证所在的社区，就是世界程序员的大家庭，当你走进Github，看着各种有趣的项目的时候，相信我，你的视野就会在此为起点，快速打开并不断增长，进入一个良性循环。 有的人会问，学会上网不是一名计算机专业的人的必修课吗？有必要再三强调吗？ 是的，这句话没错，但是有没有想过，身为当初萌新的你，对着网上庞大的资源，会不会感到手足无措呢？对着全英文文档的时候，会不会感到一丝恐惧？有多少学生，连stackoverflow都不知道是什么。 视野是很重要的一点，视野打不开，一身本领就会被扼杀在摇篮中 职业规划：现互联网处于蓬勃发展的阶段，更是等着我们这一代人去带动他，我能保证，互联网的红火还能至少维持二十年。且现在的红利虽不如几年前那么丰富，但现在也是不容小觑的。我们经常听到过，穷人家的孩子学计算机，富人家的孩子学金融就是这么来的。如果已经就读计算机专业，那么恭喜你，已经处于风口上，接下来就看你能不能把握住了。 这里引用一张知乎上k点赞的图片，里面所讲述的，描述了计算机行业中多个方向的选择，我们可以从中挑选出适合我们自己的。 到了大学，请不要再有“学生思维”，碰到问题时不要立马请教他人，解决问题的过程就是你进步的一种。而且这种解决问题的能力，更是会伴随我们整个职业发展。可以说，这种技能胜过于任何一个知识点，也是自学过程中必须掌握的。 上面这些，是我的这一年来的心得体会，希望能帮到需要的人。]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day4-urllib实战]]></title>
    <url>%2F2019%2F07%2F15%2F%E7%88%AC%E8%99%ABDay4-urllib%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[urllib爬虫实战之爬取笔趣阁小说一、前言上一节我们介绍了urllib这个基本库，它的内容还是蛮多的，要想熟练掌握爬虫，必须配合实战进行巩固。一开始小伙伴们可能会觉得无从下手，特别是解析响应内容这一步。这一次，作者将会从小白的角度，讲述如何对网页进行爬取、解析。 二、环境运行平台： WindowsPython版本： Python3.xIDE： Pycharm 三、BeautifulSoup简介 BeautifulSoup 就是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据。BeautifulSoup提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据。 解析器 BeautifulSoup 在解析的时候实际上是依赖于解析器的，它除了支持 Python 标准库中的 HTML 解析器，还支持一些第三方的解析器比如 LXML 。 Beautiful方法有两个参数，第一个参数传入待解析的文本，第二个参数指定解析器，如果我们想要使用 LXML 这个解析器，在初始化 BeautifulSoup 的时候指定 lxml 即可，如下： 好了，简单介绍了BeautifulSoup，我们快点进入今天的实战环节吧~ 四、实战 背景介绍：笔趣看是一个盗版小说网站，这里有很多起点中文网的小说，该网站小说的更新速度稍滞后于起点中文网正版小说的更新速度。并且该网站只支持在线浏览，不支持小说打包下载。因此，本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说。 PS：本实例仅为交流学习，支持耳根大大，请上起点中文网订阅。 URL: ‘https://www.biqukan.com/1_1094/5403177.html&#39; 预备知识： 本文提及的代码并不复杂，如果对BeautifulSoup想要更加了解，可以去官方文档进行翻阅 URL：‘http://beautifulsoup.readthedocs.io/zh_CN/latest/‘ BeautifulSoup环境搭建： pip install bs4 导入使用 – from bs4 import BeautifulSoup 小说内容爬取获取请求① 打开url链接，按F12或者右键- 检查 进入开发者工具 ② 在开发者工具中，捕获我们要找到的请求条目信息 将正文中部分内容进行复制 在开发者工具中，使用 ctrl + f 进入全局搜索框，将复制内容进行粘贴 然后会在下方得到条目信息，点击，页面会跳转到加载正文的请求响应条目中。 分析发现，正文部分存储在 id 为 content 和 class 为 showtxt 的 div 中。这就是我们解析网页得到的重要信息啦 ③ 构造url请求 有了上面的信息还不够，现在的网站都有了反爬能力，我们需要模拟一条正常从浏览器中发出的url请求链接。 例如： User-Agent（浏览器标识）、Cookies（标识客户端身份的“钥匙”）、referer等等字段 那么，这些包含在请求头的字段信息，我们应该如何构造呢? 还是开发者工具，点击Headers，就可以看到Request-Response条目明细。 ④ 发出请求： 有了字段的详细内容，我们就可以编写出请求网页的代码… 12345678import urllib.requestdef get_page(url): headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0&apos;,&apos;Cookie&apos;:&apos;UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333&apos;&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() print(html) return html ⑤ 获得相应内容：运行，得到内容如下: 发现是一个二进制流的数据，因此得知，我们在解析时，需要编码 解析响应数据接下来，我们就可以使用BeautifulSoup进行解析 12345678910def parser_page(html): soup = BeautifulSoup(html,&apos;lxml&apos;,from_encoding=&apos;utf-8&apos;) # 传入编码方式，为utf-8 text = soup.find(&apos;div&apos;, class_=&apos;showtxt&apos;) # 使用BeautifulSoup的find方法，匹配符合的节点，该节点为我们上文得知的属性 cont = text.get_text() # 输出文本内容 cont = str(cont).split() # 将空白符进行分割，这里的作用是消除 \xa0、\u3000空白符 print(cont) 运行….代码结果如图： 永久存储数据：我们这里使用最简单的方式，存储写入本地文件 打开result.txt文件，内容如下（完成！） 写在最后本次我们实战项目就告一段落啦，最后再为大家做一次梳理总结。 观察网页信息，获取相应内容 构造get请求，拿到响应内容 用解析库对相应内容进行解析 写入文件 全文代码如下： 12345678910111213141516171819202122232425262728import urllib.requestfrom bs4 import BeautifulSoupurl = &apos;https://www.biqukan.com/1_1094/5403177.html&apos;def get_page(url): headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0&apos;,&apos;Cookie&apos;:&apos;UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333&apos;&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() return htmldef parser_page(html): soup = BeautifulSoup(html,&apos;lxml&apos;,from_encoding=&apos;utf-8&apos;) text = soup.find(&apos;div&apos;, class_=&apos;showtxt&apos;) cont = text.get_text() cont = str(cont).split() return contif __name__ == &apos;__main__&apos;: html = get_page(url) results = parser_page(html) with open(&apos;result.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f : for result in results: f.write(result) f.write(&apos;\n&apos;) print(&quot;爬取完毕&quot;) 耗费一上午时间，终于把这篇文章给赶出来了~ 爬虫Day系列会持续更新，让我们共同期待吧！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day3-urllib介绍]]></title>
    <url>%2F2019%2F07%2F10%2F%E7%88%AC%E8%99%ABDay3-urllib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Urllib介绍在 Python2 版本中，有 Urllib 和 Urlib2 两个库可以用来实现Request的发送。而在 Python3 中，已经不存在Urllib2 这个库了，统一为 Urllib urllib分为四个模块： 第一个模块是request，它是最基本的HTTP请求模块，我们可以用它模拟发送请求，就像在浏览器手动输入网址然后敲击回车一样，只需要给库方法传入URL，以及额外的参数，就可以模拟实现整个过程了。 第二个模块是error，即异常处理，如果出现请求错误，我们可以捕获这个异常，然后进行异常的处理，保证程序的稳定性 第三个模块是paser，它是一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等操作 第四个模块是robotparser，主要是用来识别网站的robots.txt的（反爬虫的文本说明） 下面作者就对以上模块进行重点讲解~ 一、request模块1.urlopen方法： 我们知道，request是HTTP请求模块，那么具体的请求方法是什么呢？没错，正是urlopen！ 12urlopen的API：urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 我们通过urllib.request.urlopen（“url链接”）就可以请求到网页，如下图所示 这个请求的返回结果response，就是服务器响应内容。紧接着，我们可以调用这个对象包含的方法对返回的内容进行处理。 响应的内容是HTML纯文本，它以字节流的形式返回的，于是要解码成utf-8，得到字符串 response.read（）读取响应内容到控制台 respon.status返回网页的状态码，如果网页的状态码是200，则表示正常 利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取。 2.data参数 data 参数是可选的，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。我们就可以使传递data参数，完成模拟登陆等类似功能… 具体如何模拟from表单，后文的urllib.parser会讲解 3. timeout参数 timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。 我们可以通过设置timeout值，从而缩短超时时间。或者是让程序不会一直处于阻塞状态（我们知道，客户端和服务器的交互是一个同步过程，我们可以利用timeout参数，以及异常处理来完成非阻塞的效果，以及让程序不会异常终止，让程序更加稳定） 4.构建Request对象 我们知道利用 urlopen()方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。比如可以构建请求链接的浏览器标识等等 下面是正常的urlopen请求和使用Request构建URL的代码举例： 可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，这样的好处是： 一方面我们可以将请求独立成一个对象 另一方面可配置参数更加丰富和灵活（将参数全部加到Ruquest对象中作为一个整体发送） Request配置举例 之前我们说到过，可以构建url链接，比如关键字查询、模拟登陆。还可以模拟浏览器标识，让爬虫更像是一个人为发出的请求信息。具体内容如下： headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。 data就是我们发出的数据了，他会以表单形式向服务器发出 method是方法的意思，我们可以指定method来声明这是一个什么类型的URL 二、Error模块前文我们了解了 Request 的发送过程，但是在网络情况不好、被反爬的情况下，出现了异常怎么办呢？这时如果我们不处理这些异常，程序很可能报错而终止运行，所以异常处理还是十分有必要的。 Urllib 的 error 模块定义了由request 模块产生的异常。如果出现了问题，request 模块便会抛出error 模块中定义的异常 1.URLError URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。它具有一个属性 reason，即返回错误的原因。 2.HTTPError 它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。 它有三个属性。 code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。 reason，同父类URLError一样，返回错误的原因。(有时候 reason 属性返回的不一定是字符串，也可能是一个对象—可以用 isinstance() 方法来判断它的类型，做出更详细的异常判断。) headers，返回 Request Headers。 因为 URLError是HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，这样捕获的代码更加精准，且有利于程序猿处理。 请求到响应信息不足为奇，也不是什么难事，最主要的，是让程序可以正常的”跑起来”，所以，异常处理是爬虫工程师的必修课，需好好掌握。 三、parse模块1.urlparse（） urlparse() 方法可以实现 URL 的识别和分段,返回六个部分的内容，字段分别是 scheme、netloc、path、params、query、fragment。 2.urlencode（） 常用的 urlencode() 方法，它在构造 GET 请求参数的时候非常有用——字典转序列化 3.parse_qs（） 有了序列化必然就有反序列化，如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典 quote（） quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码 5.unquote（） quote() 方法的逆操作，它可以进行 URL 解码（默认是以utf-8编码） 四、 robotparser在介绍rebotparser模块之前呢，我们需要对robots协议进行了解 Robots 协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。 robotparse：用来解析rebots文件的类 robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。 123接口API：import urllib.robotparserurllib.robotparser.RobotFileParser(url='') 使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。当然也可以声明时不传入，默认为空，再使用 set_url() 方法设置一下也可以。 RobotFileParser常用方法解析 read()，读取 robots.txt 文件并进行分析，注意这个函数是执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法，这个方法不会返回任何内容，但是执行了读取操作。（注意是“读取”而不是读出） parse()，用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。 can_fetch()，方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。（判断是否能抓取的方法） 好了，对urllib库的介绍就到这儿了，下次作者会对第三方请求库requests讲解，这是一个当前的流行库，让我们一起期待吧~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day2_爬虫基础]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%88%AC%E8%99%ABDay2-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[爬虫基础：简单来说，爬虫就是请求网页并提取数据和保存信息的自动化程序。 人们常常把爬虫比做蜘蛛爬网，把网的节点比做一个个网页，爬虫爬到这就相当于访问了该页面获取了其信息，节点间的连线可以比做网页与网页之间的链接关系，这样蜘蛛通过一个节点后可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，这样网站的数据就可以被抓取下来了。 简单流程介绍： 下面我们说说爬虫的具体流程，一般分为四步： 发起请求， 通过请求库对目标站点发起请求，我们可以模拟携带浏览器信息、cookie值、关键字查询等信息，从而访问服务器端，然后等待服务器的响应 解析内容： 服务器响应的内容有可能是HTML纯文本，也有可能是json格式的数据、或者是二进制（如图片视频），我们可以利用相关的解析库进行解析、保存或进一步处理。 处理信息：一般来说，我们使用解析库解析出来的内容，是冗杂晦涩的，我们通过用format或者切片等技术，使得内容清晰明了，这时，才算得上的有用的信息。 接下来，我们可以将信息存储到本地、数据库或者远程服务器等等。 爬虫优化：爬虫优化的方面有很多，例如，控制爬虫速率、使用高匿代理服务器、使用分布式爬取、处理异常… 接下来作者针对爬虫最重要的部分做出详细介绍： 一、 获取网页爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。 在前面我们讲到了 Request 和 Response 的概念，我们向网站的服务器发送一个 Request，返回的 Response 的 Body 便是网页源代码。 所以最关键的部分就是构造一个 Request 并发送给服务器，然后接收到 Response 并将其解析出来。具体相关的库有：urllib（python自带的库）、requests（优雅的第三方库） 二、提取信息我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。 另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。 能抓取怎样的数据： 常规网页， 也就是html源代码。有些网页返回的不是不是 HTML 代码，而是返回一个 Json 字符串（使用Ajax渲染的网页），API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。 二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。 我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。 JavaScript 渲染页面：有时候我们在用 Urllib 或 Requests 抓取网页时，得到的源代码实际和浏览器中看到的是不一样的。 这个问题是一个非常常见的问题，现在网页越来越多地采用 Ajax、前端模块化工具来构建网页，整个网页可能都是由 JavaScript 渲染出来的，意思就是说原始的 HTML 代码就是一个空壳。 下面，是JS常规的渲染代码： body 节点里面只有一个 id 为 container 的节点，但是注意到在 body 节点后引入了一个 app.js，这个便负责了整个网站的渲染。 在浏览器打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中里面引入了一个 app.js 文件，然后浏览器便会接着去请求这个文件，获取到该文件之后便会执行其中的 JavaScript 代码，而 JavaScript 则会改变 HTML 中的节点，向内添加内容，最后得到完整的页面。 但是在用 Urllib 或 Requests 等库来请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中看到的内容了。 这也解释了为什么有时我们得到的源代码和浏览器中看到的是不一样的。 所以使用基本 HTTP 请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台的Ajax接口，通过模拟Ajax请求得到渲染的页面，也可以使用Selenium这样的库来实现模拟JavaScript。 本期对爬虫基础原理的讲述就到这里了~ 下期预告：“urllib请求库的使用”]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day1-网页基础]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%88%AC%E8%99%ABDay1-%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[网页基础一、网页交互过程： URL：统一资源定位符，通俗的讲，它是一个链接，它能使我们从互联网上找到我们访问的资源。 例如： http://www.4399.com/flash/58508.htm 这样的一个网站 它包含三个部分： 访问协议 http 访问路径 “www.4399.com” 资源名称 “flash/58508” 我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。实际上这就是和服务器做交互行为， 通过发送请求，再到服务器响应服务，这就完成了我们获取资源的目的。 二、客户端请求–Request： Request，即请求，由客户端向服务端发出。可以将Request分为四部分： 请求方法（Request Method） 请求链接（Request URL） 请求头（Request Headers） 请求体（Request Body） 接下来，我们就分别来谈谈这四个部分的具体内容… Request Method 请求方式，请求方式常见的有两种类型，GET 和 POST。 Get： 我们在浏览器中直接输入一个URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里（GET 方式请求提交的数据最多只有 1024 字节） Post： Post 请求大多为表单提交发起，如一个登录表单，输入用户名密码，点击登录按钮，这通常会发起一个 POST 请求，其数据通常以 Form Data 即表单的形式传输，不会体现在 URL 中。（POST 方式请求提交的数据没有限制） 而且Post请求在Request Headers中的Content-Type标识——只有设置为application/x-www-form-urlencoded 才是Post请求。 其他请求方式 我们平常遇到的绝大部分请求都是GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下： Request URL： 顾名思义，就是请求的链接网址，我们发起的请求，必须包含URL Request Headers： 请求头，用来说明客户端要使用的附加信息，比较重要的信息有：Cookie，Referer、User-Agent 以及Content-Type等等。 Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。 Request Body： 请求体，一般承载的内容是Post请求的Form Data，即表单数据，而对于Get请求中，Request Body的内容为空。 三、服务器响应–ResponseResponse，即响应，由服务端返回给客户端。 Response 可以划分为三部分，Response Status Code、Response Headers、Response Body。 Response Status Code： 响应状态码，此状态码表示了服务器的响应状态，如 200则代表服务器正常响应，404 则代表页面未找到，500则代表服务器内部发生错误。 Response Headers： 响应头，其中包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等，下面对一些常用的头信息说明： Date：日期，标识 Response 产生的时间。 Last-Modified，指定资源的最后修改时间。 Content-Encoding，指定 Response 内容的编码。 Server，包含了服务器的信息，名称，版本号等。 Content-Type，文档类型，指定了返回的数据类型是什么，如text/html 则代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回了图片。 Set-Cookie，设置Cookie，Response Headers 中的 Set-Cookie即告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。 Response Body： 响应体，响应的正文数据都存放于此，如果请求一个网页，它的响应体就是网页的HTML代码，或者是Json格式的数据；请求一张图片，它的响应体就是图片的二进制数据。 爬虫的解析都是根据响应体进行操作的。 好了，以上就是简单的讲解了爬虫相关的网页基础内容，下期作者会讲解爬虫的原理基础，以及实现过程。Bye~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
