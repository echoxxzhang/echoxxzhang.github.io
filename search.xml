<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[吐槽下环境部署]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%90%90%E6%A7%BD%E4%B8%8B%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言今天，用了大半天的时间，部署了爬虫环境。 历经千辛万苦，终于全部搞定了。现在是晚上十一点，有些感概，对于这么一套流程，有一些思考，于是有了写下来的冲动。 鬼知道我经历了什么不夸大的说，今天遇到的BUG，已经超出了十几个，这么debug过来，一个又一个新的问题又爆出来。有时候，并不是那么顺风顺水。 当心情开始暴躁的时候，不妨将手头的事情放一放（进度不是很紧的话） 其次，要相信，在计算机中没有解决不了的问题。 在每一次遇到bug时，我都不会慌张，因为我知道，这是必然的、肯定会出现的一种现象，于是我面对bug的时候会很自信，因为最终它都会被我消灭掉。 真正的难题，是你如何去debug。 处理BUG的流程这套流程不一定适用于任何场景，但是作用于解决问题的参考还是行得通的。 出现报错一个BUG的出现往往是由ERROR这个字眼开始的，这时候，第一步要做的就是将它提示的内容，翻译为自己能理解的意思，再配合你当前所做的事情，联想一下，思考它为什么会报错。 一个谦卑的程序员总是先设想自己的代码是错的，排除之后再找其他方面的问题 谷歌、stackoverflow还有一点要相信的是，你遇到的BUG绝对不是历史第一次，什么意思呢？ 我们遇到的BUG，前辈们已经踏过无数次，我们可以利用互联网这个大资源，将自己遇到的东西，提取出关键字进行谷歌搜索，或者，直接将报错的代码直接复制粘贴。这是最简单粗暴的办法，但是没有前者精准。 所以，学会如何提问也是一门技术。 记录debug将问题解决了之后，如果你还是不能理解为什么，我们就可以将面临的问题、如何解决的，记下来。做好笔记，这是一个好的习惯。有很大几率，你会在某一段时间再次碰到你之前遇到的BUG，这时候，翻翻笔记，比盲目的谷歌好用多了，同时也促进了你对问题的理解。 为自己欢呼最终的最终，我们把BUG踩在脚下，这时候，请给自己鼓一下掌。因为这是不容易的，有句话是这么说的：“击不倒我的，必然使我更加强大” 既然选择了当程序猿，就不可能不遇到BUG，我们要学会坦然处之。这些debug的经验，都是你身为一名计算机人士的财富，你应该感谢一路走来的BUG，他成就了明天更强大的你。 写在最后今天被Docker 环境部署折腾的够呛，其中包含了 VirtualBox的配置、git 版本的更迭设置、 DockerToolbox 的配置 等等，其中遇到的难题之前都没接触过，历时半天，我终于将环境部署完成。等到看到内容完全输出到 git bash 的时候，真的挺开心。 最后，我想说的是，计算机是最公平的，对就是对，错就是错。只有耐心与它沟通，才能解决问题。 于 2019.8.02 晚 23.51 的深夜吐槽~]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>随便聊聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day8-Selenium魔法师]]></title>
    <url>%2F2019%2F07%2F31%2F%E7%88%AC%E8%99%ABDay8-Selenium%E9%AD%94%E6%B3%95%E5%B8%88%2F</url>
    <content type="text"><![CDATA[前言久违的爬虫Day 系列！！ 自从计算机网络系列更新完之后，我又把目光投到爬虫系列来了hhh… 今天介绍的技术可厉害了，在外人眼中，这近乎是“魔法”，那就是——selenium 同时，这也是我目前最为喜欢的爬取工具，让我们往下看看吧~ Selenium 是一个自动化测试工具，利用它我们可以驱动浏览器执行特定的动作，如点击、下拉等等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。用 Selenium 来驱动浏览器加载网页的话，我们就可以直接拿到 JavaScript 渲染的结果了，不管是什么加密统统不用再需要担心。 环境准备： selenium库 python3、Pycharm Chrome浏览器 ChromeDriver 以上内容，请自行谷歌、百度安装 selenium介绍：模拟登陆：声明浏览器对象：selenium 驱动浏览器进行操作的，于是我们要实例化一个浏览器对象，有以下： 接下来我们要做的就是调用 生成的 browser 对象，让其执行各个动作，就可以模拟浏览器操作了。 访问页面：基于browser对象， 我们可以用 get() 方法来请求一个网页，参数传入链接 URL 即可（get方法没有返回值，他只是模拟了浏览器的运行） 123456from selenium import webdriverbrowser = webdriver.Chrome() # 实例化一个浏览器对象，后续操作都是针对此对象进行操作的browser.get('http://www.baidu.com') # 模拟登陆网页print(browser.page_source) # page_source（）方法输出了页面的源代码browser.close() # 使用close（）对浏览器进行关闭。 让我们看看效果： 有没有看到嗖的一下子，就访问了百度这个网站，然后解析出了HTML。 注意： 这里我们拿到了一个 browser 对象，这个在后续操作中，是最关键的（节点选择，动作交互等等，都是基于 browser对象进行操作） 查找节点Selenium 可以驱动浏览器完成各种操作，比如填充表单、模拟点击等等，比如我们想要完成向某个输入框输入文字的操作，在这这前我们需要得知这个输入框的位置。 所以 Selenium 提供了一系列查找节点的方法，我们可以用这些方法来获取想要的节点，以便于下一步执行一些动作或者提取信息。 我们想要驱动浏览器完成自动搜索的任务，首先要先找到文本输入框。图中&lt;input&gt;节点就是我们要找的目标，熟悉前端的胖友，肯定倍感亲切。 可以发现它的 ID 是 q，Name 也是 q，还有许多其他属性，那我们获取它的方式就有多种形式了，第一种是通过节点名称查找，比如find_element_by_name() 是根据 Name 值获取，find_element_by_id()是根据 ID 获取，另外还有根据XPath、CSS Selector 等获取的方式。 以下代码可以查找出文本框的节点： 123456789101112131415from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get("https://www.taobao.com")input_first = browser.find_element_by_id('q') # 通过节点名称指定input_second = browser.find_element_by_css_selector('#q') # css选择器input_third = browser.find_element_by_xpath('//*[@id="q"]') # 使用xpath路径list = [input_first,input_second,input_third]for i in list: print(i)time.sleep(3)browser.close() 效果演示： 进入到淘宝页面中，通过selenium的节点选择，使我们的光标直接移动到了文本框中。 可以看到三个节点都是 WebElement 类型，是完全一致的。证明三种方法选择效果是一样的。 其他获取节点的方法： 这些API 都通过节点名称表明了它的作用。 我们就可以利用这些API，获取我们需要的节点了。 节点交互Selenium 可以驱动浏览器来执行一些操作（文本输入和提交），也就是说我们可以让浏览器模拟执行一些动作，比较常见的用法有：输入文字用 send_keys() 方法，清空文字用 clear() 方法，另外还有按钮点击，用 click() 方法。 12345678910111213from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone') # 输入文字time.sleep(1)input.clear() # 清除文字input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search') # 获取搜索框的节点属性button.click() # 点击，在这里是提交搜索内容的意思 节点交互的方法基于WebElement对象 我们发现，每个搜素框都有对应的属性名、id值，我们需先利用find_element_by_id（） 函数找到他（返回一个WebElement对象），然后传递交互方法。这样就实现了模拟浏览器的效果 使用send_keys 方法输入文字（这个方法挺常用的，例如selenium模拟登陆，我们可以输入用户名密码…） 其他交互动作也是由各个API封装好了，具体观看官方文档 : https://selenium-python-zh.readthedocs.io/en/latest/ 动作链在上面的实例中，一些交互动作都是针对某个节点执行的，比如输入框我们就调用它的输入文字和清空文字方法，按钮就调用它的点击方法，其实还有另外的一些操作它是没有特定的执行对象的，比如鼠标拖拽、键盘按键等操作。所以这些动作我们有另一种方式来执行，那就是动作链。 我们下面使用一个网站来测试 selenium 的动作链 123456789101112131415from selenium.webdriver import ActionChains # 动作链使用的类browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')# selenium提供了switch_to.frame()方法来切换frame,括号内是传入的参数，# 用来定位frame，可以传入id、name、index以及selenium的WebElement对象source = browser.find_element_by_css_selector('#draggable') # 选择节点target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser) # 实例化动作链对象actions.drag_and_drop(source, target) # 基于此对象，进行拖动并放入的操作actions.perform() # 必须的调用，执行动作 效果演示： 搜的一下子，selenium 就出色的完成了拖动释放的操作，是不是很棒！ 执行JavaScript语句对于某些操作，SeleniumAPI 是没有提供的，如下拉进度条等，可以直接模拟运行 JavaScript，使用 execute_script() 方法即可实现。 123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To Bottom")') 效果演示： 获取节点信息获取源代码 page_source 属性可以获取网页的源代码，获取源代码之后就可以使用解析库如正则、BeautifulSoup、PyQuery 等来提取信息了。不过既然 Selenium 已经提供了选择节点的方法，返回的是WebElement 类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等等。这样的话我们就可以不用通过解析源代码来提取信息了。 获取属性使用find_element获取节点，基于WebElement对象 ，使用 get_attribute() 方法，传入想要获取的属性名，就可以得到它对应的值了。 获取文本值原理同上，每个 WebEelement 节点都有 text 属性，我们可以通过直接调用这个属性就可以得到节点内部的文本信息了，就相当于 BeautifulSoup 的 get_text() 方法、PyQuery 的 text() 方法。 获取ID、相对位置、标签名、大小等信息 WebElement 节点还有一些其他的属性，比如 id 属性可以获取节点 id，location 可以获取该节点在页面中的相对位置，tag_name 可以获取标签名称，size 可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。 设置延时等待 动态加载的页面需要时间等待页面上的所有元素都渲染完成，如果在没有渲染完成之前我们就switch_to_或者是find_elements_by_，那么就可能出现元素定位困难而且会产生错误信息 在 Selenium 中，get() 方法会在网页框架加载结束之后就结束执行，此时如果获取 page_source ，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的 Ajax 请求（Ajax发出请求和服务器响应网页渲染需要一定时间），我们在网页源代码中也不一定能成功获取到。 所以这里我们需要延时等待一定时间确保节点已经加载出来。在这里等待的方式有两种，一种隐式等待，一种显式等待。 隐式等待当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是 0（固定的等待，如果已经加载出来，那么立即返回），如果超出设定时间后则抛出找不到节点的异常 显示等待实际情况中，显示等待用的比较多，因为隐式等待的效果其实并没有那么好，因为我们只是规定了一个固定时间，而页面的加载时间是受到网络条件影响的。 所以在这里还有一种更合适的显式等待方法，它直接指定好要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，那就返回查找的节点，如果到了规定时间依然没有加载出该节点，则会抛出超时异常。并且还有一个好处——当页面加载很慢时，使用显示等待，等到需要操作的那个元素加载成功之后就直接操作这个元素，不需要等待其他元素的加载。 123456789101112131415161718from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.get("https://www.zhihu.com/explore")wait = WebDriverWait(driver, 10) # 传入一个wait对象try: element = wait.until( EC.presence_of_element_located((By.CLASS_NAME, "zu-top-add-question"))) # 调用until方法 print(element) # 得到的依旧是Element对象finally: print("ok") driver.quit() 主要涉及到selenium.webdriver.support 模块下的expected_conditions类。 接着，我们首先引入了 Wait 这个对象，指定好最长等待时间，实例化出来一个 wait 等待对象。 然后调用它的 until() 方法。第一个参数传入要等待条件expected_conditions ，第二个参数传入对应的定位节点。比如在这里我们传入了 presence_of_element_located 这个条件，就代表当节点被加载出来时，条件被满足。其参数是节点的定位元组，也就是 class 为 “zu-top-add-question” 的节点搜索框。 所以这样可以做到的效果就是，在 10 秒内如果 ID 为 q 的节点即搜索框成功加载出来了，那就返回该节点，如果超过10 秒还没有加载出来，那就抛出异常。 设置Cookies使用 Selenium 还可以方便地对 Cookies 进行操作，例如获取、添加、删除 Cookies 等等。 我们使用了三次 get_cookies() 来显示对Cookies 的一些操作，可以看到 第三次调用之前，我们对cookies进行删除，因此打印此出空列表 异常处理在使用 Selenium 过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了，所以异常处理在程序中是十分重要的。我们可以使用 try except 语句来捕获各种异常。 1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: # 超时的异常处理 print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: # 没有找到节点的异常处理 print('No Element')finally: # 最终执行的语句 browser.close() 异常处理有利于我们后期的排错，也有利于爬取过程中一些错误的处理，一个完整的爬虫脚本应该含括异常处理 我们把 selenium 大概的介绍完了！ 有没有像我一样，一接触selenium 就被他高明的抓取手段迷住了呢？哈哈]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day7-数据转发过程]]></title>
    <url>%2F2019%2F07%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay7-%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8F%91%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[实例引入——LAN计算机近距离构成的小型网络 叫局域网， 简称LAN。局域网能小到是同一个房间里的两台机器，或大到校园里的上千台机器。LAN技术中最为典型的就是“以太网”，开发于1970年代，今日仍被广泛使用。 CSMA/CD一条以太网电线连接数台计算机，这就构成了一个LAN。当一台计算机要传数据给另一台计算机时，它以电信号形式，将数据传入电缆。因为电缆是共享的，所以连在同一个网络里的其他计算机也看得到数据，但不知道数据是给它们的，还是给其他计算机的——为了解决这个问题 以太网需要每台计算机有唯一的 媒体访问控制地址 简称 MAC地址，这个唯一的地址放在头部，作为数据的前缀发送到网络中，所以，计算机只需要监听以太网电缆 只有看到自己的 MAC 地址，才处理数据。 多台电脑共享一个传输媒介， 这种方法叫 “载波侦听多路访问” 简称”CSMA”。而这种共享载体式网络有一个弊端：一个共享网段中，多个主机不能同时传输数据，其会发生冲突。 解决办法是，每一台计算机都监听电线中的信号检测这些冲突，当检测到冲突时，就停止传输，等待网络空闲, 然后再试一遍。其等待的时间使用“指数退避”算法进行计算，这样，每台计算机都有机会传输数据了。 因此，CSMA/CD 全程就叫做 ： 载波侦听多路访问 / 冲突检测 虽然CSMA/CD能够一定程度上的解决问题，但是，当主机数量多起来之后，意味着传输速率大大减少，我们使用交换机进行分割为多个“冲突域”，身处不同冲突域的主机传输互不影响。 传输数据的方式：电路交换：连接两台相隔遥远的计算机或网路，最简单的办法是分配一条专用的通信线路，这就叫电路交换。这种方法虽然能用，但是不灵活而且价格昂贵，因为总有闲置的线路，好处是：如果有一条专属于自己的线路你可以最大限度地随意使用，无需共享。 报文交换：“报文交换” 就像邮政系统一样，每个站点都知道下一站发哪里， 因为站点有“表格”，记录到各个目的地，信件该怎么传等等信息。报文交换的好处是 可以用不同路由，灵活性、可靠性大大提高。 而这个过程中，每个站点就是一个路由器，消息沿着路由跳转的次数叫”跳数”，记录跳数很有用，因为可以分辨出路由问题，有利于网络工作人员的差错排查。 报文交换的缺点之一是有时候报文比较大，会堵塞网络，因为要把整个报文从一站传到下一站后才能继续传递其他报文，设想一下，如果你现在有一个非常大的文件在传输，整条路都堵塞了。即便你只有一个1KB的小文件要传输也只能等大文件传完，或是选另一条效率稍低的路线。 因此，分组交换出现了… 分组交换：将大报文分成很多小块，叫”数据包”，就像报文交换 每个数据包都有目标地址，因此路由器知道发到哪里。报文具体格式由”互联网协议”定义，简称“IP协议”。将数据拆分成多个小数据包，然后通过灵活的路由传递，非常高效且可容错，如今互联网就是这么运行的。 有了上面大概的介绍，让我们看看，我们是如何实现上网的~ 分布式网络计算机首先要连到局域网（例如WIFI，也可能是4G），局域网再连到 WAN。WAN 的路由器一般属于你的”互联网服务提供商”，简称 ISP。就这么层层叠加，最终到达了互联网主干（互联网主干由一群超大型、带宽超高路由器组成）。 例如 我们现在在网络上请求了一个资源，数据包（packet）要先到互联网主干，沿着主干到达有对应文件的 服务器，最后拿到资源并返回，其过程跨越了多个路由器交换机。 在路由器交换机中传递的数据包依据 IP头部中的“信息”进行传递（我们之前提到过），最终使用传输层协议控制，你的计算机内核就拿到了资源。 数据转发过程：接下来我们谈谈… 数据转发的底层实现（前面几篇的汇总） 假设两台主机已经建立了TCP连接。主机对应用数据（Data）压缩加密 ，然后基于传输层协议封装数据 填充源端口跟目的端口（源端口一般是随机端口号），初始序列号和确认序列号字段、标识位、窗口字段等等信息。 随后进行网络层封装 ，一般使用IP封装时，需要源目IP地址，还有其他元数据： 如果IP报文大于MTU （最大传输单元） 则会被分片。 接着，封装生命周期TTL值（每经过一个三层设备这个值将减一 默认值是255 如果三层设备发送报文时 TTL值减为0 将丢弃数据包 这样就形成了防环机制 ） 协议字段标识了传输层所使用的协议，例如上层是TCP，所以该字段的填充值为0x06 （若是UDP 则0x017 ICMP是0x01） 接下来，数据包被封装成数据帧，需填充源目MAC地址字段。主机首先会查询ARP缓存表，如果有相应MAC地址，则直接封装，如果没有，则发送ARP请求： 不同网段通信时，发送ARP查询 ：如果ARP回应不在同一个网段，主机则封装网关的mac地址，对网关进行ARP请求（源MAC是本机，目的MAC是广播地址）， 路由器网关设备收到广播数据帧后 ，将源Mac地址记录到本地Mac缓存表中，并进行ARP回复。 如果ARP回应在同一个网段，那么直接使用ARP对目标主机发出请求 这时 主机便有了目的网段的mac地址 。主机在链路层封装数据帧时，会遵循一般为以太二型帧标准 （其中的帧头的Type字段为0x0800 表明上层是IP ，ARP是0x0806 ） 主机在半双工的状态下 采用CSMA/CD 机制来检测链路是否空闲 如果链路空闲 主机会将一个前导码跟帧首定界符附加到帧头并进行传输 前导码的作用是使接受设备进行同步并做好接受数据帧的准备， 前导码是包括 七个字节的 二进制0 1交替的代码 共56位。帧首定界符是由一个字节的01 交替的 二进制序列 他的作用是使接收端对帧的第一位进行定位。 主机发送数据帧到共享以太网中 身处同一个冲突域的设备都将收到数据帧 ，他们会做以下步骤的处理： ① 进行FCS校验 如果不通过就丢弃 ②对于通过的帧 设备会检查帧中的目的MAC地址 ，若与本地MAC地址不同 ，将进行丢弃 。 ③经过上面筛选，应该是合格的数据帧 ，这时，将帧头帧尾剥去（解封装），剩下数据报文会根据帧头的Type字段来送到上层对应协议模块去处理。 网络层的处理 ： ① 检查IP头部的校验和字段 ，看ip数据报文头部是否完整 ，然后根据目的IP地址查看路由表 ，确定是否能够将数据包转发给目的端 ② 设备在转发出去之前，会对TTL值进行处理，另外报文大小也不能超过MTU值 否则将分片，网络层处理完成后 报文将被送到数据链路层进行重新封装 ，添加新的源目MAC地址，但是源目IP地址则不会改变。 目的端最终收到数据包，执行以上步骤，接着查看Protocol字段，送往传输层进行处理 传输层协议 查看数据段头部信息的端口号 ，将数据段头部剥离后 将剩下的应用数据发送给应用层处理。 这个就是数据转发过程中发生的事情，其中还省略了物理层、应用层等具体实现。不过计算机网络主要关注的是网络层、传输层，所以有所侧重。 计算机网络系列更新完结啦~ 撒花撒花！！ 这里贴出几篇之前写的计算机网络的文章： 计算机网络Day4-TCP协议 计算机网络Day3-网络层 计算机网络Day2-数据链路层]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day6-应用层协议]]></title>
    <url>%2F2019%2F07%2F29%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay6-%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[这一篇文章主要讲述的是应用层协议的几个协议 —— DNS 、 HTTP 和 DHCP DNS协议域名(domain name)是IP地址的代号。域名通常是由字符构成的。对于人类来说，字符构成的域名，比如www.yahoo.com，要比纯粹数字构成的IP地址(106.10.170.118)容易记忆。域名解析系统(DNS, domain name system)就负责将域名翻译为对应的IP地址。在DNS的帮助下，我们可以在浏览器的地址栏输入域名，而不是IP地址。 另一方面，处于维护和运营的原因，一些网站可能会变更IP地址。这些网站可以更改DNS中的对应关系，从而保持域名不变，而IP地址更新。由于大部分用户记录的都是域名，这样就可以降低IP变更带来的影响。 DNS服务器域名和IP地址的对应关系存储在DNS服务器(DNS server)中。所谓的DNS服务器，是指在网络中进行域名解析的一些服务器(计算机)。这些服务器都有自己的IP地址，并使用DNS协议(DNS protocol)进行通信。DNS协议主要基于UDP，是应用层协议 DNS服务器构成一个分级(hierarchical)的树状体系。一次DNS查询就是从树的顶端节点出发，最终找到相应末端记录的过程。 查询的顺序：中间节点根据域名的构成，将DNS查询引导向下一级的服务器。比如说一个域名cs.berkeley.edu，DNS解析会将域名分割为cs, berkeley, edu，然后按照相反的顺序查询(edu, berkeley, cs)。出口DNS首先根据edu，将查询指向下一层的edu节点。然后edu节点根据berkeley，将查询指向下一层的berkeley节点。这台berkeley服务器上存储有cs.berkeley.edu的IP地址。所以，中间节点不断重新定向，并将我们引导到正确的记录。 在整个DNS查询过程中，无论是重新定向还是最终取得对应关系，都是用户计算机和DNS服务器使用DNS协议通信。用户计算机根据DNS服务器的反馈，依次与下一层的DNS服务器建立通信。用户计算机经过递归查询，最终和末端节点通信，并获得IP地址。 DNS缓存用户计算机的操作系统中的域名解析模块(DNS Resolver)负责域名解析的相关工作。任何一个应用程序(邮件，浏览器)都可以通过调用该模块来进行域名解析。 并不是每次域名解析都要完整的经历解析过程。DNS Resolver通常有DNS缓存(cache)，用来记录最近使用和查询的域名/IP关系。在进行DNS查询之前，计算机会先查询cache中是否有相关记录。这样，重复使用的域名就不用总要经过整个递归查询过程。 HTTP协议：我们在TCP流通信中说明了，TCP协议实现了数据流的传输。然而，在实践中发现，人们往往习惯以文件为单位传输资源，比如文本文件，图像文件，超文本文档(hypertext document)。超文本文档中包含有超链接，指向其他的资源。超文本文档是万维网(World Wide Web，即www)的基础。 HTTP协议是应用层协议，它随着万维网发展起来。HTTP协议最初只是一套实践标准。其本质目的是，如何在万维网的网络环境下，更好的使用TCP协议(尽管HTTP协议也可以用UDP协议作为底层，但绝大部分都是基于TCP协议)，以实现文件，特别是超文本文件的传输。 早期的HTTP协议主要是传输静态文件，也就是存储在服务器上的文件。随着万维网的发展，HTTP协议被用于传输“动态文件”，这样的文件是服务器上的程序根据HTTP请求即时生成的文件。我们将HTTP的传输对象统称为资源(resource)。 HTTP交互过程HTTP协议的通信是一次request-responce交流。客户端(guest)向服务器发出请求(request)，服务器(server)回复(response)客户端。 通过上图一问一答式的交互，就完成了资源请求与资源响应。HTTP协议规定了请求和回复需要遵循的格式。请求和回复需要满足下面的格式: 12345起始行(start line)头信息(headers)主体(entity body) 起始行只有一行。它包含了请求/回复最重要的信息。请求的起始行表示“想要什么”。回复的起始行表示”响应的简要信息”。 头信息可以有多行。每一行是一对键值对(key-value pair)，比如:Content-type: text/plain 它表示，包含有一个名为Content-type的参数，该参数的值为text/plain。头信息是对起始行的补充。请求的头信息对服务器有指导意义 主体部分包含了具体的资源。上图的请求中并没有主体，因为这是个GET请求 (请求是可以有主体内容的)。回复中包含的主体是一段文本文字(Hello World!)。 请求报文：123我们深入一些细节。先来看一下请求：GET /index.html HTTP/1.1 # 起始行Host:www.example.com # 头信息 起始行 ： GET /index.html HTTP/1.1 GET 方法。用于说明想要服务器执行的操作。 /index.html 资源的路径。这里指向服务器上的index.html文件。 HTTP/1.1 协议的版本。HTTP第一个广泛使用的版本是1.0，当前版本为1.1。 头信息 ：Host:www.example.com 该头信息的名字是Host。HTTP的请求必须有Host头信息，用于说明服务器的地址和端口。HTTP协议的默认端口是80，如果在HOST中没有说明端口，那么将默认采取该端口。 响应报文：服务器在接收到请求之后，会根据程序，生成对应于该请求的回复，比如： 123HTTP/1.1 200 OK # 起始行Content-type: text/plain # 头部字段标识Hello World! # 响应内容 一、起始行： HTTP/1.1 协议版本 200 状态码(status code)。 OK 状态描述 OK是对状态码200的文字描述，它只是为了便于人类的阅读。电脑只关心三位的状态码(status code)，200 表示访问正常 其它常见的状态码还有: 302，重新定向(redirect): 我这里没有你想要的资源，但我知道另一个地方xxx有，你可以去那里找。 404，无法找到(not found): 我找不到你想要的资源，无能为力。 (重新定向时，客户端可以根据302的建议前往xxx寻找资源，也可以忽略该建议。) 二、字段：Content-type：说明了主体所包含的资源的类型。根据类型的不同，客户端可以启动不同的处理程序(比如显示图像文件，播放声音文件等等)。下面是一些常见的资源： text/plain 普通文本 text/html HTML文本 image/jpeg jpeg图片 image/gif gif图片 三、响应内容：服务器响应的内容主要信息都存放于此 HTTP无状态根据早期的HTTP协议，每次request-reponse时，都要重新建立TCP连接。TCP连接每次都重新建立，所以服务器无法知道上次请求和本次请求是否来自于同一个客户端。因此，HTTP通信是无状态(stateless)的。服务器认为每次请求都是一个全新的请求，无论该请求是否来自同一地址。 详细的 HTTP 协议介绍 可以阅读 我之前的爬虫文章： 爬虫Day1-网页基础 DHCP协议：DHCP协议用于动态的配置电脑的网络相关参数，如主机的IP地址，路由器出口地址、DNS域名服务器地址等。一台电脑只要接上网，就可以通过DHCP协议获得相关配置（例如IP地址等配置信息） DHCP协议全称为“动态主机设置协议”（Dynamic Host Configuration Protocol）。通常来说，普通电脑中都内置有DHCP客户端模块。电脑接上网络后，DHCP客户端发现新连通的网络，会在该网络上找DHCP服务器。DHCP服务器将给电脑提供合理的网络配置，并把设置信息传回本机。所谓的DHCP服务器，其实就是一些运行有DHCP服务器端软件的特殊电脑。本机和DHCP服务器之间的通信，都是通过DHCP协议进行的。 地址分配DHCP服务器的首要任务是分配IP地址。分配的IP地址要符合以下原则： 地址合理，即对应该局域网的IP地址和子网掩码。 地址空闲，同一网络下没有其他设备使用该地址。 地址池DHCP服务器上存有一个地址池，里面是可用的IP地址，当有一台主机接入该局域网时，DHCP服务器就会从地址池中取出一个IP地址分配给主机。此外，服务器还会说明IP地址的占用时间，也就是租期： 当然，主机使用网络的时间可能超过租期。如果主机在租期到时都没有联系DHCP服务器，那么DHCP服务器会收回IP地址，再分配给其他主机。可如果主机想继续使用IP地址，就要在中途申请延长租期。收到申请的DHCP服务器通常会答应主机的请求，允许它继续使用现有IP地址。 有了动态分配，DHCP服务器不但简化了网络配置过程，还可以有效利用IP地址资源。利用不断分配收回IP地址，实现地址的灵活分配使用，其利用率大大提高。 通信过程DHCP协议的底层是UDP协议。使用UDP的广播，把UDP数据包发送到网络的广播地址，网络上的每个设备都能收到。因此，DHCP通信主要靠这种广播的形式进行。 DHCP通信分为四步： 客户机发广播，搜寻DHCP服务器（一个网段不一定只有一台DHCP服务器）。 DHCP服务器发出Offer报文，提供一个可用的IP地址。 客户机携带该IP地址发出Request报文请求。 DHCP服务器回复ACK报文进行确认，并提供其他配置参数。 应用层的介绍就结束啦。最近更新可谓非常频繁了，啊哈哈]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day5-UDP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay5-UDP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[UDP协议简介UDP(User Datagram Protocol)传输与IP传输非常类似。你可以将UDP协议看作IP协议暴露在传输层的一个接口。UDP协议同样以数据包(datagram)的方式传输。 当应用程序对传输的可靠性要求不高，但是对传输速度和延迟要求较高时，可以用UDP协议来替代TCP协议在传输层控制数据的转发。UDP将数据从源端发送到目的端时，无需事先建立连接。UDP采用了简单、易操作的机制在应用程序间传输数据，没有使用TCP中的确认技术或滑动窗口机制，因此UDP不能保证数据传输的可靠性，也无法避免接收到重复数据的情况。 UPD的诞生：尽管UDP协议非常简单，但它的产生晚于更加复杂的TCP协议。早期的网络开发者开发出IP协议和TCP协议分别位于网络层和传输层，所有的通信都要先经过TCP封装，再经过IP封装(应用层-&gt;TCP-&gt;IP)。开发者将TCP/IP视为相互合作的套装。但很快，网络开发者发现，IP协议的功能和TCP协议的功能是相互独立的。对于一些简单的通信，我们只需要简单的IP传输就可以了，而不需要TCP协议复杂的建立连接的方式(如果过多的建立TCP连接，会造成很大的网络负担，而UDP协议可以相对快速的处理这些简单通信)。UDP协议随之被开发出来。 UDP运用场景：UDP适合传输对延迟敏感的流量，如语音和视频。在使用TCP协议传输数据时，如果一个数据段丢失或者接收端对某个数据段没有确认，发送端会重新发送该数据段。 让我们设想一下，平时打电话过程中，偶尔出现的信号不好的情况，此时，如果对方在说话，那么我们肯定是听不清楚的，而这些遗失的语音是不会重新出现的（除非你叫对方重新讲一遍） 上面的打电话案例其传输层协议就是使用的UDP，而不是TCP。UDP采用实时传输机制和时间戳来传输语音和视频数据。 使用UDP传输数据时，由应用程序根据需要提供报文到达确认、排序、流量控制等功能。 UDP说的东西不多，只要我们IP协议学的好，UDP并不是很大难题，重点还是TCP的熟练掌握。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day4-TCP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay4-TCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[传输层协议之TCP协议传输层最重要的协议为TCP协议和UDP协议。TCP协议复杂且面向连接，但传输可靠。UDP协议简单且不面向连接，传输不可靠。 TCP比较出众的一点就是提供一个可靠的，流控的数据传输。而传输层的出现有个重要的原因：端口。 IP协议进行的是IP地址到IP地址的传输，这意味者两台计算机之间的对话。但每台计算机中需要有多个通信通道，并将多个通信通道分配给不同的进程使用。一个端口就代表了这样的一个通信通道。网络层在逻辑上提供了端口的概念。一个IP地址可以有多个端口。一个具体的端口需要IP地址和端口号共同确定(我们记为IP:port的形式)。 TCP协议的两个特性：一、双向连接： TCP连接是双工(duplex)的。双向连接实际上就是建立两个方向的TCP传输。因此，主机与服务器的交互可能是双向的~ 二、TCP端口号 TCP允许一个主机同时运行多个应用进程。每台主机可以拥有多个应用端口，每对端口号、源和目标IP地址的组合唯一地标识了一个会话。端口分为知名端口和动态端口。 有些网络服务会使用固定的端口，这类端口称为知名端口，端口号范围为0-1023。动态端口号范围从1024到65535 TCP 头部： TCP通常使用IP作为网络层协议,这时TCP数据段被封装在IP数据包内。TCP数据段由TCP Header（头部）和TCP Data（数据）组成。TCP最多可以有60个字节的头部，如果没有Options字段，正常的长度是20字节。 TCP Header是由上图标识一些字段组成，下面是对字段的介绍： 16位源端口号：源主机的应用程序使用的端口号。 16位目的端口号：目的主机的应用程序使用的端口号。每个TCP头部都包含源和目的端的端口号，这两个值加上IP头部中的源IP地址和目的IP地址可以唯一确定一个TCP连接。 32位序列号：用于标识从发送端发出的不同的TCP数据段的序号。数据段在网络中传输时，它们的顺序可能会发生变化；接收端依据此序列号，便可按照正确的顺序重组数据。 32位确认序列号：用于标识接收端确认收到的数据段。确认序列号为成功收到的数据序列号加1。 4位头部长度：表示头部占32bit字的数目，它能表达的TCP头部最大长度为60字节。 16位窗口大小：表示接收端期望通过单次确认而收到的数据的大小。由于该字段为16位，所以窗口大小的最大值为65535字节，该机制通常用来进行流量控制。 16位校验和：校验整个TCP报文段，包括TCP头部和TCP数据。该值由发送端计算和记录并由接收端进行验证。 TCP 三次握手建立起一个TCP连接需要经过“三次握手”： 第一次握手：客户端发送请求建立连接报文（seq = x，同时SYN置1，表示第一次建立连接）到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到SYN包，回复客户的请求报文（seq =y，ack=x+1），同时回包，即SYN+ACK包（SYN、ACK置1，ACK置1表示同意建立连接），此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手” TCP四次挥手： 我们可以看到，连接终结的过程中，连接双方也交换了四片信息(两个FIN和两个ACK)。TCP并没有合并FIN与ACK片段。原因是TCP连接允许单向关闭(half-close)。 假设Client端发起中断请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说“我client端要发给你了”，但是如果你还没有数据要发送完成，则不必急着关闭Socket，可以继续发送数据。所以所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。 当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，”就知道可以断开连接了”。Client端等待了2MSL(最大报文段生存时间)后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。 Ok，TCP连接就这样关闭了！ “流”通信TCP协议是传输层协议，实现的是端口到端口(port)的通信。更进一步，TCP协议虚拟了文本流(byte stream)的通信。在TCP协议与”流”通信中讨论的TCP传输需要一个前提：TCP连接已经建立。 IP协议和UDP协议采用的是数据包的方式传送，后发出的数据包可能早到，我们并不能保证数据到达的次序。 TCP协议确保了数据到达的顺序与文本流顺序相符。当计算机从TCP协议的接口读取数据时，这些数据已经是排列好顺序的“流”了。比如我们有一个大文件要从本地主机发送到远程主机，如果是按照“流”接收到的话，我们可以一边接收，一边将文本流存入文件系统。这样，等到“流”接收完了，硬盘写入操作也已经完成。如果采取UDP的传输方式，我们需要等到所有的数据到达后，进行排序，才能组装成大的文件。 片段与编号我们知道了TCP是通过报文流进行通信的，那么同时发出的多个报文流，对端是如何按正确的顺序进行组装呢？ TCP片段的头部(header)会存有该片段的序列号(sequence number)。这样，接收的计算机就可以知道接收到的片段在原文本流中的顺序了，也可以知道自己下一步需要接收哪个片段以形成流。比如已经接收到了片段1，片段2，片段3，那么接收主机就开始期待片段4。如果接收到不符合顺序的数据包(比如片段5)，接收方的TCP模块可以拒绝接收，从而保证呈现给接收主机的信息是符合次序的“流”。 TCP 可靠性 TCP的可靠性是这么体现的：在每收到一个正确的、符合次序的片段之后，就向发送方(也就是连接的另一段)发送一个特殊的TCP片段，回复一个ACK给发送方(ACK，acknowledge)：“我已经收到那个片段了。” 这个特殊的TCP片段叫做ACK回复。如果一个片段序号为L，对应ACK回复有回复号L+1，也就是接收方期待接收的下一个发送片段的序号。如果发送方在一定时间等待之后，还是没有收到ACK回复，那么它推断之前发送的片段一定发生了异常。发送方会重复发送(retransmit)那个出现异常的片段，等待ACK回复，如果还没有收到，那么再重复发送原片段… 直到收到该片段对应的ACK回复(回复号为L+1的ACK)。 当发送方收到ACK回复时，它看到里面的回复号为L+1，也就是发送方下一个应该发送的TCP片段序号。发送方推断出之前的片段已经被正确的接收，随后发出L+1号片段。ACK回复也有可能丢失。对于发送方来说，这和接收方拒绝发送ACK回复是一样的。发送方会重复发送，而接收方接收到已接收过的片段，推断出ACK回复丢失，会重新发送ACK回复。 通过ACK回复和重新发送机制，TCP协议将片段传输变得可靠。但我们也看出来了—— 这也太麻烦了吧！ 而TCP 的连接是“可靠的”，这一点还体现在 滑动窗口机制和重传机制上。 TCP 滑动窗口机制：滑窗：上面的工作方式中，发送方保持发送-&gt;等待ACK-&gt;发送-&gt;等待ACK…的单线工作方式，这样的工作方式叫做stop-and-wait。stop-and-wait虽然实现了TCP通信的可靠性，但同时牺牲了网络通信的效率。在等待ACK的时间段内，我们的网络都处于闲置(idle)状态。我们希望有一种方式，可以同时发送出多个片段。然而如果同时发出多个片段，那么由于IP包传送是无次序的，有可能会生成乱序片段。 在stop-and-wait的工作方式下，乱序片段完全被拒绝（序列号对不上，会等待重发），这也很不效率。毕竟，乱序片段只是提前到达的片段。我们可以在缓存中先存放它，等到它之前的片段补充完毕，再将追加在后面。 然而，如果一个乱序片段实在是太过提前(太“乱”了)，该片段将长时间占用缓存。我们需要一种折中的方法来解决该问题：利用缓存保留一些“不那么乱”的片段，期望能在段时间内补充上之前的片段(暂不处理，但发送相应的ACK)；对于“乱”的比较厉害的片段，则将它们拒绝(不处理，也不发送对应的ACK)。 这个机制就是——窗口机制 窗口机制TCP滑动窗口技术可以通过动态改变窗口大小来实现对端到端设备之间的数据传输进行流量控制。 滑窗(sliding window)被同时应用于接收方和发送方，以解决以上问题。发送方和接收方各有一个滑窗。当片段位于滑窗中时，表示TCP正在处理该片段。滑窗中可以有多个片段，也就是可以同时处理多个片段。滑窗越大，越大的滑窗同时处理的片段数目越多(当然，计算机也必须分配出更多的缓存供滑窗使用)。 流量控制主机和服务器之间可以通过滑动窗口来实现流量控制。 TCP协议会根据情况自动改变滑窗大小，以实现流量控制。流量控制(flow control)是指接收方将 window的大小通知给发送方，从而指导发送方修改发送 window的大小。接收方将该信息放在TCP头部的window size区域：发送方在收到window size的通知时，会调整自己滑窗的大小。这样，发送窗口变小，文本流发送速率降低，从而减少了接收方的负担。 TCP重传机制：超时重传机制超时重传机制是指：当发送方送出一个TCP片段后，将开始计时，等待该TCP片段的ACK回复。如果接收方正确接收到符合次序的片段，接收方会利用ACK片段回复发送方。发送方得到ACK回复后，继续移动窗口，发送接下来的TCP片段。如果直到计时完成，发送方还是没有收到ACK回复，那么发送方推断之前发送的TCP片段丢失，因此重新发送之前的TCP片段。这个计时等待的时间叫做重新发送超时时间(RTO, retransmission timeout)。 快速重传机制快速发送机制如果被启动，将打断计时器的等待，直接重新发送TCP片段。 快速重传机制：实现了另外的一种丢包评定标准，即如果连续收到3次 ACK，发送方就认为这个seq的包丢失了，立刻进行重传，这样如果接收端回复及时的话，基本就是在重传定时器到期之前，提高了重传的效率。 TCP的可靠机制虽然保证了数据安全，但是往往也带来某些方面的缺陷：例如TCP慢启动和TCP全局同步现象。 TCP的缺陷：一个概念——慢启动：TCP连接刚建立时，不是一下子就能把握TCP窗口值大小的，会从一个小的窗口值慢慢往上加，最后协商成功，达到稳定的windows大小。 TCP全局同步：当网络发生堵塞时，报文就不能及时送达目的端。对于TCP报文，如果大量的报文被丢弃，将造成TCP超时，从而引发TCP慢启动，使得TCP减少报文的发送。当队列同时丢弃多个TCP连接的报文时，将造成多个TCP连接同时进入拥塞避免和慢启动状态以调整并降低流量，这就被称为TCP全局同步现象。这样多个TCP连接发往队列的报文将同时减少，而后又会在某个时间同时出现流量高峰，如此反复，使网络资源利用率低。 资源消耗：TCP 最大的缺点是那些”确认码”数据包把数量翻了一倍，但并没有传输更多信息。有时候这种代价是不值得的，于是有了UDP这个协议来代替一些用户的其他需求（下一篇我们会讲到） 这一章节我不知不觉就写了四千字（捂脸），可见TCP之重要性… 刚开始学TCP的时候，我还没有意识到问题的严重性…hhh 这几天写的博客有点多，因为热爱，所以无惧！ 当回过头来看这些文章时，更多的是一种自豪感，我会继续坚持下去的。Fighting！]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day3-网络层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay3-%E7%BD%91%E7%BB%9C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[网络层：网络层(network layer)是实现互联网最重要的一层。正是在网络层面上，各个局域网根据IP协议相互连接，最终构成覆盖全球的Internet。更高层的协议，无论是TCP还是UDP，必须通过网络层的IP数据包(datagram)来传递信息。操作系统也会提供该层面的套接字(socket)，从而允许用户直接操作IP包。 IP数据包： IP数据包简称为IP包。它是符合IP协议的0/1序列。信息包含在这一序列中。IP包分为头部(header)和数据(Data)两部分。数据部分是要传送的信息，头部是为了能够实现传输而附加的信息。 与帧类似，IP包的头部也有多个区域。我们将注意力放在源地址(source address)和目的地(destination address)。它们都是IP地址。IPv4的地址为4 bytes的长度(也就是32位)。我们通常将IPv4的地址分为四个十进制的数，每个数的范围为0-255,比如192.0.0.1就是一个IP地址。填写在IP包头部的是该地址的二进制形式。 IP地址是全球地址，它可以识别局域网和主机。这是通过将IP地址分类实现的。下图则为 IP 地址分类： 每个IP地址的32位分为前后两部分，第一部分用来区分局域网，第二个部分用来区分该局域网的主机。 子网掩码(Subnet Mask)告诉我们这两部分的分界线，比如255.0.0.0(也就是8个1和24个0)表示前8位用于区分局域网，后24位用于区分主机。由于A、B、C分类是已经规定好的，所以当一个IP地址属于B类范围时，我们就知道它的前16位和后16位分别表示局域网和主机。 网卡与路由器网卡（NIC）： IP地址是分配给每个计算机的。但这个说法并不精确。IP地址实际上识别的是网卡(NIC, Network Interface Card)。 网卡是计算机的一个硬件，它在接收到网路信息之后，将信息交给计算机的CPU处理。当计算机需要发送信息的时，也要通过网卡发送。一台计算机可以有不只一个网卡，比如笔记本就有一个以太网卡和一个WiFi网卡。计算机在接收或者发送信息的时候，要先决定想要通过哪个网卡。 路由器： 路由器(router)实际上就是一台配备有多个网卡的专用电脑。它让网卡接入到不同的网络中。他专门负责与多个网络之间的寻址与通信。跨网段的数据通信一般都会经过路由器 路由寻址：IP包的传输要通过路由器的接力。每一个主机和路由中都存有一个路由表(routing table)。路由表根据目的地的IP地址，规定了等待发送的IP包所应该走的路线。 始发：比如我们从主机145.17生成发送到146.21的IP数据包，剩下数据部分可以是TCP包，可以是UDP包，我们暂时不关心。注明目的地IP地址(199.165.146.21)和发出地IP地址(199.165.145.17)。主机145.17随后参照自己的路由表，看到路由表中的记录： 145.17 routing table (Genmask为子网掩码,Iface用于说明使用哪个网卡接口) 这里有两行记录： 第一行，如果IP目的地是199.165.145.0这个网络的主机，那么只需要自己在eth0上的网卡直接传送(“本地社区”：直接送达)，不需要前往路由器(Gateway 0.0.0.0 = “本地送信”)。 第二行，所有不符合第一行的IP目的地，都应该送往Gateway 199.165.145.17，也就是中间路由器接入在eth0的网卡IP地址。因为 目的地址和子网掩码全为 0.0.0.0 是缺省路由（当匹配不到路由明细时，就会往这个接口发出） 中转我们的IP包目的地为199.165.146.21，不符合第一行，所以按照第二行，发送到中间的路由器。主机145.17会将IP包放入帧的payload，并在帧的头部写上199.165.145.17对应的MAC地址，这样，就可以在局域网中传送了。 中间的路由器在收到IP包之后(实际上是收到以太协议的帧，然后从帧中的payload读取IP包)，提取目的地IP地址，然后对照自己的路由表： 送达：数据包的目的地符合第二行，所以将IP放入一个新的帧中， 在帧的头部写上199.165.146.21的MAC地址，直接发往主机146.21。 总结： 这样，我们就完成了“路由”的过程。整个过程中，IP包不断被主机和路由封装入帧并拆开，然后借助连接层，在局域网的各个NIC之间传送帧。整个过程中，我们的IP包的内容保持完整，没有发生变化（有变化的是TTL值、FCS校验码等字段）。最终的效果是一个IP包从一个主机传送到另一个主机。利用IP包，我们不需要去操心底层(比如连接层)发生了什么。 路由协议：之前的信息传递基于一个假设：每个人手里都有份准确的地图。用计算机的话来说，就是每个主机和路由上都已经有一张路由表。这个路由表描述了网络上的路径信息。如果你了解自己的网络连接，可以手写自己主机的路由表。但是，一个路由器可能有多个出口，所以路由表可能会很长。更重要的是，周围连接的其他路由器可能发生变动(比如新增路由器或者拓扑变更)，我们就需要路由表能及时将交通导向其他的出口。我们需要一种更加智能的探测周围的网络拓扑结构，并自动生成路由表—— 路由协议 一些比较出名的路由协议有： OSPF、BGP、ISIS 等 这里不过多介绍，不然可以讲一天 =.=]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day2-数据链路层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay2-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82%2F</url>
    <content type="text"><![CDATA[以太网的帧格式帧本身是一段有限的0/1序列。它可以分为以下几部分： Preameble 和 SFD： 帧的最初7个字节被称为前导码，前导码是包括 七个字节的二进制0 1交替的代码 共56位。 定界符是由一个字节的01 交替的 二进制序列 ，他的作用是使接收端对数据帧的第一位进行定位 通常，我们都会预定好以一定的频率发送0/1序列(比如每秒10bit)。如果接收设备以其他频率接收(比如每秒5bit)，那么就会错漏掉应该接收的0/1信息。但是，由于网卡的不同，发送方和接收方即使预订的频率相同，两者也可能由于物理原因发生偏差。这就好像两个人约好的10点见，结果一个人表快，一个人表慢一样。前导码是为了让接收设备调整接收频率，以便与发送设备的频率一致。 dst 和 src 紧跟着后面的是6 字节的目的地和6 字节的源地址。要注意，这里写的是MAC地址。MAC地址是物理设备自带的序号，只能在同一个以太网中被识别 Data 数据一般包含有符合更高层协议的数据，比如IP包。连接层协议本身并不在乎数据是什么，它只负责传输。注意，数据尾部可能填充有一串0(PAD区域)。填充0的原因是，一个帧需要超过一定的最小长度。 数据的字节长限制为 64～1500字节： 最大包长1518字节，其中数据1500字节为MTU，18字节为字段值 最小包长64字节，为了限制冲突，有足够时间检测到冲突（CSMA/CD） Type字段： type字段两个字节，它用于区分两种帧类型，当type字段值小于1500，则是IEEE802.3格式。当type字段值大于或等于1536，帧使用的是以太Ethernet格式。0×0800标识IP，0×0806标识ARP) FCS效验码 接着是校验序列(FCS, Frame Check Sequence)。校验序列是为了检验数据的传输是否发生错误。在物理层，我们通过一些物理信号来表示0/1序列(比如高压/低压，高频率/低频率等)，但这些物理信号可能在传输过程中受到影响，以致于发生错误。 FCS采用了循环冗余校验(CRC, Cyclic Redundancy Check)算法。利用特定的算法得到的余数与实际拿到数据包的FCS进行比较， 如果两者不相符，我们就知道数据在传输的过程中出现错误，不能使用。 以上的数据包封装需要MAC 地址，本端的MAC地址我们知道（由厂商分配写好），那么对端的MAC地址我们如何得知呢？ ARP协议ARP协议介于连接层和网络层之间，ARP包需要包裹在一个帧中。它的工作方式如下：主机会发出一个ARP包，该ARP包中包含有自己的IP地址和MAC地址。通过ARP包，主机以广播的形式询问局域网上所有的主机和路由：我是IP地址xxxx，我的MAC地址是xxxx，有人知道199.165.146.4的MAC地址吗？拥有该IP地址的主机会回复发出请求的主机：哦，我知道，这个IP地址属于我的一个NIC，它的MAC地址是xxxxxx。由于发送ARP请求的主机采取的是广播形式，并附带有自己的IP地址和MAC地址，其他的主机和路由会同时检查自己的ARP cache，如果不符合，则更新自己的ARP cache。 这样，经过几次ARP请求之后，ARP cache会达到稳定。如果局域网上设备发生变动，ARP重复上面过程。 总的来说： APR协议就是通过IP 地址请求对端的MAC地址的过程。 免费arp ： 主机被分配IP地址或者IP地址发生变更之后，检测该网段是否唯一，避免冲突。主机用过发送一个arp request 报文进行检测，将广播报文中目的ip地址设置为自己的IP地址。当收到其他主机的回复时，则表示该网段被占用。 ICMP协议：ICMP(Internet control message protocol)是网络层的的协议，用来在网络设备间传递各种差错和控制信息，他对于收集各种网络信息，诊断和排除各种网络故障具有至关重要的作用。 ICMP重定向，ICMP差错检查:用于诊断源和目的之前的连通性(ICMP echo request查询和ICMP echo reply响应)，ICMP错误报告(各种传输数据失败的原因) ICMP基于IP协议。也就是说，一个ICMP包需要封装在IP包中，然后在互联网传送。ICMP是IP套装的必须部分，也就是说，任何一个支持IP协议的计算机，都要同时实现ICMP。 常见的ICMP包类型目的端不可达目的地无法到达(Destination Unreachable)属于错误信息。如果一个路由器接收到一个没办法进一步接力的IP包，它会向出发主机发送该类型的ICMP包。比如当IP包到达最后一个路由器，路由器发现目的地主机down机，就会向出发主机发送目的地无法到达(Destination Unreachable)类型的ICMP包。目的地无法到达还可能有其他的原因，比如不存在路由表项，比如不被接收的端口号等等。 重定向重新定向(redirect)属于错误信息。当一个路由器收到一个IP包，对照其routing table，发现自己不应该收到该IP包，它会向出发主机发送重新定向类型的ICMP，提醒出发主机修改自己的routing table（“我”不是你的目标，请更换目的地址） ICMP 的两个运用： Ping 和 Ttracertping可以检测网络的连通性，这一点运用很广。ping命令就是利用了该类型的ICMP包。当使用ping命令的时候，将向目标主机发送Echo-询问类型的ICMP包，而目标主机在接收到该ICMP包之后，会回复Echo-回答类型的ICMP包，并将询问ICMP包包含在数据部分。ping命令是我们进行网络排查的一个重要工具。如果一个IP地址可以通过ping命令收到回复，那么其他的网络协议通信方式也很有可能成功。 tracert 用于测试链路的跳数：IPv4中的Time to Live(TTL)会随着经过的路由器而递减，当这个区域值减为0时，就认为该IP包超时(Time Exceeded)。Time Exceeded就是TTL减为0时的路由器发给出发主机的ICMP包，通知它发生了超时错误。traceroute命令用来发现IP接力路径(route)上的各个路由器。它向目的地发送IP包，第一次的时候，将TTL设置为1，引发第一个路由器的Time Exceeded错误。这样，第一个路由器回复ICMP包，从而让出发主机知道途径的第一个路由器的信息。随后TTL被设置为2、3、4，…，直到到达目的主机。这样，沿途的每个路由器都会向出发主机发送ICMP包来汇报错误。traceroute将ICMP包的信息打印在屏幕上，就是路径的信息了。 集线器(Hub) vs. 交换器(Switch)二层设备集线器： 以太网使用集线器或者交换器将帧从发出地传送到目的地。一台集线器或交换器上有多个端口，每个端口都可以连接一台计算机(或其他设备)。 集线器像一个广播电台。一台电脑将帧发送到集线器，集线器会将帧转发到所有其他的端口。每台计算机检查自己的MAC地址是不是符合DST。如果不是，则保持沉默。集线器是比较早期的以太网设备。它有明显的缺陷： 任意两台电脑的通信在同一个以太网上是公开的。所有连接在同一个集线器上的设备都能收听到别人在传输什么，这样很不安全。可以通过对信息加密提高安全性。 不允许多路同时通信。如果两台电脑同时向集线器发信，集线器会向所有设备发出“冲突”信息，提醒发生冲突。可以在设备上增加冲突检测算法：一旦设备发现有冲突，则随机等待一段时间再重新发送（CSMA/CD） 交换机： 交换器克服集线器的缺陷。交换器记录有各个设备的MAC地址。当帧发送到交换器时，交换器会检查DST，然后将帧只发送到对应端口。交换器允许多路同时通信。由于交换器的优越性，交换器基本上取代了集线器。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识之操作系统]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[操作系统的诞生现代计算机系统是由一个或者多个处理器，主存，磁盘，打印机，键盘，鼠标显示器，网络接口以及各种其他输入输出设备组成的复杂系统，每位程序员不可能掌握所有系统实现细节，并且管理优化这些部件是一件挑战性极强的工作。所以，我们需要为计算机安装一层软件，也就是操作系统。操作系统的任务就是用户程序提供一个简单清晰的计算机模型，并管理以上所有设备。 操作系统充当软件和硬件之间的媒介，更具体来说，操作系统提供API来抽象硬件，叫“设备驱动程序” 操作系统使得程序员可以用标准化机制 和 输入输出硬件（I/O）交互。还有就是说，程序员只负责编写程序，如何调用、处理程序、输出内容的任务就交给了操作系统 总结：操作系统是一个用来协调、管理和控制计算机硬件和软件资源的系统程序，其中包括：文件系统、内存管理、设备管理和进程管理。它位于硬件和应用程序之间。需要注意的是，操作系统本质上也是一个软件，它由硬件上的程序进行调用运行。 CPU的资源利用我们知道，CPU的运算是很快的，而 I/O、打印机之类的设备运行很慢，这导致了一个后果，CPU闲置（程序阻塞在I/O上，这很浪费资源） 我们想实现的是：当cpu遇到阻塞时，它会把程序休眠，切换到下一个程序，等到阻塞的任务完成了，再回过头来进行运行处理。 所以我们需要有一种方式来最大限度的利用它，这样使得多个程序可以同时运行，在单个CPU上共享时间——“多任务处理” 内存地址的分配：每个程序都会占用一些内存，当切换到另一个程序时，我们不能丢失数据，需要记录当前状态，以方便回过头来处理程序时不会丢失数据。 解决办法就是： 给每个程序分配专属内存块 把程序A分配到内存地址0~999 把程序B分配到内存地址1000~1999 分配内存地址面临的一个问题： 如果一个程序请求更多内存，操作系统会决定是否同意，以及决定分配具体哪些内存块。这样虽然灵活性很好，但是会遇到个问题，由于更多的内存块是后续申请的，同个程序的内存块可能分配到非连续的情况。 虚拟内存其实真正的程序可能会分配到内存中数十个地方，这对于程序员来说很难跟踪，为了隐藏这种复杂性，操作系统会把内存进行“虚拟化”—— 称为虚拟内存，程序可以假定内存总是从地址0开始，而实际物理地址，被操作系统隐藏和抽象了。 如上图所示，此时程序B的虚拟内存地址为0999，而实际的物理地址，却是10001999。程序员不用关心实际的内存地址，操作系统会自动处理虚拟内存和物理内存之间的映射。 这种功能对于程序A的情况更为有用： 程序A的物理内存是不连续的，而从程序A的角度上看，它是连续的（虚拟内存） 这种机制使程序的内存大小可以灵活增减，叫“动态内存分配”，它的简化，为操作系统同时运行多个程序提供了更大的灵活性 内存保护给程序分配专用的内存范围，还有另外一个好处： “程序之间的隔离” 程序与程序直接互相独立，互不影响。内存保护是操作系统对计算机上的内存进行访问权限管理的一个机制。内存保护的主要目的是限制某个进程访问的只是操作系统配置给它的地址空间。这个机制可以防止某个行程，因为某些程序错误或问题，而有意或无意地影响到其他进程或是操作系统本身的运行状态和数据。 这也是程序与程序之间不能直接通信的原因。 UNIXUNIX操作系统把操作系统的各个任务总的划为两个部分： 如内存管理，多任务和输入/输出处理，这叫内核（Kernel）。操作系统的内核是一个管理和控制程序，负责管理计算机的所有物理资源，其中包括：文件系统、内存管理、设备管理和进程管理。 第二部分是一堆有用的工具（比如程序和运行库） UNIX的另一个特点就是被归类为分时操作系统，所谓分时操作系统，值得就是计算机将CPU的处理以时间段进行划分，优先级高的优先执行，若执行程序优先级降低，则推出执行，转而执行优先级更高的程序和事务。分时操作系统的执行效率是非常高的。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day1-TCP/IP协议]]></title>
    <url>%2F2019%2F07%2F27%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay1-TCP-IP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[引言：感谢！计算机网络可以说是一门计算机的必修课吧，私以为，计算机网络和计算机组成原理是一名计算机专业学生最应该先学习的知识~ 于是我决定展开对于这门学科一些知识总结与感悟。 在这里要感谢一名老师，在刚踏上大学这段旅途的时候，余老师像一盏明灯一样，使得我面临计算机这个钢铁巨兽面前不会感到手足无措，带我踏进了计算机这个门槛、揭开了计算机的奥妙。 实例引入：计算机之所以能够通信，是因为其遵守了相约定好的协议(protocol)。就像人与人之间对话所使用的语言一样，语言不同，自然也就不能实现“通信”。于是，协议这种约定俗成的东西就诞生了。 TCP/IP 模型： 下面的 OSI 七层模型是最早的协议，但由于某些原因，他更多的是一种标准的形式存在，没有实际推广（有兴趣的同学可以自行百度~） TCP/IP 四层模型的网络接口层又被人拆分为 数据链路层和 物理层。（又叫TCP 五层模型） 下面我们针对TCP/IP的各层进行简单介绍 物理层所谓的物理层，是指光纤、电缆或者电磁波等真实存在的物理媒介。这些媒介可以传送物理信号，比如电信号、电压。对于数字应用来说，我们只需要两种物理信号来分别表示0和1，比如用高电压表示1，低电压表示0，就构成了简单的物理层协议。针对某种媒介，电脑可以有相应的接口，用来接收物理信号，并解读成为0/1序列。 数据链路层息以帧(frame)为单位传输。信息是一段有序的0/1序列，而帧，是这个序列中符合特定格式的一小段。连接层协议的功能就是识别0/1序列中所包含的帧。并且负责 物理层和网络层之间的通信。 帧中，有源地址和目的地址，还有能够探测错误的校验序列(FCS)。当然，帧中最重要的最重要是所要传输的数据 通过连接层协议，我们可以建立局域的以太网或者WiFi局域网，并让同一局域网中的两台计算机通信。 网络层数据链路层使得同一个局域网的人可以互相通信，那么不处于同一个局域网如何通信呢？我们需要一个“中间人”。这个“中间人”必须有以下功能: 能从物理层上在两个网络的接收和发送0/1序列， 能同时理解两种网络的帧格式。 路由网关： 路由器(router)就是为此而产生的“中间人”，也称为“网关”。一个路由器有多个网卡(NIC，Network Interface Controller)。每个网卡可以接入到一个网络，并理解相应的连接层协议。在帧经过路由到达另一个网络的时候，路由会读取帧的信息，并改写以发送到另一个网络。 两个不同局域网的计算机是这么通信的： 主机发出数据 -&gt; 发现与对端主机处在不同个局域网 -&gt; 数据交给网关进行转发 -&gt; 送达目的端计算机 而实际情况比这更加复杂，上述内容省略了ARP、ICMP、路由等步骤 在数据链路层，一个帧中只能记录目的地址和源地址两个地址。而完成上面的过程需要经过四个地址(计算机1，网关地址，以太网接口，计算机2)，因此，这也是 IP协议的主要功能——寻址。 传输层上面的三层协议让不同的计算机之间可以通信。但计算机中可能运行了许多个进程，每个进程都可能有自己的通信需求。我们使用网络接口层、网络层实现了主机与主机之间的通信，可是， 软件与软件之间的通信该如何解决呢？ 没错，就是传输层要做的事情了。 端口号： 计算机有多个进程，那么我们如何辨别是某个进程发出的信息请求呢？ 比如我们打开firefox浏览网页，与此同时，又用QQ来接收邮件。我们需要有一个标志来识别，也就是端口号啦。 我们知道IP层的ip地址可以唯一标识主机，而端口号可以唯一标示主机的一个进程。 TCP 和 UDP Internet 在传输层有两种主要的协议：一种是面向连接的协议 TCP ，一种是无连接的协议 UDP。利用端口号，一台主机上多个进程可以同时使用 TCP/UDP 提供的传输服务，并且这种通信是端到端的，它的数据由 IP 传递，但与 IP 数据报的传递路径无关。这俩兄弟的具体情况，我们以后会涉及到。 应用层应用层协议定义了运行在不同端系统上的应用程序进程如何相互传递报文。向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。 常用服务 协议 端口号 HTTP TCP 80 FTP TCP 20、21 Telnet TCP 23 ​ 以上就是这篇文章的全部内容，我们只是介绍了个大概的框架，具体内容会在以后进行详细介绍。 还有一点要说的是，计算机网络是基础，大多数内容都是理论。这些知识点都是需要牢牢记住的~ 因为听说面试大多数都会问到 TCP 如何建立连接、HTTP 的内部实现 等等一些理论性知识点。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day7-Mysql的那些事]]></title>
    <url>%2F2019%2F07%2F26%2F%E7%88%AC%E8%99%ABDay7-Mysql%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[前言： 爬虫爬取到的数据最终是要保存下来滴~ 别以为爬下来就完事了，具体流程还有数据去重，数据分析，最后数据可视化等等操作呢… 这一章节，我们聊聊 “数据存储” 在 Python2 中，连接 MySQL 的库大多是使用 MySQLDB，但是此库官方并不支持 Python3，所以在这里推荐使用的库是 PyMySQL。 环境准备： python 3 Pycharm 安装好PyMysql库 本地mysql环境的部署 PyMysql使用介绍：获取连接对象我们要连接到 mysql 这个库，首先应该获取一个connection对象（输入本地部署的Mysql参数） 1234import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)# 通过 PyMySQL 的 connect() 方法声明了一个 MySQL 连接对象 获取操作游标紧接着，需要实例化一个操作游标对象，这个对象的作用是执行sql语句 12345import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)cursor=db.cursor()# 注意：cursor 对象是基于db 这个connection对象实例化出来的 我们拿到操作游标后，就很容易了。因为python执行的 sql 语句跟 mysql 里面是一样的…但还是有一丢丢不同的，让我们接着往下看。 插入数据 注意我们这里使用了异常处理，因为在python里面是采用事务这个概念进行操作的 插入数据需要使用commit进行数据提交；当捕捉到错误时，db对象进行rollback事件回滚（只针对插入、更新、删除这些对数据库进行更改的操作，而创建数据库、创建表则不需要） 还有一点要注意的是：里面的参数，不管什么类型，统一使用%s作为占位符 动态插入数据：有时候我们面临的实际情况，你不知道有多少字段插入（上面的情况是我们”写死“的），可以说，插入的字段由你爬取的数据决定，我们应该学会根据实际情况去构造 sql 语句。 在很多情况下，我们要达到的效果是插入方法无需改动，做成一个通用方法，只需要传入一个动态变化的字典给就好了。比如我们构造这样一个字典： 上面这条执行语句， 就替代之前那条 很 low 的写法，这么一来，无论字典有多少个键值对，我们都能进行插入。 数据查询：是的，对于数据库的增删改查，什么最重要？ （大声告诉我！）—— 没错！ 就是查询！！ 旁白君吐槽：“自嗨型作者…” 于是，旁白君大手一挥，说：“这个还不简单，我最拿手了。”于是抢走了作者的键盘，刷刷的写下这么一串代码 12345678910111213import pymysqldb = pymysql.connect(host='localhost',port=3306,user='root',password='root',db='spiders')cursor = db.cursor()table = 'students'sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: results = cursor.execute(sql) print(results)except Exception as e: print(e) 但是，结果却是这样的… what？ 这是什么鬼东西，我的数据呢？？ 作者邪魅一笑，眼中三分不屑三分冷漠四分讥笑，慢吞吞的拿走旁白君的键盘，自信的啪啪啪添加上了这么几条代码 12345678910sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: cursor.execute(sql) results = cursor.fetchall() # fetchall（）方法，获取查询数据的返回值 for row in results: print(row)except Exception as e: print(e) 看到输出的结果，作者在旁白君羡慕的眼神中满意的点了点头 旁白君对着图仔细看了看说：“刚好四条！原来我刚才输出的4是执行成功的条目数！” “没错，悟性还挺高！” 作者留下潇洒离去的背影，并甩给旁白君一本秘籍。 数据查询的方法： 使用select语句查询后不会直接返回查询的具体数据，而是返回执行成功的条目数，我们需要使用fetchone或者fetchall方法读取出来。 fetchone() 方法，这个方法可以获取结果的第一条数据，返回结果是元组形式，元组的元素顺序跟字段一一对应，也就是第一个元素就是第一个字段 id，第二个元素就是第二个字段 name，以此类推。 fetchall() 方法，它可以得到结果的所有数据，然后将其结果和类型打印出来，它是二重元组，每个元素都是一条记录。我们将其遍历输出，将其逐个输出出来。 我们还可以用 while 循环加 fetchone() 的方法来获取所有数据，而不是用 fetchall() 全部一起获取出来，fetchall() 会将结果以元组形式全部返回，如果数据量很大，那么占用的开销会非常高。所以推荐使用如下的方法来逐条取数据 总结这一篇主要是理解如何连接数据库、事务的概念（默认开启事务），以及使用对数据库对字典形式的数据进行操作。然后 使用sql语句进行增删改查（主要是动态数据、插入更新数据部分） 各位，我们下期再见！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day6-Beautiful介绍]]></title>
    <url>%2F2019%2F07%2F24%2F%E7%88%AC%E8%99%ABDay6-BeautifulSoup%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[之前的urllib实战中我们谈到了BeautifulSoup这个解析库，那么…是不是应该来一篇详细介绍呢？ 今天，他来了！！ BeautifulSoup的基本使用：拿到网页数据：一般来说，我们使用解析库的前提，已经是通过请求库拿到了网页的数据，假设现在由下图的 html 变量存储的数据正是我们请求的文本内容，下面我们将使用BeautifulSoup进行加工解析。 获取Soup对象：使用BeautifulSoup方法，第一个参数传入 HTML字符串文本，第二个参数传入的是解析器的类型，在这里我们使用lxml。不指定会使用默认的解析器，但是会发出警告 123from bs4 import Beautifulsoupsoup = BeautifulSoup(html,"lxml")print(soup) 运行后，发现内容如下（发现不完整的HTML字符串被补齐了）： 属性选择调用 soup 的各个方法和属性（选择器）对这串HTML代码解析。这个soup对象支持 “.” 获取节点操作，也可以通过”.”表达嵌套关系。 我们直接拿到的soup打印不美观，使用prettify（）方法可以打印出树形结构的 html 文本 这里我们打印 title 的文本内容，以及 head 节点信息 上面就是BeautifulSoup的使用流程，有时候，我们需要的内容存放于某些节点中，这些节点有它的特征，我们可以通过选择器获取这些节点，最后拿到我们需要的结果。 节点选择器：选择元素： 元素选择—— 直接通过调用节点的名称就可以选择节点元素了，这种选择方式速度非常快，如果单个节点结构话层次非常清晰，可以选用这种方式来解析。 注意，我们的选择器是基于soup这个对象的 输出了soup.head的类型，是 bs4.element.Tag 类型.这是 BeautifulSoup 中的一个重要的数据结构，经过选择器选择之后，选择结果都是这种 Tag 类型，它具有着一些属性，比如 string 属性（输出文本信息），可以进行获取属性、嵌套等操作。 不过这种选择方式只会选择到第一个匹配的节点，其他的后面的节点都会忽略——比如我们打印的p节点，只呈现一个信息（html字符串中有三个p节点信息） 获取属性：每个节点可能有多个属性，比如id，class 等等，我们选择到这个节点元素之后，可以调用 attrs 获取所有属性。 可以看到 attrs 的返回结果是字典形式，把选择的节点的所有属性和属性值组合成一个字典。因此，可以通过键值形式进行取值传入节点的属性，获取该节点的属性值 contents 属性： 选取到了一个节点元素之后，如果想要获取它的直接子节点的信息 （不包含子孙节点），可以调用contents 属性（和直接选择不同的是，contents返回的列表包含了所有子节点和所有文本信息） 关联选择：我们在做选择的时候有时候不能做到一步就可以选择到想要的节点元素，有时候在选择的时候需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等等。这里列出了获取节点的各个方法： Children 查询某个节点的直接子节点（返回一个迭代器） Descendants 查询某个节点的所有子孙节点（返回生成器） parent 查询某个节点的元素的父节点 Parents 查询某个节点的元素的祖父节点（返回生成器） soup.a.next_sibling 查询某个节点的下一个节点 previous_sibling 查询某个节点的上一个节点 next_siblings 和 previous_siblings 分别返回所有前面和后面的兄弟节点的生成器。 需要注意的是：上面这些节点选择，但凡涉及到多个节点内容，就会返回一个生成器类型。 关于这些节点选择，我们要注意 html 树型结构的排列关系，例如，我们不能跨越父级节点，去拿到另一个子孙节点的内容。 方法选择器前面我们所讲的选择方法都是通过属性来选择元素的，这种选择方法非常快，但是如果要进行比较复杂的选择的话则会比较繁琐，不够灵活。所以 BeautifulSoup 还为我们提供了一些查询的方法，这里介绍非常常用的 find_all()、find() 等方法，我们可以调用方法然后传入相应等参数就可以灵活地进行查询。 find_all( )查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，并使用一个列表进行返回。 我们接下来针对它的 API 下的属性进行介绍 ①name：根据标签名选择节点 方法选择器会返回一个Tag标签的类型，也就是说，其支持嵌套获取元素、层层迭代使用find_all等等操作。 当name=True时，匹配所有标签。 ② attrs / id ： 可以通过这些attrs、id 这两个关键字对我们需要的节点传入进行匹配 ③recursive：是否对子孙全部检索，默认为True，改为False则忽略孙节点 ④ text：用来匹配节点的文本，文本参数 text是用标签的文本内容去匹配，而不是用标签的属性。传入的形式可以是文本字符串，也可以是正则表达式对象（compile） 也可以使用text = “Hello，this is a link” 这样的形式进行匹配节点。 find( ) find() 方法返回的是单个元素，也就是第一个匹配的元素。类型依然是 Tag 类型。（当某个标签只有一个是使用find比较好，因为find返回一个Tag类型，而find_all返回一个列表，这是最大的区别了，有时候我们需要拿到Tag类型的节点，这样对我们的后续操作更为便利），跟find_all很接近，API都是相同. 获取文本经过节点选择，我们已经匹配到我们需要的节点了，接下来应该是获取文本内容了，通过Tag 类型的string 属性，我们可以拿到文本内容，有一个方法那就是 get_text（），同样可以获取文本值。 只需基于 Tag类型，调用 get_text（） 方法即可。不过需要注意的是： .get_text（）会把你正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如你正在处理一个包含许多超链接、段落和标签的大段源代码，那么 .get_text() 会把这些超链接、段落和标签都清除掉，只剩下一串不带标签的文字。 所以，get_text（）方法一般留到最后面使用，不然会影响我们获取节点 写在最后： 说一下节点选择的建议：页面布局总是不断变化的。一个标签这次是在表格中第一行的位置，没准儿哪天就在第二行或第三行了。如果想让你的爬虫更稳定，最好还是让标签的选择更加具体。如果有属性，就利用标签的属性。 解析库呢，就像一把锤子，使用的是我们，我们学会如何使用一把锤子的同时呢，也要关注什么时候去使用它，不要手上握着一把锤子，看任何问题都是钉子。使用适当的解析库去拿到我们需要的内容才是我们应该做的，使用什么样的锤子并不重要。 不经意间，爬虫专栏已经走到的Day6了，过阵子应该会更新 爬取网站的实战。还是那句话，学理论归理论，但只有实战才能使我们将其内化，成为我们真正的技能。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day5-requests介绍]]></title>
    <url>%2F2019%2F07%2F19%2F%E7%88%AC%E8%99%ABDay5-requests%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Requests的基本使用我们知道在 Urllib 库中有 urlopen() 的方法，实际上它是以 GET 方式请求了一个网页。那么在 Requests 中，相应的方法就是 get() 方法，相比较方便（使用什么请求方法就用对应的方法名封装的API） GET 请求下面是使用requests库发出的简单get请求，是不是相对于urllib，步骤变简单了很多呢？嘿嘿~ 响应内容的类型是type，我们可以基于这个对象，使用text方法获取文本，也可以调整编码格式等等。 响应状态码封装在status_code这个属性中，200表示正常访问。 模拟其他请求接下来，在这里分别用post()、put()、delete() 等方法实现了 POST、PUT、DELETE 等请求。requests这些方法时是直接使用的（不像urllib那么隐晦） 附加信息我们知道，使用百度这样的搜素引擎搜索关键字时，会在url生成类似这样的url，比如我们现在搜索的关键词是python，会生成下面这条url——“http://www.baidu.com/s?wd=python” 那么，在requests中，如何模拟这样的关键词查询呢？ 回答：将信息数据存储到字典中，然后利用params这个参数进行传递。 我们可以看到，字典的键值被传递到url中。 注意字典里值为None 的键都不会被添加到 URL 的查询字符串里。 定制请求头 注意：定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。 如果被重定向到别的主机，授权 header 就会被删除。 代理授权 header 会被 URL 中提供的代理身份覆盖掉。 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。 更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 二进制流数据上面例子中，实际上它返回的是一个 HTML 文档，那么如果我们想抓去图片、音频、视频等文件的话应该怎么办呢？我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，所以就需要拿到他们的二进制码。 需要记住的是：（用什么保存格式编码就要用对应的解析方式解码） 前两行便是 r.text 的结果，最后一行是 r.content 的结果。 前者出现了乱码，由于图片是二进制数据，所以前者在打印时转化为 str 类型（默认的编码格式），也就是图片直接转化为字符串，理所当然会出现乱码； 而后者结果前面带有一个 b，代表这是 bytes 类型的数据。 得出结论： r.test返回的是字符串类型，如果返回结果是文本文件，那么用这种方式直接获取其内容即可。 如果返回结果是图片、音频、视频等文件，应该使用r.content，Requests 会为我们自动解码成 bytes 类型，即获取字节流数据。此时，我们就可以通过写入文件（注意编码格式是wb），将其保存为照片格式。 关于编码：编码可谓是蛰伏在黑暗中的精灵，如果你不搞懂他，它就会偶尔出来耍一下脾气… Requests 会自动解码来自服务器的内容 。 大多数 unicode 字符集都能被无缝地解码 。 请求发出后 ， Requests 会基于 HTTP 头部对响应的编码作出有根据的推测 。 当你访问 r.text 时 ， Requests 会使用其推测的文本编码 。 不过你也可以找出 Requests 使 用 了 什 么 编 码 ， 并 且 能 够 使 用 r.encoding来更改编码方式。如果你改变了编码，每当你访问r.text，Request都会使用r.encoding的新值。 不过每一次设置r.encoding都需要去浏览器中查看相应字符集，这很麻烦。我们可以这样写： 1r.encoding = r.apprent_encoding # apprent_encoding是在响应报文的头部获取字符集，这样设置比较灵活 post 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典（也可以是个元组）给 data 参数。你的数据字典在发出请求时会自动编码为表单形式 是不是觉得好简单？哈哈~ 这就是requests向我们提供的简单接口，然后我们直接拿来调用就行了！ Request的高级用法文件上传我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传。使用files关键字传递文件，就可以达到我们想要的效果。 content-type的值是multipart/from-data，表示是以表单数据提交的，证明请求方法为post 发送到网页，需要以字节流形式传输，所以我们打开文件时必须指定为“wb” 最后使用files关键字指定传入文件，文件需以字典形式传入（键名是什么不重要） 设置Cookies在前面我们使用了 Urllib 处理过 Cookies，写法比较复杂，而有了 Requests，获取和设置Cookies 就比较方便快捷了。而且requests有两种设置方法，我们一起来看看吧~ ① 在请求报文头中设置 ② 通过cookies参数进行传递 代理设置对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会提出登录验证、验证码，甚至直接把IP给封禁掉。（我就曾使用selenium对淘宝网进行爬取，结果没爬几页，发现被淘宝反爬虫给识别出来了，这就很尴尬了…） 那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在 Requests 中需要用到 proxies 这个参数。 超时设置在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能会收到一个响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，我们应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错，或者谁用异常处理来处理这类事情。 设置超时时间需要用到 timeout 参数（默认是None）。这个时间的计算是发出 Request 到服务器返回Response 的时间。 上面就是requests的简单介绍了，是不是发现功能都被封装称为各个API中，我们只需要进行调用就好了，有没有像我当初一样，被requests这个库的魅力所感染到了呢？]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识扫盲篇]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2%E7%AF%87%2F</url>
    <content type="text"><![CDATA[计算机组成基础谈谈汇编语言：机器语言——“计算机运行的基础” 机器语言是机器指令的集合，机器指令展开来讲就是一台机器可以正确执行的命令。电子计算机的机器指令是一列二进制数字。计算机将之转变为一列高低电平，以使计算机的电子器件受到驱动，进行运算。 汇编语言的诞生： 汇编语言是直接在硬件上工作的编程语言，用来替代机器语言麻烦的编程问题（难以辨别和记忆）。 汇编语言的主题是汇编指令，汇编指令和机器指令的差别在于指令的表示方法上，汇编指令是机器指令便于记忆的书写格式 编译器： 但是计算机能读懂的只有机器指令，那么如何让计算机执行程序员用汇编语言编写的程序呢？ 这时，一种能够将汇编语言转成机器指令的翻译程序——编译器 产生了。 #### 汇编语言的组成： 汇编指令：有对应的机器码，决定了汇编语言的特性 伪指令：没有对应的机器码，由编译器执行，计算机并不执行 其他符号：如+、-、*、/ 等，由编译器识别，也没有对应的机器码 计算机组成的各个部分 存储器： CPU是计算机的核心部件，它控制整个计算机的运作，要想让CPU工作，就必须向它提供指令和数据。指令和数据在存储器中存放，也就是平时说的内存。 离开了内存，CPU也无法工作。 磁盘是计算机永久性存储数据的空间所在，磁盘上的数据要想被**CPU**执行调用，必须先读到内存当中。 指令和数据： 指令和数据是应用上的概念，在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。 CPU在工作时，把有的信息看做指令，有的信息看做数据。（有特殊意义的二进制信息视为指令） 存储单元： 存储器又被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127。 微型机存储器的每一个存储单元可以存储一个字节，即8个bit（电子计算机的最小信息单位）。 也就是说，一个存储器有128个存储单元，它可以存储128个字节。 微机存储器的容量是以字节为最小单位来计算的。 CPU对存储器的读写： 存储器被划分成多个存储单元，存储单元从零开始顺序编号，这些编号可以看作存储单元在存储器中的地址，就像每个房子的门牌号 CPU要从内容中读取数据，首先要指定存储单元的地址，也就是说它要先确定读取哪一个存储单元中的数据。 另外，在微机中，不当当只有存储器这一种器件，CPU在进行读写数据操作时，需要指明它要对哪一个器件进行操作、进行哪种操作、是从中读出数据，还是写进数据。 可见，CPU要想进行数据的读写，必须和外部器件（可以说成芯片）进行三类信息的交互： 存储单元的地址（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据） 那么CPU是通过什么介质将这些信息传到存储器芯片中呢？ 电子计算机能处理、传输的信息都是电信号，电信号当然要用导线传送。在计算机中，专门连接CPU和其他芯片的导线，称为总线。 从物理上来讲，总线是一根根导线的集合，根据传送信息的不同，又从逻辑上分为三类、即地址总线、控制总线和数据总线。 总线 地址总线： CPU是通过地址总线来指定存储器单元的。地址总线上能传送多少个不同的信息，CPU就可以对多少个存储单元进行寻址。 在电子计算机中，一根导线可以传送的稳定状态有两种，高电平或是低电平。用二进制表示就是1或者0。 有 n 根导线就能表示 2的n次方种信息。假设一个CPU有N跟地址线，则可以说这个CPU的地址总线的宽度为N，这样的CPU最多可以寻找2的N次方个内存单元。 数据总线： CPU与内存或其他器件之间的数据交互是通过数据总线进行传递的。 数据总线的宽度决定了CPU与外界的数据传输速度。8跟数据总线一次可传送一个8位二进制数据（即一个字节）。 控制总线： CPU对外部器件的控制是通过控制总线来进行的。 控制总线是一些不同控制线的集合，有多少根控制总线，就意味着CPU提供了对外部器件的多少种控制，所以，控制总线的宽度决定了CPU对外部器件的控制能力。 内存地址空间： 举例来说，一个CPU的地址宽度为10，那么可寻址1024个内存单元，这1024个可寻址的内存单元就构成了这个CPU的内存地址空间。CPU在操纵和控制各类存储器时，都把他们当作内存来对待，把它们总的看做一个由若干存储单元组成的逻辑存储器，这个逻辑存储器就是我们所说的内存地址空间。 主板： 每一台PC机中，都会有一个主板，主板上的器件通过总线相连。 这些器件有：CPU、内存、外围芯片组、扩展插槽等。扩展插槽一般插有RAM内存条和各类接口卡. 接口卡： 计算机系统中，所有可用程序控制其工作的设备，都归CPU控制。 CPU对于外部设备如显示器、音箱等，是通过控制这些设备直接相连的接口卡，从而实现间接的控制这些外部设备的。简单来说，CPU通过总线向接口卡发送命令，接口卡根据CPU的命令控制外设进行工作。 各类存储器芯片： 一台PC机中，装有多个存储器芯片。不同的器件，从读写属性上分为两大类： 随机存储器（RAM）和只读存储器（ROM） 随机存储器可读可写，但必须带电存储，关机后存储的内容丢失； 只读存储器只能读取不能写入，关机后内容也不会消失。 这些存储器从功能和连接上又可分为以下几类： 随机存储器：用于存放供CPU使用的绝大部分程序和数据，主随机存储器一般由两个位置上的RAM组成，装在主板上的RAM和插在插槽上的RAM 装有BIOS（基本输入输出系统）的ROM：BIOS是由主板和给某些接口卡（如：显卡、网卡等）厂商提供的软件系统。 接口卡上的RAM：某些接口卡需要大批量输入输出数据进行暂时存储，因此会在其上装有RAM。最为经典的是显示卡上的RAM，一般称为显存。我们将需要显示的内容写入显存，就会出现在显示器上。 **** 以上就是总结的部分计算机基础知识。我们下篇见，see you ~]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程新手的必修课]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BC%96%E7%A8%8B%E6%96%B0%E6%89%8B%E7%9A%84%E5%BF%85%E4%BF%AE%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[论一名计算机专业学生的修养从事计算机行业，私以为，是最为公平且投入风险小的一件事。不像当今社会同样红火的金融行业，需要人脉与资金等铺垫。我们听过不少这样的话——“只要你们用心学，不要放弃，就一定能学会编程”。 刚开学，学习安排的课程，发几本计算机专业的教材，按时上课，老师在那念念PPT，你若认真听了你可能懂了但是发现不会写，你若没听玩手机了你是既不懂，也不会写，蒙在鼓里的人出不来，水平就一直这样龟速增长。 有的学生学的一头雾水开始怀疑自己准备转专业放弃计算机；有的学生学懂写程序一直报错开始怀疑自己；有的学生觉得教材写得不好去书店转了一圈，买几本语言的书但最后厚厚的灰坐盖在了书上，再也没翻过。 以上这几种学生，若没有领悟到计算机的真正奥妙，最终都要凉凉。 为什么会这样呢？ 是啊，我们有没有反省过，究竟是环境问题，还是自驱动力的不足？ 努力：计算机虽然是目前最火的专业，但若人人都想分一杯羹，不付出实际行动，一切都是空谈。 视野：视野决定高度，有句话说，站在风口上，猪都能飞来。 职业规划： 计算机虽然被统称为计算机专业，但其包含的方向数不胜数，方向选择是很重要的。拥有一份明确的职业规划，绝对让你披荆斩棘，一路高歌。 接下来我们围绕这些观点，展开描述： 投入努力：在计算机专业，有一点是不容质疑的，那就是—— “付出 = 实际回报”。有的同学，因为上课听不懂，就觉得计算机很难，那我更要打击一下他了。课堂所学，只是计算机最最基础的东西，就算你课堂上听懂了，那些知识也无法支撑你拿到满意的工作。你之所以觉得听不懂，你要清楚是因为天赋不行还是投入时间不足，这一点自己最为清楚。 这是一门全靠自学为主的学科，只要你用心付出，就会领先于其他人。 视野的打开网上资源一抓一大把，学会如何科学上网，等于坐拥了这些资源，对于计算机专业的人来说，更是一笔最为宝贵的财富，让遇到问题Google，而不是百度成为习惯，进入这个世界编程大社区，哪怕什么都不懂，你也能保证所在的社区，就是世界程序员的大家庭，当你走进Github，看着各种有趣的项目的时候，相信我，你的视野就会在此为起点，快速打开并不断增长，进入一个良性循环。 有的人会问，学会上网不是一名计算机专业的人的必修课吗？有必要再三强调吗？ 是的，这句话没错，但是有没有想过，身为当初萌新的你，对着网上庞大的资源，会不会感到手足无措呢？对着全英文文档的时候，会不会感到一丝恐惧？有多少学生，连stackoverflow都不知道是什么。 视野是很重要的一点，视野打不开，一身本领就会被扼杀在摇篮中。 职业规划：现互联网处于蓬勃发展的阶段，更是等着我们这一代人去带动他，我能保证，互联网的红火还能至少维持二十年。且现在的红利虽不如几年前那么丰富，但现在也是不容小觑的。我们经常听到过，穷人家的孩子学计算机，富人家的孩子学金融就是这么来的。如果已经就读计算机专业，那么恭喜你，已经处于风口上，接下来就看你能不能把握住了。 这里引用一张知乎上k点赞的图片，里面所讲述的，描述了计算机行业中多个方向的选择，我们可以从中挑选出适合我们自己的。 写在最后到了大学，请不要再有“学生思维”，碰到问题时不要立马请教他人，解决问题的过程就是你进步的一种。而且这种解决问题的能力，更是会伴随我们整个职业发展。可以说，这种技能胜过于任何一个知识点，也是自学过程中必须掌握的。 上面这些，是我的这一年来的心得体会，希望能帮到需要的人。]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day4-urllib实战]]></title>
    <url>%2F2019%2F07%2F15%2F%E7%88%AC%E8%99%ABDay4-urllib%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[urllib爬虫实战之爬取笔趣阁小说前言上一节我们介绍了urllib这个基本库，它的内容还是蛮多的，要想熟练掌握爬虫，必须配合实战进行巩固。一开始小伙伴们可能会觉得无从下手，特别是解析响应内容这一步。这一次，作者将会从小白的角度，讲述如何对网页进行爬取、解析。 环境运行平台： WindowsPython版本： Python3.xIDE： Pycharm BeautifulSoup简介 BeautifulSoup 就是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据。BeautifulSoup提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据。 解析器 BeautifulSoup 在解析的时候实际上是依赖于解析器的，它除了支持 Python 标准库中的 HTML 解析器，还支持一些第三方的解析器比如 LXML 。 Beautiful方法有两个参数，第一个参数传入待解析的文本，第二个参数指定解析器，如果我们想要使用 LXML 这个解析器，在初始化 BeautifulSoup 的时候指定 lxml 即可，如下： 好了，简单介绍了BeautifulSoup，我们快点进入今天的实战环节吧~ 实战 背景介绍：笔趣看是一个盗版小说网站，这里有很多起点中文网的小说，该网站小说的更新速度稍滞后于起点中文网正版小说的更新速度。并且该网站只支持在线浏览，不支持小说打包下载。因此，本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说。 PS：本实例仅为交流学习，支持耳根大大，请上起点中文网订阅。 URL: ‘https://www.biqukan.com/1_1094/5403177.html&#39; 预备知识： 本文提及的代码并不复杂，如果对BeautifulSoup想要更加了解，可以去官方文档进行翻阅 URL：‘http://beautifulsoup.readthedocs.io/zh_CN/latest/‘ BeautifulSoup环境搭建： pip install bs4 导入使用 – from bs4 import BeautifulSoup 获取内容① 打开url链接，按F12或者右键- 检查 进入开发者工具 ② 在开发者工具中，捕获我们要找到的请求条目信息 将正文中部分内容进行复制 在开发者工具中，使用 ctrl + f 进入全局搜索框，将复制内容进行粘贴 然后会在下方得到条目信息，点击，页面会跳转到加载正文的请求响应条目中。 分析发现，正文部分存储在 id 为 content 和 class 为 showtxt 的 div 中。这就是我们解析网页得到的重要信息啦 ③ 构造url请求 有了上面的信息还不够，现在的网站都有了反爬能力，我们需要模拟一条正常从浏览器中发出的url请求链接。 例如： User-Agent（浏览器标识）、Cookies（标识客户端身份的“钥匙”）、referer等等字段 那么，这些包含在请求头的字段信息，我们应该如何构造呢? 还是开发者工具，点击Headers，就可以看到Request-Response条目明细。 ④ 发出请求： 有了字段的详细内容，我们就可以编写出请求网页的代码… 12345678import urllib.requestdef get_page(url): headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0&apos;,&apos;Cookie&apos;:&apos;UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333&apos;&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() print(html) return html ⑤ 获得相应内容：运行，得到内容如下: 发现是一个二进制流的数据，因此得知，我们在解析时，需要编码 解析响应数据接下来，我们就可以使用BeautifulSoup进行解析 12345678910def parser_page(html): soup = BeautifulSoup(html,&apos;lxml&apos;,from_encoding=&apos;utf-8&apos;) # 传入编码方式，为utf-8 text = soup.find(&apos;div&apos;, class_=&apos;showtxt&apos;) # 使用BeautifulSoup的find方法，匹配符合的节点，该节点为我们上文得知的属性 cont = text.get_text() # 输出文本内容 cont = str(cont).split() # 将空白符进行分割，这里的作用是消除 \xa0、\u3000空白符 print(cont) 运行….代码结果如图： 永久存储数据：我们这里使用最简单的方式，存储写入本地文件 打开result.txt文件，内容如下（完成！） 写在最后本次我们实战项目就告一段落啦，最后再为大家做一次梳理总结。 观察网页信息，获取相应内容 构造get请求，拿到响应内容 用解析库对相应内容进行解析 写入文件 全文代码如下： 12345678910111213141516171819202122232425262728import urllib.requestfrom bs4 import BeautifulSoupurl = &apos;https://www.biqukan.com/1_1094/5403177.html&apos;def get_page(url): headers = &#123;&apos;User-Agent&apos;:&apos;Mozilla/5.0&apos;,&apos;Cookie&apos;:&apos;UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333&apos;&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() return htmldef parser_page(html): soup = BeautifulSoup(html,&apos;lxml&apos;,from_encoding=&apos;utf-8&apos;) text = soup.find(&apos;div&apos;, class_=&apos;showtxt&apos;) cont = text.get_text() cont = str(cont).split() return contif __name__ == &apos;__main__&apos;: html = get_page(url) results = parser_page(html) with open(&apos;result.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;) as f : for result in results: f.write(result) f.write(&apos;\n&apos;) print(&quot;爬取完毕&quot;) 耗费一上午时间，终于把这篇文章给赶出来了~ 爬虫Day系列会持续更新，让我们共同期待吧！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day3-urllib介绍]]></title>
    <url>%2F2019%2F07%2F10%2F%E7%88%AC%E8%99%ABDay3-urllib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Urllib介绍在 Python2 版本中，有 Urllib 和 Urlib2 两个库可以用来实现Request的发送。而在 Python3 中，已经不存在Urllib2 这个库了，统一为 Urllib urllib分为四个模块： 第一个模块是request，它是最基本的HTTP请求模块，我们可以用它模拟发送请求，就像在浏览器手动输入网址然后敲击回车一样，只需要给库方法传入URL，以及额外的参数，就可以模拟实现整个过程了。 第二个模块是error，即异常处理，如果出现请求错误，我们可以捕获这个异常，然后进行异常的处理，保证程序的稳定性 第三个模块是paser，它是一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等操作 第四个模块是robotparser，主要是用来识别网站的robots.txt的（反爬虫的文本说明） 下面作者就对以上模块进行重点讲解~ request模块1.urlopen方法： 我们知道，request是HTTP请求模块，那么具体的请求方法是什么呢？没错，正是urlopen！ 12urlopen的API：urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 我们通过urllib.request.urlopen（“url链接”）就可以请求到网页，如下图所示 这个请求的返回结果response，就是服务器响应内容。紧接着，我们可以调用这个对象包含的方法对返回的内容进行处理。 响应的内容是HTML纯文本，它以字节流的形式返回的，于是要解码成utf-8，得到字符串 response.read（）读取响应内容到控制台 respon.status返回网页的状态码，如果网页的状态码是200，则表示正常 利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取。 2.data参数 data 参数是可选的，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。我们就可以使传递data参数，完成模拟登陆等类似功能… 具体如何模拟from表单，后文的urllib.parser会讲解 3. timeout参数 timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。 我们可以通过设置timeout值，从而缩短超时时间。或者是让程序不会一直处于阻塞状态（我们知道，客户端和服务器的交互是一个同步过程，我们可以利用timeout参数，以及异常处理来完成非阻塞的效果，以及让程序不会异常终止，让程序更加稳定） 4.构建Request对象 我们知道利用 urlopen()方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。比如可以构建请求链接的浏览器标识等等 下面是正常的urlopen请求和使用Request构建URL的代码举例： 可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，这样的好处是： 一方面我们可以将请求独立成一个对象 另一方面可配置参数更加丰富和灵活（将参数全部加到Ruquest对象中作为一个整体发送） Request配置举例 之前我们说到过，可以构建url链接，比如关键字查询、模拟登陆。还可以模拟浏览器标识，让爬虫更像是一个人为发出的请求信息。具体内容如下： headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。 data就是我们发出的数据了，他会以表单形式向服务器发出 method是方法的意思，我们可以指定method来声明这是一个什么类型的URL Error模块前文我们了解了 Request 的发送过程，但是在网络情况不好、被反爬的情况下，出现了异常怎么办呢？这时如果我们不处理这些异常，程序很可能报错而终止运行，所以异常处理还是十分有必要的。 Urllib 的 error 模块定义了由request 模块产生的异常。如果出现了问题，request 模块便会抛出error 模块中定义的异常 1.URLError URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。它具有一个属性 reason，即返回错误的原因。 2.HTTPError 它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。 它有三个属性。 code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。 reason，同父类URLError一样，返回错误的原因。(有时候 reason 属性返回的不一定是字符串，也可能是一个对象—可以用 isinstance() 方法来判断它的类型，做出更详细的异常判断。) headers，返回 Request Headers。 因为 URLError是HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，这样捕获的代码更加精准，且有利于程序猿处理。 请求到响应信息不足为奇，也不是什么难事，最主要的，是让程序可以正常的”跑起来”，所以，异常处理是爬虫工程师的必修课，需好好掌握。 parse模块1.urlparse（） urlparse() 方法可以实现 URL 的识别和分段,返回六个部分的内容，字段分别是 scheme、netloc、path、params、query、fragment。 2.urlencode（） 常用的 urlencode() 方法，它在构造 GET 请求参数的时候非常有用——字典转序列化 3.parse_qs（） 有了序列化必然就有反序列化，如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典 4.quote（） quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码 5.unquote（） quote() 方法的逆操作，它可以进行 URL 解码（默认是以utf-8编码） robotparser在介绍rebotparser模块之前呢，我们需要对robots协议进行了解 Robots 协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。 robotparse：用来解析rebots文件的类 robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。 123接口API：import urllib.robotparserurllib.robotparser.RobotFileParser(url='') 使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。当然也可以声明时不传入，默认为空，再使用 set_url() 方法设置一下也可以。 RobotFileParser常用方法解析 read()，读取 robots.txt 文件并进行分析，注意这个函数是执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法，这个方法不会返回任何内容，但是执行了读取操作。（注意是“读取”而不是读出） parse()，用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。 can_fetch()，方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。（判断是否能抓取的方法） 好了，对urllib库的介绍就到这儿了，下次作者会对第三方请求库requests讲解，这是一个当前的流行库，让我们一起期待吧~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day2_爬虫基础]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%88%AC%E8%99%ABDay2-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[爬虫基础：简单来说，爬虫就是请求网页并提取数据和保存信息的自动化程序。 人们常常把爬虫比做蜘蛛爬网，把网的节点比做一个个网页，爬虫爬到这就相当于访问了该页面获取了其信息，节点间的连线可以比做网页与网页之间的链接关系，这样蜘蛛通过一个节点后可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，这样网站的数据就可以被抓取下来了。 简单流程介绍： 下面我们说说爬虫的具体流程，一般分为四步： 发起请求， 通过请求库对目标站点发起请求，我们可以模拟携带浏览器信息、cookie值、关键字查询等信息，从而访问服务器端，然后等待服务器的响应 解析内容： 服务器响应的内容有可能是HTML纯文本，也有可能是json格式的数据、或者是二进制（如图片视频），我们可以利用相关的解析库进行解析、保存或进一步处理。 处理信息：一般来说，我们使用解析库解析出来的内容，是冗杂晦涩的，我们通过用format或者切片等技术，使得内容清晰明了，这时，才算得上的有用的信息。 接下来，我们可以将信息存储到本地、数据库或者远程服务器等等。 爬虫优化：爬虫优化的方面有很多，例如，控制爬虫速率、使用高匿代理服务器、使用分布式爬取、处理异常… 接下来作者针对爬虫最重要的部分做出详细介绍： 获取网页爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。 在前面我们讲到了 Request 和 Response 的概念，我们向网站的服务器发送一个 Request，返回的 Response 的 Body 便是网页源代码。 所以最关键的部分就是构造一个 Request 并发送给服务器，然后接收到 Response 并将其解析出来。具体相关的库有：urllib（python自带的库）、requests（优雅的第三方库） 提取信息我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。 另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。 能抓取怎样的数据： 常规网页， 也就是html源代码。有些网页返回的不是不是 HTML 代码，而是返回一个 Json 字符串（使用Ajax渲染的网页），API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。 二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。 我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。 JavaScript 渲染页面：有时候我们在用 Urllib 或 Requests 抓取网页时，得到的源代码实际和浏览器中看到的是不一样的。 这个问题是一个非常常见的问题，现在网页越来越多地采用 Ajax、前端模块化工具来构建网页，整个网页可能都是由 JavaScript 渲染出来的，意思就是说原始的 HTML 代码就是一个空壳。 下面，是JS常规的渲染代码： body 节点里面只有一个 id 为 container 的节点，但是注意到在 body 节点后引入了一个 app.js，这个便负责了整个网站的渲染。 在浏览器打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中里面引入了一个 app.js 文件，然后浏览器便会接着去请求这个文件，获取到该文件之后便会执行其中的 JavaScript 代码，而 JavaScript 则会改变 HTML 中的节点，向内添加内容，最后得到完整的页面。 但是在用 Urllib 或 Requests 等库来请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中看到的内容了。 这也解释了为什么有时我们得到的源代码和浏览器中看到的是不一样的。 所以使用基本 HTTP 请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台的Ajax接口，通过模拟Ajax请求得到渲染的页面，也可以使用Selenium这样的库来实现模拟JavaScript。 本期对爬虫基础原理的讲述就到这里了~ 下期预告：“urllib请求库的使用”]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day1-网页基础]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%88%AC%E8%99%ABDay1-%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[网页基础网页交互过程： URL：统一资源定位符，通俗的讲，它是一个链接，它能使我们从互联网上找到我们访问的资源。 例如： http://www.4399.com/flash/58508.htm 这样的一个网站 它包含三个部分： 访问协议 http 访问路径 “www.4399.com” 资源名称 “flash/58508” 我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。实际上这就是和服务器做交互行为， 通过发送请求，再到服务器响应服务，这就完成了我们获取资源的目的。 客户端请求–Request： Request，即请求，由客户端向服务端发出。可以将Request分为四部分： 请求方法（Request Method） 请求链接（Request URL） 请求头（Request Headers） 请求体（Request Body） 接下来，我们就分别来谈谈这四个部分的具体内容… Request Method请求方式，请求方式常见的有两种类型，GET 和 POST。 Get： 我们在浏览器中直接输入一个URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里（GET 方式请求提交的数据最多只有 1024 字节） Post： Post 请求大多为表单提交发起，如一个登录表单，输入用户名密码，点击登录按钮，这通常会发起一个 POST 请求，其数据通常以 Form Data 即表单的形式传输，不会体现在 URL 中。（POST 方式请求提交的数据没有限制） 而且Post请求在Request Headers中的Content-Type标识——只有设置为application/x-www-form-urlencoded 才是Post请求。 其他请求方式 我们平常遇到的绝大部分请求都是GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下： Request URL：顾名思义，就是请求的链接网址，我们发起的请求，必须包含URL Request Headers：请求头，用来说明客户端要使用的附加信息，比较重要的信息有：Cookie，Referer、User-Agent 以及Content-Type等等。 Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。 Request Body：请求体，一般承载的内容是Post请求的Form Data，即表单数据，而对于Get请求中，Request Body的内容为空。 服务器响应–Response Response，即响应，由服务端返回给客户端。 Response 可以划分为三部分，Response Status Code、Response Headers、Response Body。 Response Status Code：响应状态码，此状态码表示了服务器的响应状态，如 200则代表服务器正常响应，404 则代表页面未找到，500则代表服务器内部发生错误。 Response Headers：响应头，其中包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等，下面对一些常用的头信息说明： Date：日期，标识 Response 产生的时间。 Last-Modified，指定资源的最后修改时间。 Content-Encoding，指定 Response 内容的编码。 Server，包含了服务器的信息，名称，版本号等。 Content-Type，文档类型，指定了返回的数据类型是什么，如text/html 则代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回了图片。 Set-Cookie，设置Cookie，Response Headers 中的 Set-Cookie即告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。 Response Body：响应体，响应的正文数据都存放于此，如果请求一个网页，它的响应体就是网页的HTML代码，或者是Json格式的数据；请求一张图片，它的响应体就是图片的二进制数据。 爬虫的解析都是根据响应体进行操作的。 好了，以上就是简单的讲解了爬虫相关的网页基础内容，下期作者会讲解爬虫的原理基础，以及实现过程。Bye~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>技术</tag>
      </tags>
  </entry>
</search>
