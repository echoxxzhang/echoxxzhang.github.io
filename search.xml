<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux之文件目录管理命令]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[touch 命令touch命令用于创建空白文件或设置文件的时间，格式为“touch [选项] [文件]”。 如果创建的文件已存在，并不会进行覆盖操作。 Touch 还可以修改属性：比如修改设置文件内容的修改时间（mtime）、文件权限或属性的更改时间（ctime）与文件的读取时间（atime）。 选项： 实例： 使用 echo 输出了 “hello world” 追加到 new_log 文件中，会让文件时间发生改变。 然后再使用 touch -d 命令将文件改为原来的时间，别人就看不出来你修改过文件。 mkdir 命令mkdir命令用于创建空白的目录，格式为“mkdir [选项] 目录”。 在Linux系统中，文件夹是最常见的文件类型之一。除了能创建单个空白目录外，mkdir 命令还可以结合 -p 参数来递归创建出具有嵌套叠层关系的文件目录。 实例： 嵌套叠层关系的文件目录类似树型结构的问题，如果某一段树枝还没“长”出来，那么就无法生出树叶。 但是使用了 -p 后，如果检测到 某个文件还未创建，它会自动帮你创建，直到长出“树叶”。 cp 命令cp命令用于复制文件或目录，格式为“cp [选项] 源文件 目标文件”。大家对文件复制操作应该不陌生，在Linux系统中，复制操作具体分为3种情况： 如果目标文件是目录，则会把源文件复制到该目录中； 如果目标文件也是普通文件，则会询问是否要覆盖它； 如果目标文件不存在，则执行正常的复制操作。 选项： mv 命令mv命令用于剪切文件或将文件重命名，格式为“mv [选项] 源文件 [目标路径|目标文件名]”。 剪切操作不同于复制操作，因为它会默认把源文件删除掉，只保留剪切后的文件。如果在同一个目录中对一个文件进行剪切操作，其实也就是对其进行重命名： 可以看到，当前目录已经没有 old_log ，new_log 已经取而代之了。 rm 命令rm命令用于删除文件或目录，格式为“rm [选项] 文件”。 在Linux系统中删除文件时，系统会默认向您询问是否要执行删除操作，如果不想总是看到这种反复的确认信息，可在rm命令后跟上-f参数来强制删除。另外，想要删除一个目录，需要在 rm 命令后面加一个 -f 参数才可以，否则删除不掉。 可以看到，如果不带 -r 参数，如果删除的东西是个目录，它会提示无法删除目录的信息 如果删除的是个目录，那么里面的所有内容都会跟着删除，使用这个 rm 命令要慎用。 file 命令file命令用于查看文件的类型，格式为“file 文件名”。 在Linux系统中，文本、目录、设备等所有这些一切都统称为文件，而我们又不能单凭后缀就知道具体的文件类型，这时就需要使用file命令来查看文件类型了。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之打包压缩与搜索命令]]></title>
    <url>%2F2019%2F08%2F26%2FLinux%E4%B9%8B%E6%89%93%E5%8C%85%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%90%9C%E7%B4%A2%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[tar 命令tar命令用于对文件进行打包或解包，格式为“tar [选项] [文件] [可选：目标文件]”。 在Linux系统中，常见的文件格式比较多，其中主要使用的是.tar或.tar.gz或.tar.bz2格式，不过不用担心格式太多而记不住，其实这些格式大部分都是由tar命令来生成的。 选项： -c参数用于创建打包文件，-x参数用于解包文件，因此这两个参数不能同时使用。 -z参数指定使用Gzip格式来打包或解包文件，-j参数指定使用bzip2格式来打包或解包文件。 用户使用解压时则是根据文件的后缀来自动决定应使用何种格式参数进行解压。 -f 代表要打包或解包的软件包名称。（后面跟文件名，严格来说-f参数应该放到最后面） 实例： 使用 “tar -czvf 压缩包名称 .tar.gz 要打包的目录” 相应的解压命令为 “tar -xzvf 压缩包名称.tar.gz” 压缩压缩：将一个大的文件通过一些压缩算法得到一个小文件。（主要是缩小文件的大小，节省磁盘空间，便于网络传输） gzip：将后面的文件进行压缩 gunzip：将后面的gz压缩包进行解压缩 grep 命令grep命令用于在文本中执行关键词搜索，并显示匹配的结果，格式为“grep [过滤参数] [目标文件]”。 选项： 比较常用的参数： -n参数用来显示搜索到信息的行号； -v参数用于反选信息（即没有包含关键词的所有信息行）。 实例： ps -aux 输出所有进程 | 称为管道符，可以把前一个命令原本要输出到屏幕的标准正常数据当作是后一个命令的标准输入 以上命令的意思：将ps -aux 查看到的内容交给 grep 进行过滤，将 Firefox 的相关进程过滤出来。 find 命令find命令用于按照指定条件来查找文件，格式为“find [匹配参数] [寻找条件后执行的操作]”。 “Linux系统中的一切都是文件”。在Linux系统中，搜索工作一般都是通过find命令来完成的，它可以使用不同的文件特性作为寻找条件（如文件名、大小、修改时间、权限等信息），一旦匹配成功则默认将信息显示到屏幕上。 匹配参数： 实例： 使用 -name 匹配名称，搜索名叫 new_log 的文件 可以看到终端输出了 “./new_log”，也就是说，这个文件是在当前目录。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之文件型相关命令]]></title>
    <url>%2F2019%2F08%2F26%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言Linux系列文章的相关内容大多数都是一些我所记笔记的输出，同时也是对一些命令的心得，因此是纯技术文章。 话不多说，进入今天的主题！ 文件型相关命令pwd 命令pwd命令用于显示用户当前所处的工作目录，格式为“pwd [选项]”。 工作目录指的是用户当前在系统中所处的位置。 选项： -L 目录连接链接时，输出连接路径 -P 输出物理路径 cd 命令cd命令用于切换工作路径，格式为“cd [目录名称]”。 这个命令应该是最常用的一个Linux命令了。可以通过cd命令迅速、灵活地切换到不同的工作目录。 所谓的[目录名称] 还有两种路径区分： 绝对路径/相对路径： 绝对路径：必须以斜杆开始，绝对路径包括从文件系统的根节点开始要查找的对象（目录或文件），所必须遍历的每一个目录的名字，它是文件位置的完整路标，因此在任何情况下都可以使用绝对路径找到所需文件。 相对路径：不是以斜杆开始，相对路径可以包含从当前目录到要查找的对象（目录或文件）所必需遍历的每一个目录的名字。 实例演示： 在Linux系统中，.. 表示父级目录， . 表示当前目录（这一点和Windows一致） 于是 “cd ..” 就是返回到上层目录。 ls 命令ls命令用于显示目录中的文件信息，格式为“ls [选项] [文件] ”。所处的工作目录不同，当前工作目录下的文件肯定也不同。 使用ls命令的“-a”参数看到全部文件（包括隐藏文件），使用“-l”参数可以查看文件的属性、大小等详细信息。将这两个参数整合之后，再执行 “ls -al” 命令即可查看当前目录中的所有文件并输出这些文件的属性信息： 选项： cat 命令cat命令用于查看纯文本文件（当文件内容较少时），格式为“cat [选项] [文件]”。 如果在查看文本内容时还想顺便显示行号的话，在cat命令后面追加一个-n参数： 为什么建议当查看的文件内容较少时才使用 “cat” 呢？ 因为有更棒的命令啦~ 我们接着往下看！ more 命令more命令用于查看纯文本文件（内容较多的），格式为“more [选项]文件”。 more命令会在最下面使用百分比的形式来提示您已经阅读了多少内容。使用空格键或回车键向下翻页。（按q 退出） tail 命令tail 命令用于查看纯文本文档的后N行，格式为“tail [选项] [文件]”。 假设我们添加了新用户（普通用户UID从1000开始），所以查看新建的用户应该在文档最后方，为避免使用more命令然后一直翻一直翻的情况出现，这时候 tail 就派上用场了。 wc 命令wc命令用于统计指定文本的行数、字数、字节数，格式为“wc [选项] 文本”。 选项： 如果不输入选项，命令 “wc” 显示的内容 默认是 行数、单词数、字节数 diff 命令diff命令用于比较多个文本文件的差异，格式为“diff [参数] 文件”。 在使用diff命令时，不仅可以使用–brief参数来确认两个文件是否不同，还可以使用-c参数来详细比较出多个文件的差异之处，这绝对是判断文件是否被篡改的有力神器。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git入门指引]]></title>
    <url>%2F2019%2F08%2F24%2FGit%E5%85%A5%E9%97%A8%E6%8C%87%E5%BC%95%2F</url>
    <content type="text"><![CDATA[前言Git是目前最先进的版本控制系统，拥有最多的用户数量并管理着数量庞大的实际软件项目；之前在 Github 上慢慢摸索，逐渐了解 Git 这个工具的好处还有实用性。 这篇文章即将介绍相关概念和简单的Git用法。 版本控制工具所谓版本控制，就是对代码修改的记录。 我们知道，Github 上面的项目是开源的，意味着人人都可以对代码进行修改，我们一般把修改的代码提交到分支上面去，当项目的主人——master 看到你提交的代码时，他如果觉得此处修改是有作用的，就可以将分支与主线进行合并。 假设如果这时候，代码合并之后代码逻辑混乱，多个BUG爆出，而又不记得合并之前的代码是如何编写的，他就可以通过版本控制工具，进行版本回退。回到合并代码之前的样子，就避免了源码逻辑错误这种尴尬的事情发生。 还有，对于代码的修改，Git 可以实现每一次修改的记录。当修改多次之后，我们看着版本记录，就很清楚之前做过什么、修改了什么代码。 Git是什么Git 是一个版本控制软件。在进行软件开发时，一个团队的人靠使用Git，就能轻松管理好项目版本，做项目的追踪和辅助进度控制。Git 具有“集中式版本控制“和”分布式版本控制“的特点。 Git是“集中式版本控制“，将代码开源之后，人人都可以从 Git 上面获取代码仓库，就好比这一个团队中，版本库都集中在一台服务器上，每个开发者都要从服务器上获取最新的版本库后才能进行开发，开发完了再把新的版本提交回去。 同时，Git 又是”分布式版本控制“，则是这个团队中每个人的电脑上都会有一份完整的版本库，将源码Fork到本地之后，版本库就在你自己的电脑上。团队之间的成员各自修改了代码逻辑之后，只需把各自的修改推送给对方，就可以互相看到对方的修改了。（多人干活） Git 的使用在实际应用中，Git有非常多的用法，下面举一下简单的例子： 比如，在刚才建好的版本库中，我新建了README文件（项目说明文档）。写好后想给项目做个版本，就需要这样： 12$ git add README$ git commit -m &quot;add README&quot; 第一个命令是告诉 Git 要处理什么操作 第二个命令是进行提交，并对此次提交做个简答说明。然后 Git会自动为此次提交生成一个16进制的版本号。 如果此时查看本地的版本库，就会发现最新的一次提交是在刚才，提交说明为：add README。 分支的概念分支是版本控制里面的一个概念：在项目做大了之后，如果要在原基础上进行扩展开发，最好新建一个分支，以免影响原项目的正常维护，新的分支开发结束后再与原来的项目分支合并。 Git 常用命令：]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之常用系统工作命令]]></title>
    <url>%2F2019%2F08%2F24%2FLinux%E4%B9%8B%E5%B8%B8%E7%94%A8%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言最近开始了 Linux 的学习，打算将每天学习的东西记下来，将知识内化输出。这个系列估计会更新很久，敬请期待~ 我使用的是 Centos 7，是当今主流操作系统。Linux系统是一款优秀的软件产品，具有类似UNIX的程序界面，而且继承了UNIX的稳定性，能够较好地满足工作需求。 为什么学Linux？ 一部分是因为招聘需求大多数有要求是Linux开发；另一部分是因为…windows系统很那啥….嗯你懂的。 还有就是，我觉得使用命令行各种唰唰唰然后调兵遣将的样子很厉害~ 所以， Linux 我来啦！！ 命令的语法格式Linux的命令由三部分组成：“命令 [选项] [参数]” 命令：告诉 操作系统 执行的什么命令 选项：说明命令运行的方式（一般是以 “-”开始的） 参数：说明命令作用于什么（如一个文件、目录、文字等等） 记住这个公式，有利于理解一大串命令的作用。 还有就是，一般在命令最前头，是有用户提示符的，因为 Linux是多用户进行管理（就像多台电脑进行同一台电脑的使用），我们可以使用ctrl + alt +F[1~6] 进行切换用户视角。 那么，所谓的用户提示符是这么规定的： 普通用户提示符：$ root 用户提示户：# root 表示电脑最高权限者，也就是说，对于这台电脑来说，root账号的人就像上帝一般，无所不能。 查看帮助文档这个可谓是学习 Linux 开发人员人手必备的技能了，因为命令那么多，如果不常用，那么就会遗忘。这时候，我们就可以翻阅命令的相关帮助文档，唤起我们的回忆。 查看帮助文档有两个常用方法： ① –help： 例如 free，你如果不知道怎么使用，或者后面携带的选项，那么就使用 “–help” ② man： 一般来讲，使用man命令查看到的帮助内容信息都会很长很多。导致了终端输出的内容无法全部一次性显示出来，那么就涉及到了翻页等技巧，我们看看 man 所包含的常用操作按键及其用途： 使用了 man 命令之后，你就会进入一个类似说明文档的界面中，下面对man命令的帮助信息的结构进行说明： 在学会了如果使用命令之后呢，我们进入这篇文章的主题：常用系统工作的命令 常用系统工作命令echo 命令echo命令用于在终端输出字符串或变量提取后的值，格式为“echo [字符串 | $变量]”。（相当于打印出来） 学过编程的同学，会很容易理解这个命令的作用。就像 python 中的 print 函数嘛， data命令date 命令用于显示及设置系统的时间或日期，格式为“date [选项] [+指定的格式]”。 date 还能自定义时间格式，输入以“+”号开头的参数，即可按照指定格式来输出系统的时间或日期，这样在日常工作时便可以把备份数据的命令与指定格式输出的时间信息结合到一起。（比较常用） 具体使用： 这样一来，我们只需要看一眼文件名称就能大概了解到每个文件的备份时间了。 date 参数说明： 用户相关命令① reboot命令 reboot 命令用于重启系统 ②poweroff命令 oweroff 命令用于关闭系统 该命令与reboot命令相同，会涉及硬件资源的管理权限，因此默认只有root管理员才可以关闭电脑 ③ shutdown命令 用来关机或重启的命令，取决于后面的参数： shutdown -r [now] 重启系统； shutdown -h [now] 关机 shutdown命令的工作原理为：shutdown命令会发送请求给系统的 init 进程将系统调整为合适的运行级别 0 表示关机；6表示重启 ④ logout / exit 命令 用于退出当前用户登陆状态。 ⑤ 用户查看、用户切换 命令 作用 whoami 查看当前正在操作的用户 who 查看当前登陆的用户 su - [usename] 用于直接切换用户 wget命令wget命令用于在终端中下载网络文件，格式为“wget [参数] 下载地址”。 用法举例： 1wget http://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz 这样就通过 wget + [URL能访问到的资源]，实现了本地下载功能。 ps 命令ps命令用于查看系统中的进程状态，格式为“ps [参数]”。有时候，一些服务卡住了，我们可以通过ps 命令进行查看该服务的状态，然后进行下一步的处理。 使用公式： ps -aux Linux系统中时刻运行着许多进程，如果能够合理地管理它们，则可以优化系统的性能。在Linux系统中，有5种常见的进程状态，分别为运行、中断、不可中断、僵死与停止。 R（运行）：进程正在运行或在运行队列中等待。 S（中断）：进程处于休眠中，当某个条件形成后或者接收到信号时，则脱离该 状态。 D（不可中断）：进程不响应系统异步信号，即便用kill命令也不能将其中断。 Z（僵死）：进程已经终止，但进程描述符依然存在, 直到父进程调用wait4()系统函数后将进程释放。 T（停止）：进程收到停止信号后停止运行。 Top命令top命令用于动态地监视进程活动与系统负载等信息。 我们可以将它看作Linux中的“强化版的Windows任务管理器”。 第1行：关于系统的相关信息 第2行：进程的状态，分别对应上面我们说的五个状态，通过数量进行标识 第3行：CPU的使用状况 第4行：内存的使用状况 第5行：虚拟内存的使用状况 pidof 命令pidof命令用于查询某个指定服务进程的PID值，格式为“pidof [参数] [服务名称]”。 每个进程的进程号码值（PID）是唯一的，因此可以通过PID来区分不同的进程。 命令效果 即可得出该服务的进程id（这时候的火狐对应着81904进程号） kill 命令kill命令用于终止某个指定PID的服务进程，格式为“kill [参数] [进程PID]” 命令效果 输入 kill 命令之后，前台运行着的火狐浏览器瞬间就被终结掉了 killall 命令killall 命令用于终止某个指定名称的服务所对应的全部进程，格式为：“killall [参数] [服务名称]” 。 通常来讲，复杂软件的服务程序会有多个进程协同为用户提供服务，如果逐个去结束这些进程会比较麻烦，此时可以使用killall命令来批量结束某个服务程序带有的全部进程，可以将叫做[进程名]的进程全部杀死（不精准） 另一种关闭进程的方法： 如果我们在系统终端中执行一个命令后想立即停止它，可以同时按下Ctrl + C组合键，这样也会立即终止当前命令的进程。 对于 kill 和 killall 的使用建议： 如果知道进程id 使用 kill （这样终结的进程比较精准） 如果知道服务名称，可以使用 killall （有时候会“误伤”，慎用） 写在最后这第一篇关于Linux操作系统的文章就到此结束啦，涉及到知识点还是挺多的，希望读者能好好吸收消化。 对于Linux 命令来说，用熟悉了，自然就记住了，不需要刻意去背的，费时费力的一种行为。 旁友们，下回见~Bye!]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day17-解析库Xpath]]></title>
    <url>%2F2019%2F08%2F22%2F%E7%88%AC%E8%99%ABDay17-%E8%A7%A3%E6%9E%90%E5%BA%93Xpath%2F</url>
    <content type="text"><![CDATA[前言我们在之前介绍如何解析响应数据时讲到了 BeautifulSoup 这个解析库，传送门： 爬虫Day6-Beautiful介绍 但学习之后我们发现，BeautifulSoup 是依赖解析器的，在实际开发环境中，我们常常遇到一些特殊情况，例如编码格式导致解析时发现页面数据缺失等等情景。 这时候，除了更换解析器这个办法之外，我们还可以使用其他的解析库，例如 Xpath。 Xpath介绍XPath，全称 XML Path Language，即 XML 路径语言，它是一门在XML文档中查找信息的语言（XML也是一种标签化语言）。XPath 最初设计是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索。 Python 标准库中自带了 xml 模块，但是性能不够好，而且缺乏一些人性化的 API，相比之下，第三方库 lxml 是用Cython 实现的，而且增加了很多实用的功能，可谓爬虫处理网页数据的一件利器。lxml 大部分功能都存在lxml.etree中。 Xpath 常用规则 每一条 / 都表示一层嵌套关系，我们需要对HTML结构进行一定的了解，使用Xpath 才更加得心应手 // 匹配的节点好比喻成文件系统的绝对路径，如果有相符的节点名称，那么就会被匹配到 @ 这个符号选取属性，我们一般用来获取 URL ，例如：@href 下面列出用法举例： LXML 库 的使用读入HTML 文本1234567891011121314151617from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text) # 初始化生成一个XPath解析对象result = etree.tostring(html)print(type(result)) # bytes类型print(result.decode('utf-8')) # 将二进制内容解码成str类型的字符串 传入 HTML 文本，会自动修正（补齐缺漏的节点标签）且生成一个XPath解析对象，后续的解析都是根据解析对象来调用xpath方法进行节点选择 由于初始化 HTML 文本返回的结果是bytes类型，我们打印出来时，需要转为utf-8。 读入HTML 纯文件1234567from lxml import etreehtml = etree.parse('test.html', etree.HTMLParser())result = etree.tostring(html) # 解析成字节# etree.tolist（html） 解析成列表print(result.decode('utf-8')) test.html是我们创建的html文件，里面存放一些html文本 跟直接读取html文本不同的是，文件读取会多出 DOCTYPE 的声明，但是对内容解析没有影响 tostring 方法 可以实现 将 内容结构化打印出来（比较直观） xpath 选择节点关于怎么选择节点，除了上面讲的 xpath 常用规则之外，还有一些关于 属性匹配、属性获取的使用方法。 123456789101112131415161718192021222324252627from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''# //代表获取子孙节点，*代表获取所有result = html.xpath('//*') # 指定节点名称，比如要获取所有li节点result2 = html.xpath('//li')# 指定li标签下的直接子节点aresult3 = html.xpath('//li/a') # @符号的过滤 选取 class 为 item-1的li节点result5 = html.xpath('//li[@class="item-1"]')# 获取指定节点父节点的类值result4 = html.xpath('//a[@href="link4.html"]/../@class') 属性获取：@href 即可获取节点的 href 属性 属性匹配：使用中括号，@属性名 = 值 的方式来限定某个属性 获取文本用 XPath 中的 text() 方法可以获取节点中的文本 12result6 = html.xpath('//li[@class ="item-0"]/a/text()') #获取a节点下的内容result7 = html.xpath('//li[@class ="item-0"]//text()') #获取li下所有子孙节点的内容 这里要注意，text（）方法要结合着前面的”/“或“//” 标签看，如果是 “/” 的话，就输出当前子节点的文本；如果是 “//” 的话，就输出当前节点的所有子孙节点的文本 模糊查询如果 HTML 文本中的 li 节点的 class 属性有两个值 ，例如 “class =li li-first” 遇到这种情况，我们可以用contains()函数或者将多个值写全，才能匹配到该节点。 多属性匹配如果需要根据多个属性才能确定一个节点，这是就需要同时匹配多个属性才可以，那么这里可以使用运算符 and 来连接（xml还支持其他运算符） and 表示 “与” 的关系，只有同时满足两个属性匹配表达式，该节点才会被选中。 按序选择有时候我们在选择的时候可能某些属性同时匹配了多个节点，但是我们只想要其中的某个节点，如第二个节点，或者最后一个节点，这时该怎么办呢？ 这时可以利用中括号传入索引的方法获取特定次序的节点 类似于列表的切片，不过需要注意： 这里的切片的索引是从1开始的 支持last、position等函数 还支持+-&lt;&gt;的推算 写在最后这一篇还是干货满满的，哈哈哈，慢慢吸收吧~ 关于节点选择的编写，写熟了自然就会了。 下回见，peace~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day16-文件数据存储]]></title>
    <url>%2F2019%2F08%2F21%2F%E7%88%AC%E8%99%ABDay16-%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[前言关于数据存储，我们之前已经讲了数据库的存储： 爬虫Day7-Mysql的那些事 ，还有 爬虫Day13-Mongodb存储 但是对于一些小型的文件，我们可以使用多种格式类型的文件保存到本地，相对于数据库来说，比较便捷。今天介绍的是有关文件数据存储的相关内容。 文本存储将数据保存到 TXT 文本的操作非常简单，而且 TXT 文本几乎兼容任何平台，但是有个缺点就是不利于检索，所以如果对检索和数据结构要求不高，追求方便第一的话，可以采用 TXT 文本存储。 12345678910111213141516171819202122import requestsfrom pyquery import PyQuery as pqurl = 'http://www.zhihu.com/explore'headers = &#123; 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:67.0) Gecko/20100101 Firefox/67.0'&#125;html = requests.get(url,headers=headers).text # html纯文本内容doc = pq(html)items = doc('.explore-tab .feed-item').items()# 所有的回答内容都包含在此div中，我们使用items（）将其变成一个迭代器for item in items: # for 遍历这个迭代器 question = item.find('h2').text() # 提问的内容 author = item.find('.author-link-line').text() # 答主 answer = pq(item.find('.content').html()).text() # 将回答的内容提取 file = open('explore2.txt', 'a', encoding='utf-8') # 创建文件句柄 file.write('\n'.join([question,author,answer])) # 将各部分内容依次写入文件 file.write('\n'+'='*50+'\n') # 分割每一个回答 file.close() 我们在上面做了以下事情： 请求网页（设置headers头部） 发出get请求，获得网页响应内容 观察网页内容，调用解析库，传入获取的html文本，生成doc对象 使用选择器进行内容匹配 写入文件（优化数据） 需要注意的是，构造文件句柄的时候，需要注意编码问题，同时，文件的处理类型也要指明（默认是“r”） 媒体文件存储存储媒体文件的两种方式： 只获取文件 URL 链接 直接把源文件下载下来 对于爬虫来说，如果我们要爬取的东西很多，那么建议使用第一种方式。使爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。 那么，如果通过URL链接，下载我们想要的媒体文件呢？ urllib的urlretrieve方法 在python3 中， urllib.request.urlretrieve 可以根据文件的 URL 下载文件： 12345678910from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com')bsObj = BeautifulSoup(html, 'lxml')imageLocation = bsObj.find("a", &#123;"id": "logo"&#125;).find("img")["src"] # 获取一张图片的URL路径result = urlretrieve(imageLocation, "logo.jpg")print(result) urlretrieve方法传入两个参数： 下载的URL链接，以及所保存的文件名 注意文件后缀名要和保存的数据相符合，例如 图片是jpg格式，那么我们传入的文件名后缀也要是 jpg 格式 Json 文件存储Json，全称为 JavaScript Object Notation, 也就是 JavaScript 对象标记，通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高，它是一种轻量级的数据交换格式。 Python 为我们提供了简单易用的 json 库来供我们实现 Json 文件的读写操作，我们可以调用 json 库的 loads() 方法将已编码的 JSON 字符串解码为 Python 对象，可以通过 dumps()方法将 Python 对象编码成 JSON 字符串。 值得注意的是 Json 的数据需要用双引号来包围，不能使用单引号。 并且，如果存储的数据中包含中文，我们需要指定一个参数 ensure_ascii 为 False，另外规定文件输出的编码（不指定文件输出的编码会乱码） 读取Json 读取：使用 loads（） 方法，将Json对象解码为 Python 对象 存储Json 调用 dumps() 方法将 Python 对象编码成 JSON 字符串。 CSV文件存储CSV，全称叫做 Comma-Separated Values，中文可以叫做逗号分隔值或字符分隔值。通俗的讲，他就是就是以特定字符分隔的纯文本，我们可以使用 Excel 打开 .csv的文件。单调、简朴是这种文件类型的风格。 CSV的写入 首先打开了一个 data.csv 文件，然后指定了打开的模式为 w，即写入，获得文件句柄。 随后调用 csv 库的 writer() 方法初始化一个写入对象，传入该文件句柄，然后调用 writerow() 方法传入每行的数据即可完成写入。 第一行通常是存储字段的字段名，这起到标识字段值的作用。 文件打开效果如下： CSV的读取 关于如何打开.CSV后缀的文件，并不是在 open()返回的 File 对象上调用 read()或 readlines()方法，而是将 File 对象传递给 csv.reader()函数。这将返回一个 Reader对象。我们将基于此对象进行文件读取。 同时，我们可以使用 for 循环遍历出文件内容。 线上CSV Python 的 csv 库主要是面向本地文件，就是说你的 CSV 文件得存储在你的电脑上。而我们进行网络数据采集的时候，很多文件都是在线的。不过有几种方法可以解决这个问题： 手动把 CSV 文件下载到本机，然后用 Python 定位文件位置； 写 Python 程序下载文件，读取之后再把源文件删除； 从网上直接把文件读成一个字符串，然后转换成一个 StringIO 对象，使它具有文件的属性。 虽然前两个方法也可以用，但是既然你可以轻易地把 CSV 文件保存在内存里，就不要再下载到本地占硬盘空间了。可以直接把文件读成字符串，然后封装成*StringIO对象，让Python把它当作文件来处理，就不需要先保存成文件了。 代码实现： 跟读取本地CSV 文件步骤相似，只是多了使用 StringIO( )方法将在线的CSV文件转为StringIO对象。 这样就实现了线上 CSV 的文件读取，是不是很方便！ 写在最后ok了，这一篇我们介绍了关于文件存储的大部分流行技术，快快用起来吧~ 朋友们，下回见啦。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day15-识别图形验证码]]></title>
    <url>%2F2019%2F08%2F19%2F%E7%88%AC%E8%99%ABDay15-%E8%AF%86%E5%88%AB%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言上一篇的模拟登陆 爬虫Day14-模拟登陆Github 提到，具体的反爬手段还有验证码，而验证码又分为很多类：图形验证码、极验滑动验证码、点触验证码等等。对于简单的验证码，我们可以利用脚本识别，如果是比较复杂的验证码，建议外接到打码平台，省时省力。 今天我们要讲的是图形验证码。 环境准备 识别图形验证码需要库 tesseract 和 Pillow。请自行百度安装。 Pillow 是从 Python 图像库（Python Imaging Library，PIL）分出来的；而关于tesseract，在python中它的库为tesserocr。 有人分不清为什么安装了软件还要安装模块，其实是这样的：在python 中的库相当于软件的包装器，可以帮助我们调用软件。 tesseract 的使用我们先保存一张验证码到本地，用来测试使用。名为 code.jpg 如下图： 第一种方法： 12import tesserocrprint(tesserocr.file_to_text('code.jpg')) 通过两行代码我们就将验证码识别出来，是不是很棒！ 第二种方法： 123456import tesserocrfrom PIL import Imageimage = Image.open('code.jpg') # 使用open方法打开验证码的图，获得一个image对象result = tesserocr.image_to_text(image) # 调用 image_to_text（）方法将图片转字符串print(result) 运行结果如下： 可以看到多出来个空白格，对于一些复杂的线条，tesseroct库会识别失效，这个我们后面会说。 并且，第二种方法是基于打开图片后的image对象进行操作的，而且识别效果更高。建议采用第二种写法噢~ 转灰度、二值化上面的方法虽然快捷、但是一旦遇到有复杂线条干扰的验证码，识别准确度就会大大下降。我们可以通过转灰度、二值化进行加工处理，让图片变得更容易电脑识别。 先了解一下这个convert 方法： convert（）：将当前图像转换为其他模式，并且返回新的图像。 代码实现： 12345678910111213141516171819import tesserocrfrom PIL import Imageimage = Image.open('code.jpg')image = image.convert('L')threshold = 127 # 二值化阈值 数值越低接近白，越高越黑table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show()result = tesserocr.image_to_text(image)print(result) 利用Image对象的convert（）方法参数传入’L’，即可将图片转化为灰度图像 传入数字”1” 即对图片进行二值化处理，convert()方法采用的是默认阈值为127。不过我们不能直接转化原图，要将原图先转为灰度图像，然后再指定二值化阔值 threshold 变量存储的二值化阈值的规律是：数值越低接近白，越高越黑（并且是从左到右渲染的） images.point方法：返回给定查找表对应的图像像素值的拷贝。变量table为图像的每个通道设置256个值。 它会为输出图像指定一个新的模式（mode指定的模式） 转灰度、二值化的变化过程： 可以看到，一些复杂的线条就被我们清理掉了，剩下白色背景、黑色字体的图片（tesseroct最喜欢这个了） 运行结果： 经过后期处理之后，现在的识别准确度大大提高，对于此张验证码，我们做到了完美识别！ 写在后面验证码绝对是反爬工程师手中的一把利器，可以阻挡住多数低级爬虫。 这篇只是介绍了下图形验证码，对于验证码，还需要其他方面知识的铺垫、例如机器学习、一定的数学基础等等。 当你学会识别大多数的验证码时，就可以自信的去敲BAT级别公司的门了。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day14-模拟登陆Github]]></title>
    <url>%2F2019%2F08%2F15%2F%E7%88%AC%E8%99%ABDay14-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86Github%2F</url>
    <content type="text"><![CDATA[前言爬虫中比较重要的一环，当然是模拟登陆了，期间可能会遇到验证码等反爬行为，今天要讲的模拟登陆Github，是比较简单的类型：构建请求参数。但这也是最为常见的登陆形式。 我们知道，要想登陆到站点，肯定交互了账号密码、Cookies值的设定等等 为了安全性考虑，大多数请求形式使用POST方式进行交互，账号信息包含在一个From Data 的表单中，服务器拿到了信息之后，会到数据库进行比对，是否有此人的个人信息，如果有，就登陆成功。 模拟登陆首先要分析登录的过程，需要探究后台的登录请求是怎样发送的，登录之后又有怎样的处理过程。 如果已经登录 GitHub，先退出登录，同时清除 Cookies。 打开 GitHub 的登录页面，链接为 https://github.com/login 。 然后输入GitHub 的用户名和密码，打开开发者工具，将 Preserve Log 选项勾选上，这表示显示持续日志（preserve log的作用是，保持下面的所有的网络请求不被冲掉。因为当网页刷新的时候，如果没有选中这个按钮，之前的访问url记录会被冲掉。） 如下图所示： 点击登陆之后 发现交互了一些信息，我们查看session请求响应条目，它是个POST 请求信息（就是它了！） 状态码是302，为重定向，说明我们登陆的时候，经过了一个页面。 往下拉，查看Request Headers 和 From Data 信息： 发现有个from Data ，这对应 POST请求发送的Data表单 。 里面字段信息有 login、password （账号密码），另外还有utf8、commit、authenticity_token 这些其他字段。 有些字段可以直接构建，一些字段参数不知道其含义，只能一个一个测试。比如这里的 authenticity_token 通过观察，我们发现在登录之前我们会访问到一个登录页面，此页面是通过 GET 形式访问的。输入用户名密码，点击登录按钮，浏览器发送这两部分信息。 也就是说 Cookies 和 authenticity_token 是在访问登录页的时候设置的。 一些临时变量的参数，肯定是在登陆之前附加的参数（这是个先有鸡后有蛋的故事） token 是类似令牌的东西，这种令牌要么是用户注册的时候分配给用户，要么就是在用户调用的时候才提供，可能是长期固定的值，也可能是频繁变化的，通过服务器对用户名和密码的组合处理后生成。 我们应该弄懂 token 是如何生成的，才能完成表单提交。 于是我们再次退出登陆，清除Cookies，寻找Cookie、 authenticity_token 参数是从哪来的。 发现 交互的报文中，Response Headers 有一个 Set-Cookie 字段。这就是设置 Cookies 的过程。 然后在网页的源码探寻，搜索相关字段，发现源代码里面隐藏着 authenticity_token信息，它是一个隐藏式表单元素。 那么我们就拿到了POST的请求参数了。 可以进行模拟登陆了~ 代码示例：12345678910111213141516171819202122232425262728293031323334353637import requestsfrom lxml import etreeclass Login: def __init__(self): self.headers = &#123; 'host': 'github.com', 'Referer': 'https://github.com/login', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; self.login_url = 'https://github.com/login' # 登陆之前的URL self.post_url = 'https://github.com/session' # 发送POST请求的URL self.session = requests.Session() # 由Session实例化的对象，Cookies会自动保存 def back_tokenI(self): response = self.session.get(self.login_url, headers=self.headers) selector = etree.HTML(response.text) token = selector.xpath('//div//input[2]/@value')[0] # 选中 authenticity_token 参数的值 return token def login(self, username, password): data = &#123; 'commit': 'Sign in', 'authenticity_token': self.back_tokenI(), 'utf8': '✓', 'login': username, 'password': password &#125; response = self.session.post(url=self.post_url, headers=self.headers, data=data) if response.status_code == 200: print('模拟请求成功了！！！') return response.text # 返回登陆成功之后的数据c1 = Login()c1.login('username','password') 注意我们这里使用的Session（）实例化的对象发送请求的，这个方法构建的对象，在访问页面的时候，会自动帮我们保存 Cookies值 。这样我们就不用特定设置Cookies 参数了。 写在最后其实这种携带参数请求的模拟登陆并不复杂，主要是它携带的参数有多个，你需要一个个去试，没有其他规律可循。当然，有时候会碰到 salt 加密或CSS反爬，这个就需要其他方面的知识点了。 这篇Github模拟登陆就说到这里啦，下期见，peace~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day7-事件驱动模型]]></title>
    <url>%2F2019%2F08%2F15%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay7-%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[事件驱动性思想事件驱动模型不是什么具体的东西，而是一种“思想”。事件驱动编程是一种编程范式，这里程序的执行流由外部事件来决定。它的特点是包含一个事件循环，当外部事件发生时使用回调机制来触发相应的处理。 关于编程范式，在这里要说说传统编程和事件驱动型编程的区别： 传统编程传统的编程是如下线性模式的： 1开始---&gt;代码块A---&gt;代码块B---&gt;代码块C......---&gt;结束 事件驱动型 对于事件驱动型程序模型，它的流程大致如下： 1开始---&gt;初始化---&gt;等待信号---&gt;特定信号---&gt;相应处理 我们看到了事件驱动型和传统编程的编程风格是完全不同的 事件驱动程序在启动之后，就在那等待，等待什么呢？等待被事件触发。传统编程下也有“等待”的时候，例如：input()，需要用户输入交互数据。但这种等待是有目的性的：期待着用户某些行为，或输入一个特定的字符串、数字等等。如果用户输入不规范，还需要提醒他，并做处理。 而事件驱动程序的等待则是没有目的性的，也不强制用户输入或者干什么。只要某一事件发生，那程序就会做出相应的“反应”。这些事件包括：输入信息、鼠标、敲击键盘上某个键还有系统内部定时器触发。 事件驱动模型介绍通常，我们写服务器处理模型的程序时，有以下几种模型： 每收到一个请求，创建一个新的进程，来处理该请求； 每收到一个请求，创建一个新的线程，来处理该请求； 每收到一个请求，放入一个事件列表，让主进程通过非阻塞I/O方式来处理请求 第一种第二种都挺消耗资源的，第三种就是协程、事件驱动的方式，一般普遍认为第3种方式是大多数网络服务器采用的方式。 下图就是典型的事件驱动模型： onclick 绑定着 一个 fun（）函数，当事件被触发时（这里是被点击），相应的处理模块，就会运行。 有了这种思想，我们就可以监测用户行为，从而发送响应内容了。 那么，如何去检测用户什么时候点击、点的哪里呢？ 检测点击的方法①创建一个线程循环 这个方式有以下几个缺点： CPU资源浪费，可能鼠标点击的频率非常小，但是扫描线程还是会一直循环检测，这会造成很多的CPU资源浪费 如果扫描鼠标点击的接口是阻塞的，又会出现下面这样的问题，我们无法检测键盘是否被按下，因为扫描鼠标时被堵塞了，那么可能永远不会去扫描键盘；（此时，一个线程无法解决问题） 如果一个循环需要扫描的设备非常多，这又会引来响应时间的问题 ②事件驱动模型思想 目前大部分的UI编程都是事件驱动模型，如很多UI平台都会提供onClick()事件，这个事件就代表鼠标按下事件。事件驱动模型大体思路如下： 有一个事件（消息）队列； 鼠标按下时，往这个队列中增加一个点击事件（消息）； 有个循环，不断从队列取出事件，根据不同的事件，调用不同的函数，如onClick()、onKeyDown()等； 事件（消息）一般都各自保存各自的处理函数指针，这样，每个消息都有独立的处理函数； 模型如图所示： okk，网络编程系列更新到此结束啦，跟计算机网络系列一样，都是 day7，挺巧的哈哈哈 以后新增的更新系列应该是操作系统方面吧（Linux），当然，爬虫系列是不会断更的。byebye~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day6-队列]]></title>
    <url>%2F2019%2F08%2F14%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay6-%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[前言上此我们讲到了线程： 网络编程Day3-子线程 ，那么多线程如何应用呢？ 就是我们今天要讲的主题了——队列。 队列，是用来解决线程安全的一种“利器”，讲述队列之前，我们需要对一个模型进行了解。 生产者消费者模型生产者消费者模式是指通过一个容器来解决生产者和消费者的强耦合问题。 耦合： 用python代码来做比喻，如果更改一处代码，其他代码会随之受到影响的情景，我们就称之为耦合。 生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这是一个解耦的过程。 12PS：python 的协程，就涉及到了生产者消费者模型的相关知识。协程，又称微线程，纤程。英文名Coroutine。协程的作用，是在执行函数A时，可以随时中断，去执行函数B，然后中断继续执行函数A（可以自由切换）。但这一过程并不是函数调用（没有调用语句），这一整个过程看似像多线程，然而协程只有一个线程执行。(因此也被较为伪多线程) 队列队列是用于线程安全的，提供了一个适用于多线程编程的数据结构。queue是python标准库中的线程安全的队列，默认基本FIFO队列（FIFO即First in First Out,先进先出） Python Queue模块有三种队列及构造函数（返回一个数据容器）: Python Queue模块的FIFO队列先进先出。 LIFO类似于堆，即先进后出。 queue.LifoQueue(maxsize) 还有一种是优先级队列级别越低越先出来。 queue.PriorityQueue(maxsize) 队列的方法q.put()调用队列对象的put()方法在队尾插入一个项目。put()有两个参数，第一个item为必需的，为插入项目的值；第二个block为可选参数，默认为True。如果队列当前为空且block为True，put()方法就使调用线程暂停,直到空出一个数据单元（等待其他线程做get操作）。如果block为False，put方法将引发Full异常（设置当队列塞不进数据时直接报错）。 q.get()调用队列对象的get()方法从队头删除并返回一个项目。可选参数为block，默认为True。如果队列为空且block为True， get()就使调用线程暂停，直至有项目可用。如果队列为空且block为False，队列将引发Empty异常。 block 参数的作用：设想一下，如果一个线程运行着 put 或 get 方法，当队列为空时，是不是一直保持着，等待状态，这跟我们之前讲过的 socket 一样，类似于它的 recv（一直等待消息），这时候，就处于阻塞状态。 put和get同样可以卡住，也是依赖于同步状态进行操作的。所以我们可以通过设置block值引发错误，然后通过异常捕获来避免这种情况发生。 其他方法 q.task_done()： 在每一次完成put或get时，就会发送一个信号 q.join（）： 用于接收信号，只有全接收到信号时，才会执行下一步操作 这两者的配合就达到了同步效果 队列的使用假设现在有一个需求：开两个线程来删除某数据对象，这里我们使用一个列表来模拟。 运行之后发现报错了：原因很简单，remove是按值删除的，线程1删除掉的数据，列表已经没有这个值了，线程2想要删除这个值时，就报错。那么这个问题就是数据不安全导致的。 报错信息如下： 根据我们学到过的知识点，有这么两个解决方法： 方法一：加锁 加了锁之后，虽然能够解决问题，但对于这种 CPU密集型的任务，其并不能完美胜任这个工作。 方法二：使用队列123456789101112131415161718192021222324252627import queueimport threadingimport timethe_list = [1,2,3,4,5]q = queue.Queue()def dl(): while len(the_list) &gt;0: data = q.get() # 从队列中拿数据 q.task_done() # 发送信号，表明已拿到数据 the_list.remove(data) # 接着将拿到的数据从列表中删除 print("%s 已经被删除了"%data)def d2(): while len(the_list) &gt;0: a = the_list[-1] # 从列表中取出最后一个数据 q.put(a) # 将数据放入队列中 q.join() # 等待另一个线程的信号t1 =threading.Thread(target=dl)t2 =threading.Thread(target=d2)t1.start()t2.start() 我们使用一个线程循环来完成这个需求，循环的条件时：列表不为空 然后一个子线程不断的放、另一个子线程不断的取。最终达到删除列表数据的效果 通过队列来完成解耦操作，保证了数据的安全性 1队列的相关知识就讲到这儿了~ 下期见，各位。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发者的思维]]></title>
    <url>%2F2019%2F08%2F14%2F%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%84%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[前言之前我们提及过，一名计算机学生应有的必修课 其中提及到了： 努力、视野、职业规划 但这远远不够，怎么去努力？所谓的计算机思维是什么？ 这是今天我们要说的主题。 如何学习对于编程学习者，我决定最应该普及的，不是什么理论知识，而是思维的蜕变，这个至关重要。昨晚听了一个知乎大佬的live之后，我收益匪浅，决定将一些心得写下来。 那么对于思维的转变是这样的：不要把自己当学生，而是要把自己当开发者。 学生们刚从高中的学习进入到大学，潜意识的沿用了高中的学习方法，这很不利于计算机的学习 如果你懂计算机思维的话，就知道抱着书照啃是多么糟糕的一种事情，有些同学，就想着一本书从头到尾理解透彻，其实计算机的理论知识的递归性的东西，而不是线性的。高中的知识，学校已经为我们安排好了每一章，每一个知识点。而对于计算机，没有一种固定的学习路线，导致了学生们前期迷茫的感觉。 记笔记笔记的重要性不言而喻，当你学完一定的知识点之后，大部分所内化的东西就是你的笔记内容。但是，记笔记也是有方法可循的，不是我夸大，大部分人连技术型笔记都不会写。 我赞成使用电脑记笔记，因为纸质版并不方便，还有一点就是：你确定你记在书上的东西，会再去翻吗 下图是我记笔记的方式 大脑是有遗忘机制的，如果想着把所有东西都弄懂并完整的记下来，几乎是不可能的。 那么这时候，笔记就成了你的第三方库，你随时可以上面查阅你已经弄懂的内容，开发者的工作并不是考试，你完全可以利用网上的资源，只要你找得到。 当你学习一个东西的时候, 如果学完马上用语言讲给别人听, 你会学的更好, 而且会发现新问题，还有没发现过的知识点。 费曼学习理论都指出, 知识的学习, 输入固然重要, 输出更为重要. 几乎所有优秀的程序员, 都有攥写技术文章的习惯, 很多时候, 并不是他们什么都懂, 而是他们刚学会了什么, 然后就围绕着这个刚学会的东西, 用自己的语言讲出来, 久而久之, 就会被别人觉得是大牛, 但是他和你的区别, 有可能仅仅是是否输出的区别. 我们可以在笔记上写平时学习的东西，然后再使用博客之类的东西，将笔记的内容用自己的话写出来，这样一来，就是输入以及输出了。 1除了会记笔记之外，思维层面的东西，更为重要 开发者思维开发者思维是计算机思维典型的一种。 开发者思维是指, 从学习编程第一天起, 你的目标, 你所做的事, 永远不是以要学会XX为目标, 而是以开发出XX为目标, 深刻意识到你学习的一切, 最终是为了你的开发而服务的。 具有开发者思维的人，会这么学习： 先确定好要学习的理论知识，比如python，然后大体了解一下python语言在哪些领域比较强势，在网上搜索一番之后，他会下定一个我要做出 xx 的项目这样的目标。 目的性很强的人，学习能力往往也很强。 与开发者思维相反的是：学生思维 学生思维读教材, 听课, 记笔记, 追求把这个语言的每个知识点都记得很清楚, 追求一种”内功”的修炼, 在这个过程中, 从来不想着用它去”创造”什么，比如在学习的第一天起，给自己定下的目标是这样的： “我这个学期一定要把这个语言的基础打牢，并且为以后的学习提供更坚实的基础.” NO！！！ 这就是为什么很多学生虽然在校学习成绩很好，但毕业后找不到工作的根本原因。 拥有学生思维，总是会陷入焦虑：“我下一阶段应该学什么、学的东西以后忘掉了怎么办、学这些东西有用吗？”等等这些思想，然后学习曲线非常平缓，甚至待在舒适区久了，你就无法跨越自己了。 这两种思维所导致的结果往往是, 后者无论是知识熟练度还是实用性都会超过前者, 而且整个学习过程会有源源不断地动力。 如果你具备开发者精神, 你开发出的东西, 你做出的产品, 它就是永久存在在这个世界上的, 你的成就感来源于真实的, 具体的, 可持久延续的项目中, 而不是来源于”我学会了什么”. 一切不谈成就感, 不谈反馈的学习, 都是空谈。 一份优秀的简历当你面聘工作时，好的简历上面写着的，应该是 “用xxx语言开发出xxx的项目”，而不是苍白的“精通xx语言” 如果你有记博客的习惯，当你面聘时，拿给考官一个博客 或 github 链接，一目了然。 这就是开发者思维带来的，最直接的影响。 写在最后以上就是我的一些心得体会，在以后的日子里，望君共勉！ 这是最好的时代，这是最坏的时代，我们一无所有，我们巍然矗立]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day13-Mongodb存储]]></title>
    <url>%2F2019%2F08%2F13%2F%E7%88%AC%E8%99%ABDay13-Mongodb%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[前言之前我们讲过关于 mysql 的数据存储：爬虫Day7-Mysql的那些事 对于爬虫的数据存储来说，一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整，另外数据之间能还存在嵌套关系。如果我们使用了关系型数据库存储，一是需要提前建表，二是如果存在数据嵌套关系的话需要进行序列化操作才可以存储，比较不方便。如果用了非关系数据库就可以避免一些麻烦，简单高效。 MongoDB 是一个基于分布式文件存储的开源数据库系统，其内容存储形式类似 Json 对象，它的字段值可以包含其他文档，数组及文档数组，非常灵活。在python中使用pyMongo模块进行交互。 Mongodb基础名词比较如果之前对 Mysql 有所了解，那么在 Mongodb中呢，对一些概念性东西就能一通百通了。下图是这两者的名词比较 在 Mongodb中，区别比较大的就是，他的表名词改为 collection （集合），且 所存的数据称为 document（文档） Document文档是一组键值(key-value)对(即 JSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点 集合集合就是 MongoDB 文档组，集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 pyMongo模块在对Mongodb有了初步的了解之后呢，我们就可以使用python 与 mongodb进行交互，进行操作了。 三步曲 连接 MongoDB 我们需要使用 PyMongo 库里面的 MongoClient，一般来说传入 MongoDB 的 IP 及端口即可，第一个参数为地址 host，第二个参数为端口 port，端口如果不传默认是 27017。 MongoDB 中还分为一个个数据库，我们接下来的一步就是指定要操作哪个数据库 MongoDB 的每个数据库又包含了许多集合 Collection，也就类似与关系型数据库中的表，我们需要指定要操作的集合 指定数据库或者指定集合可以使用client[‘test’] 和 db.[‘student’] 的形式进行指定（集合的结构造就了这种选择方式） 插入数据insert_one()： 插入单条数据 其返回一个 InsertOneResult 对象 ，表示插入的数据，是个 ObjectId 对象。 insert_many()：插入多条数据 Insert_many( )传入的参数需要以列表形式传递（二维数据） 这个方式挺常用的，我们把抓取的每个数据用字典存储，再append 到一个列表中，最终使用 inser_many 进行一次性存储，非常方便。 查询数据在爬虫中，数据存储最重要的是insert，但是数据查询也必不可少，比较我们需要查看数据是否插入成功嘛 find_one()： find_one（）里面传的参数可以为字典形式，其表明了限制条件，返回的result结果为 dict 类型 发现数据中多了个字段：“id”：“ObjectId”，MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象。ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes。 find(): 返回结果是 Cursor 类型，相当于一个生成器。 我们需要遍历取到所有的结果，每一个结果都是字典类型。 更新数据 Update_One 传入两个参数，第一个是查询条件，第二个是修改的数据（键为使用功能符号@进行选定的操作指令set，值为修改的数据） 如果调用 update_many() 方法，则会将所有符合条件的数据都更新。而update_one只是取第一条匹配的数据进行更新操作。 删除数据 删除数据 有 三个 API 供我们调用，操作与 update挺相像的 今天介绍了Mongodb这种NOSQL数据库的作用和特点，还有Mongodb的基本操作：增删改查。 ok，我们下期见，撒由那拉~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day5-全局解释器锁GIL]]></title>
    <url>%2F2019%2F08%2F11%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay5-%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81GIL%2F</url>
    <content type="text"><![CDATA[GIL （Global Interpreter Lock）Python代码执行由 Python 虚拟机 (又名解释器主循环) 进行控制。Python 在设计时是这样考虑的，在主循环中同时只能有一个控制线程在执行。对 Python 虚拟机的访问由全局解释器(GIL) 控制，这个锁用于确保当有多个线程时保证同一时刻只能有一个线程在运行。 GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念（这把GIL锁加在解释器层面的），而CPython是大部分环境下默认的Python执行环境（GIL并不是Python的特性，Python完全可以不依赖于GIL） GIL遵循的原则：“一个线程运行 Python ，而其他 N 个睡眠或者等待 I/O.”（即保证同一时刻只有一个线程对共享资源进行存取）。 GIL的影响 这个例子中，两个线程分别对一个全局变量total进行加和减1000000次，但是结果并不是0！而是每次运行结果都不相同。 造成这个结果的原因是GIL的释放，python中有两种多任务处理： 协同式多任务处理：一个线程无论何时开始睡眠或等待网络 I/O，就会释放GIL锁（协程） 抢占式多任务处理：如果一个线程不间断地在 Python 3 运行15 毫秒，那么它便会放弃 GIL，而其他线程可以运行（线程） 在单核CPU下没什么不一样（也有可能有性能损失），但是在多核CPU下问题就大了，不同核心上的线程同一时刻也只能执行一个，所以不能够利用多核CPU的优势，反而在不同核间切换时会造成资源浪费，反而比单核CPU更慢。 上面的 +- 操作就是触发了抢占式多任务处理机制，导致了这么一种情况：当一个函数运行着计算密集型程序时，由于GIL的原因，只会在一个单CPU上面运行。 两个函数对于共享变量 total 进行的修改，没有同步变量，导致数据不安全现象。 解决方法 可用 multiprocess 库替代 Thread 库，即使用多进程而不是多线程，每个进程有自己的独立的GIL，因此也不会出现进程之间的GIL争抢。但这样的话也会带来很多其他问题，比如进程间数据通讯和同步的困难，但是，多进程的开销也会增大。 多进程+协程（多用于解决IO密集型情景） 用其他解析器。像JPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。用了Java/C#用于解析器实现。 总结 在IO密集型型操作下，多线程还是可以的。比如在网络通信，time.sleep()延时的时候或者是 IO 操作时，有空余出来的时间，我们就可以对其他线程进行处理。 在CPU密集型/计算密集型操作下(没有空余时间)，多线程性能反而不如单线程，此时只能用多进程（从而打破GIL对多核的限制） 其实龟叔对这个GIL是有其考虑之处的，虽然python 有了这个缺陷，但是python社区的大佬们都在为了这个GIL 奋斗，相信不久的将来，这个缺陷会被解决。（对比python2.7，如今的python3.8已经达到了运行速度更加快的进展）]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day12-代理请求]]></title>
    <url>%2F2019%2F08%2F11%2F%E7%88%AC%E8%99%ABDay12-%E4%BB%A3%E7%90%86%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[前言我们在爬虫的过程中会遇到一些反爬手段，例如封IP、封用户账号等行为，我们可以通过使用代理IP或者代理池进行代理访问，预防被反爬。如何学会代理请求是爬虫工程师必须会的技能噢~ 必须点满 代理的原理我们常称呼的代理实际上指的就是代理服务器，英文叫做 ProxyServer，它的功能是代理网络用户去取得网络信息。也就是说，我们发送请求给代理服务器，再由他再次发出请求给服务端。这样就成功实现了 IP 伪装，也是代理的基本原理。 代理服务器这种机制，还有其他功能： 隐藏真实IP，对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。 提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。 Session和Cookies接着，我们应该了解这两个概念：Session和Cookies。 在浏览网站的过程中我们经常会遇到需要登录的情况，有些页面只有登录之后我们才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就会需要重新登录。还有一些网站有时在我们打开浏览器的时候就自动登录了，而且很长的时间都不会失效，这种情况又是为什么？其实这里面涉及到 Session 和 Cookies 的相关知识 无状态HTTPHTTP的无状态是指 HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送一个 Requset后，服务器解析此 Request，然后返回对应的 Response，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化。（这就导致了如果用户后续需要对已经发送的数据再做操作就会导致重传等等一些浪费资源的现象） 所以，这时候，两个用于保持HTTP 连接状态的技术就出现了，它们分别是 Session 和Cookies。（Session在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端） Cookie的作用有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。Cookie相当于一个凭证的东西，有了它，就可以完成“身份检验”。 注意，Cookie 是由服务器端通过 set-Cookie字段发送给客户端的，而不是客户端自动产生的。 Session原理在 Web 中 Session对象用来存储特定用户会话所需的属性及配置信息。当用户在应用程序的 Web 页之间跳转时，存储在Session 对象中的变量也不会丢失，会在整个用户会话中一直存在下去。（在登陆之后访问同网页下其他页面不需要重新登陆的原因） 当客户端再次访问时，服务器就拿着客户端携带的Cookies参数，检查该 Cookies 即可找到对应的Session是什么，然后再判断 Session来以此来辨认用户状态。这样，就实现了免登陆的效果。 神秘数字403当运行的爬虫速率太高，或者被检测出你是个爬虫工具发出的访问时，网页可能会弹出这样的403警告信息：“您的 IP 访问频率太高”。出现这样的现象的原因是网站采取了一些反爬虫的措施，比如服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，那么会直接拒绝服务，返回一些错误信息。 例如，这样的写的爬虫… 1234import requestsfor i in range(10000): requests.get('http://www.baidu.com') 是不是想人家服务器挂掉… 别人不封你封谁，估计掐死你的心都有了，哈哈 爬虫代理的实现requests的实现12345678910111213import requestsproxy = '123.114.203.61:8118' # 代理IP + 端口号proxies = &#123; 'http':'http://' + proxy, 'https:':'https://' + proxy&#125;try: response = requests.get('http://httpbin.org/get', proxies =proxies) # 代理需要通过 proxies 关键字指定，不然会以为是个data print(response.text)except requests.exceptions.ConnectionError as e: print('ERROR',e.args) proxy 变量接收的是我从网上找的免费代理，然后通过 IP +端口号的形式传给 get方法中的 proxies 变量，达到代理的效果 这么设置之后，访问页面时，对端服务器看到的就是我们设置的这个IP发出的请求。如果被封了，我们可以通过更换IP进行修改 selenium的实现123456789from selenium import webdriverproxy = '123.114.203.61:8118'chrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--proxy-server=http://' + proxy )browser = webdriver.Chrome(chrome_options=chrome_options)browser.get('http://httpbin.org/get')print(browser.page_source) selenium的实现是通过实例化一个带参数的 ChromeOptions 对象，然后这个对象就携带着我们添加进去的参数 后记实现代理的途径还有代理池、付费代理、ADSL拨号等等，不过原理都是相似的，都是拿到一个IP，然后传给 请求函数，这样携带的请求就是代理 IP。 不过最好还是控制一下记己，服务器挂了大家都没得玩。利用爬虫的时间，冲冲茶养养生还是蛮不错的，哈哈哈]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day4-并发并行与同步异步]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay4-%E5%B9%B6%E5%8F%91%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[并发与并行的区别 并发：是指系统具有处理多个任务的能力。 单个CPU可以通过高频率的切换达到并发效果。 并行：是指系统具有 同时 （唯一的时间节点）处理多个任务的能力。 多核CPU能实现并行效果。 （并行是并发的一个子集，并行一定是并发，而并发不一定是并行。） 同步与异步的区别 同步： 你总是做完一件再去做另一件，不管是否需要时间等待，这就是同步(就是在发出一个功能调用时，在没有得到结果之前，此时不能做其他事情)；例如socket中的recv，同步时，就一直等待用户输入，直到有结果返回。 异步则反之，你可以同时做几件事（可以先跳过当前事件，在得到结果之后，再回过头来处理）碰到当前事件卡住时，例如IO操作，可以先跳过，处理其他事情，当等待有结果时，再回过头来处理当前事件。 同步异步与并发并行的概念需要理解透彻，后面的锁、队列才能够理解 阻塞与非阻塞的区别阻塞(blocking)、非阻塞（non-blocking）：阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。(进程或线程就阻塞在那了，即挂起状态 不能做其它事情) 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。(在等待的过程中可以做其它事情) 什么！ 阻塞与非阻塞的概念怎么如此之像同步异步？ 当初我也是感到疑惑… 在我仔细观察其变化之后，原来是这样的：同步异步是一种处理机制，而阻塞非阻塞是一种概念，他描述的是程序等待调用结果的一种状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起。而同步是一直等待着该线程的响应信息（线程还是激活状态） 呵…不愧是我， 此篇完结，peace~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day3-子线程]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay3-%E5%AD%90%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言之前我们讨论了进程与线程，网络编程Day1-进程线程概念，那么，在python中如何调用呢？今天这一篇，就是介绍如何创建子线程进行调用的。并且在学习如何创建子线程之后，子进程的创建也就会了，因为他们的 API 接口都是一样的。 创建子线程创建线程有两种方法： ① 使用python替我们封装好的模块：threading ② 基于Thread类，我们重写模块。通过重写run方法，调用这个模块 线程的运行规律我们开启了两个子线程，依次让这两个子线程执行一个hello函数，里面使用sleep维持函数的执行时间 观察发现： 我们知道，当单线程运行时，函数是从上到下按顺序执行的，那么 两个sleep函数，则要耗费 6s 以上的时间。但是我们采用了子进程之后，执行时间 只有3秒。 主线程程序按顺序执行，子线程的函数同时也执行（与主程序并驾齐驱，称为并发）三秒后，整个进程才真正结束 当主线程完成想退出时，会检验子线程是否完成，如果子线程未完成，则主线程会等待子线程完成后再退出。 Join方法概念：等待子线程运行完毕后再执行下一步操作 我们知道，子线程的运行默认是并发的，也就是说，不会因为子线程函数运行时，程序就卡在那。join方法可以实现等待子线程运行完后再进行下一步操作 setDaemon方法setDaemon又称为守护线程，被设置为守护进程的子线程，会随着主线程运行完毕而强行终止。（程序是否运行完的判断不再依据守护进程）。 同时，setDaemon方法必须置于 start（）之前 setDaemon（）默认是 Flase，表示未启动 其他实例方法：无论是start 还是 join ，亦或者是 setDarmon方法，都是我们通过Treading.Thread下实例的方法，同时还有： run（）是所有实例化的子线程都会执行的函数 start（）是将子线程设置为等待被CPU调用的状态]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day2-socket原理]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay2-socket%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言之前我们在 计算机网络系列中提到过，应用都是依靠传输层协议进行通信的，并且，不同的网络应用使用端口号进行区分。我们知道IP层的ip地址可以唯一标识主机，而TCP协议和端口号可以唯一标示主机的一个进程。本地进程通讯中我们可以使用PID来唯一标示一个进程，但PID只在本地唯一，网络中的两个进程PID冲突几率很大，这时候我们需要另辟它径了。那么，Socket（套接字）就诞生了。 socket套接字 把传输层以下的低层实现抽象化了，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 套接字介绍套接字的类型： 套接字有两种（或者称为有两个种族）,分别是基于文件型的UNIX和基于网络型的INET。 我们主要讲的是 AF_INET ，这种基于网络类型的套接字 收发消息的原理应用之间互相发送数据，并不是我们想象的那样，直接发送到对端的应用上，而是通过网卡发送到对方的缓存，再通过OS进行调用获得的。如下图…（画的丑不要介意） 客户端和服务端不是对接性、直接的往对方发送消息，而是有经过缓存的中转（应用程序产生的数据拷贝给缓存）。 操作系统如何控制发送数据是按照我们定义的传输层协议（UDP\TCP）进行管理的。 socket建立连接服务器端： 12345678910111213import socket# 实例化，以及注明是哪个套接字类型、基于TCP的流tele_phone =socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 实例化一个套接字对象，绑定、监听都是基于socket对象进行操作的tele_phone.bind(("127.0.10.1",8080)) # 绑定所监听的地址以及端口号（需要在本机网卡上存在的地址）tele_phone.listen(5) # 最多可以跟5个客户机保持连接print("执行到我啦")conn,addr =tele_phone.accept() # 等客户端的链接地址，当对面使用connect，就建立起连接了# 注意，收发消息是基于conn连接的conn.send("hello world".encode('utf-8')) # 发送数据，基于网络通信，须以二进制传输conn.recv().decode('utf-8') # 等待对方发送数据，编码格式要保持一致 函数 作用 s.bind() 绑定(主机,端口号)到套接字 s.listen() 开始TCP监听，listen对应backlog池，当有多个链接时，服务端对将收到的链接放到backlog池中，链接池里面的链接会等待着与服务器建立链接 s.accept() 被动接受TCP客户的连接,(阻塞式)等待连接的到来 s.recv() 接收TCP数据 s.send() 发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完) 客户端： 123456import socketphone = socket.socket() # 实例化一个套接字对象phone.connect(("127.0.0.1",8080)) # 连接服务器绑定的IP地址phone.recv().decode('utf-8') phone.send("OK".encode('utf-8')) 通过上面这些代码，我们就已经建立起了socket连接，然后就可以通过 send、recv 函数来完成收发信息的操作了。 粘包现象从上面我们知道了，数据的交互是通过缓存区的，那么粘包就很好理解了：上次执行的命令，没有全部显示完，还停留在缓存区，等下次执行别的命令时，这时候，缓存把残余的内容发送出来了。所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。 粘包现象两种情况： ① 发送数据量太大，超出对端缓存区限制 ② 发送数据量非常小，TCP采用Nagle算法（为了消除网络延迟），Nagle算法简单来说就是：如果你连续发送几次数据，但这些数据并不大，通常TCP会根据优化算法把这些数据合成一个TCP段后一次性发送。而对端当作一个包处理（它并不能逆向解包，导致粘包现象的产生） 当然，这种粘包现象是有解决方案的——使用UDP，它是无连接的、面向消息的，数据包与包之间互相独立，一收对应一发，因此没有粘包现象的产生。但是会丢数据，不可靠。 或者，告诉对端消息边界——在实际数据交互之前，通知对方，“我”的数据有多少；或者将多次发送数据的情况分隔开。 这篇socket就介绍到这儿了，下回见~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day11-Selenium魔法师 Vs bilibili]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%88%AC%E8%99%ABDay11-Selenium%E9%AD%94%E6%B3%95%E5%B8%88-Vs-bilibili%2F</url>
    <content type="text"><![CDATA[前言B站可谓是个资源丰富的地方，对我来说，它就是一个学习网站，里面啥都有。。。我们今天使用selenium 爬取B站信息，实现 关键词 抓取 的需求。 对于selenium 我们之前已经介绍过： 爬虫Day8-Selenium魔法师 实战url = https://www.bilibili.com/ 使用 selenium ，当然是先需要获取我们对应点击的节点，进行模拟操作了。 拿着之前的文章，这不是小意思嘛，欻欻的写出了下面这段代码： 代码v1.0 12345678910111213141516171819202122232425262728293031from selenium import webdriverfrom selenium.webdriver.support.wait import WebDriverWaitfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECimport timebrowser = webdriver.Chrome()wait = WebDriverWait(browser, 20)url = 'https://www.bilibili.com/'KEYWORD = '大仙'def get_page(): browser.get(url) # 点击首页刷新一下网页，防止有验证码登录 index = '#primary_menu &gt; ul &gt; li.home &gt; a' can_click = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,index))) can_click.click() # 找出文字搜索框 botton = '#banner_link &gt; div &gt; div &gt; form &gt; input' input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, botton))) # 找出提交按钮 search = '#banner_link &gt; div &gt; div &gt; form &gt; button' wait_search = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, search))) input.send_keys(KEYWORD) # 发送关键字 wait_search.click() # 点击按钮 ok…接着我们运行一下代码测试一下，然后就来到了这个页面 发现所有的视频内容都包括在一个 class = “all-contain”的 div 中 我们使用 find 方法将其匹配出来。 代码v1.1 1234567891011def get_page(): ... # 以上省略的代码 input.send_keys(KEYWORD) wait_search.click() # 页面跳转 browser.switch_to_window(browser.window_handles[1]) html = browser.page_source return html 为什么有一个页面跳转的步骤呢？ 因为啊，我们的浏览器发送关键词搜索之后，页面已经切换了，而我们后续需要使用page_source 属性获取响应信息，如果没有页面切换，那么得到的响应内容是原页面的内容，这不是我们想要的。 解析我们调用 爬虫Day6-Beautiful介绍 这个库 进行代码解析并写入文件 1234567891011121314def parser(html): soup = BeautifulSoup(html,'lxml') results = soup.find(class_='contain').find_all(class_='info') with open('daxian.txt', 'w', encoding='utf-8') as f: for result in results: title = result.find(class_='des hide').text.split() watch_frequency = result.find(class_='so-icon watch-num').text.split() put_time = result.find(class_='so-icon time').text.split() the_up = result.find(class_='up-name').text.split() f.write(str(title)+ '\n') f.write(str(watch_frequency) + '\n') f.write(str(put_time) + '\n') f.write(str(the_up) + '\n') f.write('='*50 + '\n') 运行结果如下： 后记这一次的selenium实战就介绍到这里了， 如果想实现多页爬取，同样道理，利用 EC.element_to_be_clickable（） 显示等待 “下一页”节点加载出来，然后模拟点击即可。 用没用感受到selenium的魅力呢？ 什么反爬什么加密统统不在话下！这可不是吹的~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day1-进程线程概念]]></title>
    <url>%2F2019%2F08%2F05%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay1-%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[前言网络编程很重要，在实际开发环境中，很多地方会用到， 举个例子，当我们的爬虫速率太慢了，我们可以开多进程多线程进行爬取；遇到 I/O 密集型任务，我们可以采用多线程处理。 今天介绍的进程线程是理论基础，应该熟练掌握。不熟悉进程线程概念的同学，认真理清其中的区别噢~ 操作系统还记得我们之前说过的操作系统基础吗？今天排上用场了~ 忘记的童鞋可以点击下方链接看一下： 计算机基础知识之操作系统 现代计算机系统是由一个或者多个处理器，主存，磁盘，打印机，键盘，鼠标显示器，网络接口以及各种其他输入输出设备组成的复杂系统，每位程序员不可能掌握所有系统实现细节，并且管理优化这些部件是一件挑战性极强的工作。所以，我们需要为计算机安装一层软件，也就是操作系统。操作系统的任务就是用户程序提供一个简单清晰的计算机模型，并管理以上所有设备。 操作系统是一个用来协调、管理和控制计算机硬件和软件资源的系统程序，其中包括：文件系统、内存管理、设备管理和进程管理。它位于硬件和应用程序之间。 简单来说，操作系统是存在于用户与程序之间的”中间商”，我们负责下令调度，它负责具体工作的实现。 程序？进程？我们知道，每一个软件都是由一串串代码构造出来的，而在我们电脑磁盘中，任何文件类型都只能以二进制存储。 那么，以一堆代码以文本形式存入一个文档，我们把这么一个二进制文件叫做程序。 当我们运行这个程序时，就会读取到内存，再用相应的解释器编译执行。当程序执行起来时，我们称之为进程。在我们的电脑上，每一个运行着的软件，就是一个进程（当然有子进程，后面会说） 进程的定义进程就是一个程序在一个数据集上的一次动态执行过程（程序运行的一种状态）。 进程一般由程序、数据集、进程控制块三部分组成。 程序：我们编写的程序用来描述进程要完成哪些功能以及主逻辑； 数据集则是程序在执行过程中所需要使用的资源； 进程控制块用来记录进程的外部特征，描述进程的执行变化过程，系统可以利用它来控制和管理进程（很重要的概念），它是系统感知进程存在的唯一标志。 进程切换我们知道，一个CPU在同一时刻只能处理一件事情，但是呢，CPU运行速度很快，它可以实现多个进程之间来回切换来达到我们看到的多个进程同时运行的效果。 进程是一种程序运行的状态，会涉及到状态挂起的保存，状态恢复、程序的识别、CPU资源的调用等等，因此，进程之间的切换是很消费资源的。 还有就是，进程的切换，具体是怎么样的呢？ 两个条件可以触发切换： ① 时间轮循 ： 每隔一段时间，CPU就会切换到其他进程。 ② I/O操作 ： 当遇到 输入输出操作时，CPU不会傻傻的等着，直接会进行切换操作。 线程定义线程的概念：“一个进程的独立运行片段”。 我们知道了，进程虽然可以完成切换，但是十分的消耗资源。线程的出现是为了降低上下文切换的消耗，提高系统的并发性，并突破一个进程只能干一样事的缺陷，使到进程内并发成为可能。 上面这三个任务，我们不能放到同一个进程里面，不然就无法实现三个任务的切换操作。 但又不想放到三个进程里（这三个任务实现的是同一个功能，所以三个进程之间需要互相访问，同时频繁的切换很费资源）。 线程也叫轻量级进程，它是一个基本的CPU执行单元，也是程序执行过程中的最小单元。——相当于进程里面的进程 线程的引入减小了程序并发执行时的开销，提高了操作系统的并发性能。线程没有自己的系统资源。所有的线程共用进程里面的资源 进程线程的关系 一个程序至少有一个进程,一个进程至少有一个线程.(进程可以理解成线程的容器)。 每一个进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。 进程是最小的资源单位，操作系统分资源只能分到进程（最小单位），而线程是最小的执行单位（程序真正在跑的，是线程） 乌拉！！ 进程线程介绍就到这里了。各位，下期见~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day10-Ajax异步请求]]></title>
    <url>%2F2019%2F08%2F04%2F%E7%88%AC%E8%99%ABDay10-Ajax%E5%BC%82%E6%AD%A5%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[Ajax引言：有时候我们在用 Requests 抓取页面的时候，得到的结果可能和在浏览器中看到的是不一样的，在浏览器中可以看到正常显示的页面数据，但是得到的 Response 并没有相应的内容。 这其中的原因是 Requests 获取的都是原始的 HTML 文档，而浏览器中的页面则是页面又经过 JavaScript 处理数据后生成的结果，这些数据的来源有多种： 通过 Ajax 加载 包含在了 HTML 文档中的（我们之前爬取的网站都是这个类型） 经过 JavaScript 经过特定算法计算后生成的。 1今天我们就来谈谈 Ajax 加载类型的网页数据。 Ajax的概念Ajax，全称为Asynchronous JavaScript and XML，即异步的 JavaScript和 XML， 对于Ajax 渲染 的页面，数据的加载是一种异步加载方式，原始的页面最初不会包含某些数据，原始页面加载完后会会再向服务器请求某个接口获取数据，然后数据再被处理才呈现到网页上，这其实就是发送了一个 Ajax请求。 这也就是我们直接使用 requests 请求 不到页面实际内容的原理所在。 这时需要做的就是分析网页的后台向接口发送的 Ajax 请求，再用Requests 来模拟 Ajax 请求，那就可以成功抓取了。 1知道了A jax 原理之后，一切就变得有头绪了些~ Ajax 观察对于Ajax 请求， 我们在现实生活中一直碰到，我描述一下你就知道了：“当你在浏览网页、APP的时候，下拉进度条到最下面时，会看到进度条唰的一下子跳到中间，然后下方有了新的内容出现” 其实这就是Ajax 的运用了。 今天，我们使用 微博来作为爬取对象。 url = https://m.weibo.cn/u/2145291155 浏览器发起请求老规矩，进入开发者模式。 然后在Network 下，点击 XHR 进行筛选（过滤出只有Ajax请求的请求响应报文） 让我们看第一个条目信息，观察其Response 发现其代码只有五十行，结构也非常简单，只是执行了一些 JavaScript。这只是最原始的链接返回的结果。 所以说，我们所看到的微博页面的真实数据并不是最原始的页面返回的，而是后来执行 JavaScript 后再次向后台发送了 Ajax 请求，拿到数据后再进一步渲染出来的。 于是我们进行测试，拖拽进度条到最下面，发现浏览器与服务器偷偷的交互了报文，效果如下： 新刷新出来的响应报文就是网站下方一些新的内容了 Ajax 渲染网页的就是这样，当用户看到下方时，才会加载内容。这样在 Web 开发上可以做到前后端分离，而且降低服务器直接渲染页面带来的压力。（以后可能都是这样类型的网页噢，赶紧学起来吧~） 查看详细信息我们点开这条响应报文进行分析。 Ajax其实有其特殊的请求类型，它叫作xhr。我们可以发现一个名称以getlndex开头的请求，其Type为xhr，这就是一个Ajax请求。 其中RequestHeaders 中有一个字段为 X-Requested-With：XMLHttpRequest，这就标记了此请求是Ajax异步请求 Ajax 结果提取我们要能将Ajax内容提取出来，就是要用Python来模拟这些Ajax请求，达到自动加载页面信息的功能。（第一步就是观察参数的规律，这是解析网页的必修课） ① URL的构建 观察一下这些请求，发现它们的 type、value、containerid 始终如一。type 始终为 uid，value 的值就是页面的链接中的数字，其实这就是用户的 id，另外还有一个containerid，经过观察发现它就是 一个常量 然后加上用户id。 所以改变的值就是 page，很明显这个参数就是用来控制分页的，page=1 代表第一页，page=2 代表第二页，以此类推。我们使用一个变量来控制页码数 ② 头部的构造 并且，我们需要构造header头部，模拟这是由浏览器发出的Ajax请求，还有一些重要字段（最基本的反爬策略） ③ 观看响应内容 它是一个 Json 格式，浏览器开发者工具自动为做了解析方便我们查看，可以看到最关键的两部分信息就是 cardlistInfo 和 cards，将二者展开，cardlistInfo 里面包含了一个比较重要的信息就是 total，经过观察后发现其实它是微博的总数量，我们可以根据这个数字来估算出分页的数目。 其中cards 则是一个列表，它包含了 10个元素，这就是每一条微博帖子存放的地方所在了。 ④我们继续展开看看 发现它包含的正是微博的一些信息。比如 attitudes_count 赞数目、comments_count 评论数目、reposts_count 转发数目、created_at 发布时间、text 微博正文等等 Python 代码模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445import requestsimport urllib.parsefrom pyquery import PyQuery as pqbase_url = 'https://m.weibo.cn/api/container/getIndex?'# 抓包得到的Ajax请求的url前部headers = &#123; 'host':'m.weibo.cn', 'Referer':'https://m.weibo.cn/u/2145291155', # 此内容用来标识这个请求是从哪个页面发过来的 'User-Agent':'Mozilla/5.0', 'X-Requested-With':'XMLHttpRequest'&#125;def get_page(page): params = &#123; 'type':'uid', 'value':'2145291155', 'containerid': '1076032145291155', 'page':page &#125; # 这个字典是我们根据xhr连接请求分析构造的 url = base_url+ urllib.parse.urlencode(params) # 构建Ajax请求的 URL response = requests.get(url=url,headers=headers) return response.json() # 返回json 格式的数据def parse_page(json): result = json.get('data').get('cards') result =result.pop() weibo = &#123;&#125; item = result.get('mblog') print(item) weibo['text'] = item.get('text') weibo['attitudes'] = item.get('attitudes_count') weibo['comments'] = item.get('comments_count') weibo['reposts'] = item.get('reposts_count') print(weibo)if __name__ == '__main__': for page in range(1): json = get_page(page) result = parse_page(json) 运行结果如下： 先抓取一页进行测试，完成！ 这样，我们只需要增加 变量 page，即可将微博数据全部爬取下来了。由于篇幅原因，略过。 写在后面：这次我们主要介绍了Ajax 请求的原理，这样一来，我们就可以模拟Ajax请求，对任何使用异步加载的网页进行爬取了。 啊~ 溜了溜了，胖友们，我们下回见。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day9-requests实战]]></title>
    <url>%2F2019%2F08%2F04%2F%E7%88%AC%E8%99%ABDay9-requests%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[前言之前介绍requests 这个库的时候在文尾说过要做一篇实战的文章，今天突然想起，然后就写一下咯。 爬虫Day5-requests介绍 今天的主题是——使用 requests请求库 和 re 解析库进行爬取当当网热门top 500书籍。 url = http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-1 网页省察第一步：我们进入url 链接之后，按F12进入开发者模式 总结出以下规律：所有书籍都存放在一个 ul 标签下面 每一本书都是 一个li 节点 第二步：切换页码数，观察 url 变化 发现 url 链接最后面的数字发生改变—— 由”1“ 变成 ”2“，这正好对应上我们翻页的页码数 每一页的书籍量为20 ， Top 500 的书籍 爬取 25 个连接就可以爬取完毕 动手写代码12345678def get_page(page): kv = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; # 伪装浏览器标识 url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page) # 由 变量 page 来进行控制翻页 response = requests.get(url=url, headers=kv) # 发出请求 return response.text # 将响应内容返回 解析我们点开的每一个 li 节点，观察我们需要爬取的节点信息 发现每一段信息都有一个div 存储着，我们只需要re 正则匹配出来，就欧克了 解析代码如下： 123456789101112131415def parser_page(html): regix = re.compile(r'&lt;li.*?list_num.*?"&gt;(\d+).*?&lt;img src="(.*?)".*?class="name".*?title="(.*?)"&gt;.*?class="tuijian"&gt;(.*?)&lt;/span&gt;.*?class="publisher_info.*?title="(.*?)".*?class="biaosheng".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?class="price_n"&gt;&amp;yen;(.*?)&lt;/span&gt;',re.S) # 编写匹配规则 doc = regix.findall(html) for i in doc: yield &#123; '排名':i[0], '图片':i[1], '书名':i[2], '推荐指数':i[3], '作者':i[4], '五星好评人数':i[5], '价格':i[6] &#125; # 将内容变成一个生成器 return doc 我们先看看 这个 叫 “doc” 生成器 的内容，使用 for 循环遍历出来： perfect！ 这样我们就把信息完美的抓取下来了。 写入文件接下来，我们就将内容写入文件，以便后期的数据分析。 1234with open("book.txt","w",encoding="utf-8") as f: for writeline in doc: print(writeline) f.write(str(writeline) + '\n') 我们运行一下代码，效果如下： 爬虫完善最后，我们需要控制爬取速率，做一个有素质的爬虫，毕竟一直爬取别人网站，服务器也会受不了的呀。并且，加上异常处理，一个简单的小爬虫就写好了。 完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import requestsimport timeimport redef get_page(page): kv = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page) response = requests.get(url=url, headers=kv) return response.textdef parser_page(html): regix = re.compile(r'&lt;li.*?list_num.*?"&gt;(\d+).*?&lt;img src="(.*?)".*?class="name".*?title="(.*?)"&gt;.*?class="tuijian"&gt;(.*?)&lt;/span&gt;.*?class="publisher_info.*?title="(.*?)".*?class="biaosheng".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?class="price_n"&gt;&amp;yen;(.*?)&lt;/span&gt;',re.S) doc = regix.findall(html) for i in doc: yield &#123; '排名':i[0], '图片':i[1], '书名':i[2], '推荐指数':i[3], '作者':i[4], '五星好评人数':i[5], '价格':i[6] &#125; return docdef main(): for i in range(1,26): html = get_page(i) doc = parser_page(html) time.sleep(4) with open("book.txt","w",encoding="utf-8") as f: for writeline in doc: print(writeline) f.write(str(writeline) + '\n')if __name__ == '__main__': main() 后记可以发现，使用 re 来解析库的难度还是有的，不过，我想说的是，使用什么模块并不重要，只有最适合当前需求的模块，取决于你如何去灵活运用它，编程语言也是这样。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吐槽下环境部署]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%90%90%E6%A7%BD%E4%B8%8B%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言今天，用了大半天的时间，部署了爬虫环境。 历经千辛万苦，终于全部搞定了。现在是晚上十一点，有些感概，对于这么一套流程，有一些思考，于是有了写下来的冲动。 鬼知道我经历了什么不夸大的说，今天遇到的BUG，已经超出了十几个，这么debug过来，一个又一个新的问题又爆出来。有时候，并不是那么顺风顺水。 当心情开始暴躁的时候，不妨将手头的事情放一放（进度不是很紧的话） 其次，要相信，在计算机中没有解决不了的问题。 在每一次遇到bug时，我都不会慌张，因为我知道，这是必然的、肯定会出现的一种现象，于是我面对bug的时候会很自信，因为最终它都会被我消灭掉。 真正的难题，是你如何去debug。 处理BUG的流程这套流程不一定适用于任何场景，但是作用于解决问题的参考还是行得通的。 出现报错一个BUG的出现往往是由ERROR这个字眼开始的，这时候，第一步要做的就是将它提示的内容，翻译为自己能理解的意思，再配合你当前所做的事情，联想一下，思考它为什么会报错。 一个谦卑的程序员总是先设想自己的代码是错的，排除之后再找其他方面的问题 谷歌、stackoverflow还有一点要相信的是，你遇到的BUG绝对不是历史第一次，什么意思呢？ 我们遇到的BUG，前辈们已经踏过无数次，我们可以利用互联网这个大资源，将自己遇到的东西，提取出关键字进行谷歌搜索，或者，直接将报错的代码直接复制粘贴。这是最简单粗暴的办法，但是没有前者精准。 所以，学会如何提问也是一门技术。 记录debug将问题解决了之后，如果你还是不能理解为什么，我们就可以将面临的问题、如何解决的，记下来。做好笔记，这是一个好的习惯。有很大几率，你会在某一段时间再次碰到你之前遇到的BUG，这时候，翻翻笔记，比盲目的谷歌好用多了，同时也促进了你对问题的理解。 为自己欢呼最终的最终，我们把BUG踩在脚下，这时候，请给自己鼓一下掌。因为这是不容易的，有句话是这么说的：“击不倒我的，必然使我更加强大” 既然选择了当程序猿，就不可能不遇到BUG，我们要学会坦然处之。这些debug的经验，都是你身为一名计算机人士的财富，你应该感谢一路走来的BUG，他成就了明天更强大的你。 写在最后今天被Docker 环境部署折腾的够呛，其中包含了 VirtualBox的配置、git 版本的更迭设置、 DockerToolbox 的配置 等等，其中遇到的难题之前都没接触过，历时半天，我终于将环境部署完成。等到看到内容完全输出到 git bash 的时候，真的挺开心。 最后，我想说的是，计算机是最公平的，对就是对，错就是错。只有耐心与它沟通，才能解决问题。 于 2019.8.02 晚 23.51 的深夜吐槽~]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>随便聊聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day8-Selenium魔法师]]></title>
    <url>%2F2019%2F07%2F31%2F%E7%88%AC%E8%99%ABDay8-Selenium%E9%AD%94%E6%B3%95%E5%B8%88%2F</url>
    <content type="text"><![CDATA[前言久违的爬虫Day 系列！！ 自从计算机网络系列更新完之后，我又把目光投到爬虫系列来了hhh… 今天介绍的技术可厉害了，在外人眼中，这近乎是“魔法”，那就是——selenium 同时，这也是我目前最为喜欢的爬取工具。 Selenium 是一个自动化测试工具，利用它我们可以驱动浏览器执行特定的动作，如点击、下拉等等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。用 Selenium 来驱动浏览器加载网页的话，我们就可以直接拿到 JavaScript 渲染的结果了，不管是什么加密统统不用再需要担心。 环境准备： selenium Chrome浏览器 与 ChromeDriver 以上内容，请自行百度安装 selenium介绍：模拟登陆：声明浏览器对象：selenium 驱动浏览器进行操作的，于是我们要实例化一个浏览器对象，有以下： 接下来我们要做的就是调用 生成的 browser 对象，让其执行各个动作，就可以模拟浏览器操作了。 访问页面：基于browser对象， 我们可以用 get() 方法来请求一个网页，参数传入链接 URL 即可（get方法没有返回值，他只是模拟了浏览器的运行） 123456from selenium import webdriverbrowser = webdriver.Chrome() # 实例化一个浏览器对象，后续操作都是针对此对象进行操作的browser.get('http://www.baidu.com') # 模拟登陆网页print(browser.page_source) # page_source（）方法输出了页面的源代码browser.close() # 使用close（）对浏览器进行关闭。 让我们看看效果： 有没有看到嗖的一下子，就访问了百度这个网站，然后解析出了HTML。 注意： 这里我们拿到了一个 browser 对象，这个在后续操作中，是最关键的（节点选择，动作交互等等，都是基于 browser对象进行操作） 查找节点Selenium 可以驱动浏览器完成各种操作，比如填充表单、模拟点击等等，比如我们想要完成向某个输入框输入文字的操作，在这这前我们需要得知这个输入框的位置。 所以 Selenium 提供了一系列查找节点的方法，我们可以用这些方法来获取想要的节点，以便于下一步执行一些动作或者提取信息。 我们想要驱动浏览器完成自动搜索的任务，首先要先找到文本输入框。图中&lt;input&gt;节点就是我们要找的目标，熟悉前端的胖友，肯定倍感亲切。 可以发现它的 ID 是 q，Name 也是 q，还有许多其他属性，那我们获取它的方式就有多种形式了，第一种是通过节点名称查找，比如find_element_by_name() 是根据 Name 值获取，find_element_by_id()是根据 ID 获取，另外还有根据XPath、CSS Selector 等获取的方式。 以下代码可以查找出文本框的节点： 123456789101112131415from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get("https://www.taobao.com")input_first = browser.find_element_by_id('q') # 通过节点名称指定input_second = browser.find_element_by_css_selector('#q') # css选择器input_third = browser.find_element_by_xpath('//*[@id="q"]') # 使用xpath路径list = [input_first,input_second,input_third]for i in list: print(i)time.sleep(3)browser.close() 效果演示： 进入到淘宝页面中，通过selenium的节点选择，使我们的光标直接移动到了文本框中。 可以看到三个节点都是 WebElement 类型，是完全一致的。证明三种方法选择效果是一样的。 其他获取节点的方法： 这些API 都通过节点名称表明了它的作用。 我们就可以利用这些API，获取我们需要的节点了。 节点交互Selenium 可以驱动浏览器来执行一些操作（文本输入和提交），也就是说我们可以让浏览器模拟执行一些动作，比较常见的用法有：输入文字用 send_keys() 方法，清空文字用 clear() 方法，另外还有按钮点击，用 click() 方法。 12345678910111213from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone') # 输入文字time.sleep(1)input.clear() # 清除文字input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search') # 获取搜索框的节点属性button.click() # 点击，在这里是提交搜索内容的意思 节点交互的方法基于WebElement对象 我们发现，每个搜素框都有对应的属性名、id值，我们需先利用find_element_by_id（） 函数找到他（返回一个WebElement对象），然后传递交互方法。这样就实现了模拟浏览器的效果 使用send_keys 方法输入文字（这个方法挺常用的，例如selenium模拟登陆，我们可以输入用户名密码…） 其他交互动作也是由各个API封装好了，具体观看官方文档 : https://selenium-python-zh.readthedocs.io/en/latest/ 动作链在上面的实例中，一些交互动作都是针对某个节点执行的，比如输入框我们就调用它的输入文字和清空文字方法，按钮就调用它的点击方法，其实还有另外的一些操作它是没有特定的执行对象的，比如鼠标拖拽、键盘按键等操作。所以这些动作我们有另一种方式来执行，那就是动作链。 我们下面使用一个网站来测试 selenium 的动作链 123456789101112131415from selenium.webdriver import ActionChains # 动作链使用的类browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')# selenium提供了switch_to.frame()方法来切换frame,括号内是传入的参数，# 用来定位frame，可以传入id、name、index以及selenium的WebElement对象source = browser.find_element_by_css_selector('#draggable') # 选择节点target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser) # 实例化动作链对象actions.drag_and_drop(source, target) # 基于此对象，进行拖动并放入的操作actions.perform() # 必须的调用，执行动作 效果演示： 搜的一下子，selenium 就出色的完成了拖动释放的操作，是不是很棒！ 执行JavaScript语句对于某些操作，SeleniumAPI 是没有提供的，如下拉进度条等，可以直接模拟运行 JavaScript，使用 execute_script() 方法即可实现。 123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To Bottom")') 效果演示： 不单单上面举例的JS语句，还能实现打开选项卡等其他操作 获取节点信息获取源代码 page_source 属性可以获取网页的源代码，获取源代码之后就可以使用解析库如正则、BeautifulSoup、PyQuery 等来提取信息了。不过既然 Selenium 已经提供了选择节点的方法，返回的是WebElement 类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等等。这样的话我们就可以不用通过解析源代码来提取信息了。 获取属性使用find_element获取节点，基于WebElement对象 ，使用 get_attribute() 方法，传入想要获取的属性名，就可以得到它对应的值了。 获取文本值原理同上，每个 WebEelement 节点都有 text 属性，我们可以通过直接调用这个属性就可以得到节点内部的文本信息了，就相当于 BeautifulSoup 的 get_text() 方法、PyQuery 的 text() 方法。 获取ID、相对位置、标签名、大小等信息 WebElement 节点还有一些其他的属性，比如 id 属性可以获取节点 id，location 可以获取该节点在页面中的相对位置，tag_name 可以获取标签名称，size 可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。 设置延时等待 动态加载的页面需要时间等待页面上的所有元素都渲染完成，如果在没有渲染完成之前我们就switch_to_或者是find_elements_by_，那么就可能出现元素定位困难而且会产生错误信息 在 Selenium 中，get() 方法会在网页框架加载结束之后就结束执行，此时如果获取 page_source ，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的 Ajax 请求（Ajax发出请求和服务器响应网页渲染需要一定时间），我们在网页源代码中也不一定能成功获取到。 所以这里我们需要延时等待一定时间确保节点已经加载出来。在这里等待的方式有两种，一种隐式等待，一种显式等待。 隐式等待当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是 0（固定的等待，如果已经加载出来，那么立即返回），如果超出设定时间后则抛出找不到节点的异常 显示等待实际情况中，显示等待用的比较多，因为隐式等待的效果其实并没有那么好，因为我们只是规定了一个固定时间，而页面的加载时间是受到网络条件影响的。 所以在这里还有一种更合适的显式等待方法，它直接指定好要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，那就返回查找的节点，如果到了规定时间依然没有加载出该节点，则会抛出超时异常。并且还有一个好处——当页面加载很慢时，使用显示等待，等到需要操作的那个元素加载成功之后就直接操作这个元素，不需要等待其他元素的加载。 123456789101112131415161718from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.get("https://www.zhihu.com/explore")wait = WebDriverWait(driver, 10) # 传入一个wait对象try: element = wait.until( EC.presence_of_element_located((By.CLASS_NAME, "zu-top-add-question"))) # 调用until方法 print(element) # 得到的依旧是Element对象finally: print("ok") driver.quit() 主要涉及到selenium.webdriver.support 模块下的expected_conditions类。 接着，我们首先引入了 Wait 这个对象，指定好最长等待时间，实例化出来一个 wait 等待对象。 然后调用它的 until() 方法。第一个参数传入要等待条件expected_conditions ，第二个参数传入对应的定位节点。比如在这里我们传入了 presence_of_element_located 这个条件，就代表当节点被加载出来时，条件被满足。其参数是节点的定位元组，也就是 class 为 “zu-top-add-question” 的节点搜索框。 所以这样可以做到的效果就是，在 10 秒内如果 ID 为 q 的节点即搜索框成功加载出来了，那就返回该节点，如果超过10 秒还没有加载出来，那就抛出异常。 设置Cookies使用 Selenium 还可以方便地对 Cookies 进行操作，例如获取、添加、删除 Cookies 等等。 我们使用了三次 get_cookies() 来显示对Cookies 的一些操作，可以看到 第三次调用之前，我们对cookies进行删除，因此打印此出空列表 异常处理在使用 Selenium 过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了，所以异常处理在程序中是十分重要的。我们可以使用 try except 语句来捕获各种异常。 1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: # 超时的异常处理 print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: # 没有找到节点的异常处理 print('No Element')finally: # 最终执行的语句 browser.close() 异常处理有利于我们后期的排错，也有利于爬取过程中一些错误的处理，一个完整的爬虫脚本应该含括异常处理。 我们把 selenium 大概的介绍完了！ 有没有像我一样，一接触selenium 就被他高明的抓取手段迷住了呢？哈哈]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day7-数据转发过程]]></title>
    <url>%2F2019%2F07%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay7-%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8F%91%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[实例引入——LAN计算机近距离构成的小型网络 叫局域网， 简称LAN。局域网能小到是同一个房间里的两台机器，或大到校园里的上千台机器。LAN技术中最为典型的就是“以太网”，开发于1970年代，今日仍被广泛使用。 CSMA/CD一条以太网电线连接数台计算机，这就构成了一个LAN。当一台计算机要传数据给另一台计算机时，它以电信号形式，将数据传入电缆。因为电缆是共享的，所以连在同一个网络里的其他计算机也看得到数据，但不知道数据是给它们的，还是给其他计算机的——为了解决这个问题 以太网需要每台计算机有唯一的 媒体访问控制地址 简称 MAC地址，这个唯一的地址放在头部，作为数据的前缀发送到网络中，所以，计算机只需要监听以太网电缆 只有看到自己的 MAC 地址，才处理数据。 多台电脑共享一个传输媒介， 这种方法叫 “载波侦听多路访问” 简称”CSMA”。而这种共享载体式网络有一个弊端：一个共享网段中，多个主机不能同时传输数据，其会发生冲突。 解决办法是，每一台计算机都监听电线中的信号检测这些冲突，当检测到冲突时，就停止传输，等待网络空闲, 然后再试一遍。其等待的时间使用“指数退避”算法进行计算，这样，每台计算机都有机会传输数据了。 因此，CSMA/CD 全程就叫做 ： 载波侦听多路访问 / 冲突检测 虽然CSMA/CD能够一定程度上的解决问题，但是，当主机数量多起来之后，意味着传输速率大大减少，我们使用交换机进行分割为多个“冲突域”，身处不同冲突域的主机传输互不影响。 传输数据的方式：电路交换：连接两台相隔遥远的计算机或网路，最简单的办法是分配一条专用的通信线路，这就叫电路交换。这种方法虽然能用，但是不灵活而且价格昂贵，因为总有闲置的线路，好处是：如果有一条专属于自己的线路你可以最大限度地随意使用，无需共享。 报文交换：“报文交换” 就像邮政系统一样，每个站点都知道下一站发哪里， 因为站点有“表格”，记录到各个目的地，信件该怎么传等等信息。报文交换的好处是 可以用不同路由，灵活性、可靠性大大提高。 而这个过程中，每个站点就是一个路由器，消息沿着路由跳转的次数叫”跳数”，记录跳数很有用，因为可以分辨出路由问题，有利于网络工作人员的差错排查。 报文交换的缺点之一是有时候报文比较大，会堵塞网络，因为要把整个报文从一站传到下一站后才能继续传递其他报文，设想一下，如果你现在有一个非常大的文件在传输，整条路都堵塞了。即便你只有一个1KB的小文件要传输也只能等大文件传完，或是选另一条效率稍低的路线。 因此，分组交换出现了… 分组交换：将大报文分成很多小块，叫”数据包”，就像报文交换 每个数据包都有目标地址，因此路由器知道发到哪里。报文具体格式由”互联网协议”定义，简称“IP协议”。将数据拆分成多个小数据包，然后通过灵活的路由传递，非常高效且可容错，如今互联网就是这么运行的。 有了上面大概的介绍，让我们看看，我们是如何实现上网的~ 分布式网络计算机首先要连到局域网（例如WIFI，也可能是4G），局域网再连到 WAN。WAN 的路由器一般属于你的”互联网服务提供商”，简称 ISP。就这么层层叠加，最终到达了互联网主干（互联网主干由一群超大型、带宽超高路由器组成）。 例如 我们现在在网络上请求了一个资源，数据包（packet）要先到互联网主干，沿着主干到达有对应文件的 服务器，最后拿到资源并返回，其过程跨越了多个路由器交换机。 在路由器交换机中传递的数据包依据 IP头部中的“信息”进行传递（我们之前提到过），最终使用传输层协议控制，你的计算机内核就拿到了资源。 数据转发过程：接下来我们谈谈… 数据转发的底层实现（前面几篇的汇总） 假设两台主机已经建立了TCP连接。主机对应用数据（Data）压缩加密 ，然后基于传输层协议封装数据 填充源端口跟目的端口（源端口一般是随机端口号），初始序列号和确认序列号字段、标识位、窗口字段等等信息。 随后进行网络层封装 ，一般使用IP封装时，需要源目IP地址，还有其他元数据： 如果IP报文大于MTU （最大传输单元） 则会被分片。 接着，封装生命周期TTL值（每经过一个三层设备这个值将减一 默认值是255 如果三层设备发送报文时 TTL值减为0 将丢弃数据包 这样就形成了防环机制 ） 协议字段标识了传输层所使用的协议，例如上层是TCP，所以该字段的填充值为0x06 （若是UDP 则0x017 ICMP是0x01） 接下来，数据包被封装成数据帧，需填充源目MAC地址字段。主机首先会查询ARP缓存表，如果有相应MAC地址，则直接封装，如果没有，则发送ARP请求： 不同网段通信时，发送ARP查询 ：如果ARP回应不在同一个网段，主机则封装网关的mac地址，对网关进行ARP请求（源MAC是本机，目的MAC是广播地址）， 路由器网关设备收到广播数据帧后 ，将源Mac地址记录到本地Mac缓存表中，并进行ARP回复。 如果ARP回应在同一个网段，那么直接使用ARP对目标主机发出请求 这时 主机便有了目的网段的mac地址 。主机在链路层封装数据帧时，会遵循一般为以太二型帧标准 （其中的帧头的Type字段为0x0800 表明上层是IP ，ARP是0x0806 ） 主机在半双工的状态下 采用CSMA/CD 机制来检测链路是否空闲 如果链路空闲 主机会将一个前导码跟帧首定界符附加到帧头并进行传输 前导码的作用是使接受设备进行同步并做好接受数据帧的准备， 前导码是包括 七个字节的 二进制0 1交替的代码 共56位。帧首定界符是由一个字节的01 交替的 二进制序列 他的作用是使接收端对帧的第一位进行定位。 主机发送数据帧到共享以太网中 身处同一个冲突域的设备都将收到数据帧 ，他们会做以下步骤的处理： ① 进行FCS校验 如果不通过就丢弃 ②对于通过的帧 设备会检查帧中的目的MAC地址 ，若与本地MAC地址不同 ，将进行丢弃 。 ③经过上面筛选，应该是合格的数据帧 ，这时，将帧头帧尾剥去（解封装），剩下数据报文会根据帧头的Type字段来送到上层对应协议模块去处理。 网络层的处理 ： ① 检查IP头部的校验和字段 ，看ip数据报文头部是否完整 ，然后根据目的IP地址查看路由表 ，确定是否能够将数据包转发给目的端 ② 设备在转发出去之前，会对TTL值进行处理，另外报文大小也不能超过MTU值 否则将分片，网络层处理完成后 报文将被送到数据链路层进行重新封装 ，添加新的源目MAC地址，但是源目IP地址则不会改变。 目的端最终收到数据包，执行以上步骤，接着查看Protocol字段，送往传输层进行处理 传输层协议 查看数据段头部信息的端口号 ，将数据段头部剥离后 将剩下的应用数据发送给应用层处理。 这个就是数据转发过程中发生的事情，其中还省略了物理层、应用层等具体实现。不过计算机网络主要关注的是网络层、传输层，所以有所侧重。 计算机网络系列更新完结啦~ 撒花撒花！！ 这里贴出几篇之前写的计算机网络的文章： 计算机网络Day4-TCP协议 计算机网络Day3-网络层 计算机网络Day2-数据链路层]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day6-应用层协议]]></title>
    <url>%2F2019%2F07%2F29%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay6-%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[这一篇文章主要讲述的是应用层协议的几个协议 —— DNS 、 HTTP 和 DHCP DNS协议域名(domain name)是IP地址的代号。域名通常是由字符构成的。对于人类来说，字符构成的域名，比如www.yahoo.com，要比纯粹数字构成的IP地址(106.10.170.118)容易记忆。域名解析系统(DNS, domain name system)就负责将域名翻译为对应的IP地址。在DNS的帮助下，我们可以在浏览器的地址栏输入域名，而不是IP地址。 另一方面，处于维护和运营的原因，一些网站可能会变更IP地址。这些网站可以更改DNS中的对应关系，从而保持域名不变，而IP地址更新。由于大部分用户记录的都是域名，这样就可以降低IP变更带来的影响。 DNS服务器域名和IP地址的对应关系存储在DNS服务器(DNS server)中。所谓的DNS服务器，是指在网络中进行域名解析的一些服务器(计算机)。这些服务器都有自己的IP地址，并使用DNS协议(DNS protocol)进行通信。DNS协议主要基于UDP，是应用层协议 DNS服务器构成一个分级(hierarchical)的树状体系。一次DNS查询就是从树的顶端节点出发，最终找到相应末端记录的过程。 查询的顺序：中间节点根据域名的构成，将DNS查询引导向下一级的服务器。比如说一个域名cs.berkeley.edu，DNS解析会将域名分割为cs, berkeley, edu，然后按照相反的顺序查询(edu, berkeley, cs)。出口DNS首先根据edu，将查询指向下一层的edu节点。然后edu节点根据berkeley，将查询指向下一层的berkeley节点。这台berkeley服务器上存储有cs.berkeley.edu的IP地址。所以，中间节点不断重新定向，并将我们引导到正确的记录。 在整个DNS查询过程中，无论是重新定向还是最终取得对应关系，都是用户计算机和DNS服务器使用DNS协议通信。用户计算机根据DNS服务器的反馈，依次与下一层的DNS服务器建立通信。用户计算机经过递归查询，最终和末端节点通信，并获得IP地址。 DNS缓存用户计算机的操作系统中的域名解析模块(DNS Resolver)负责域名解析的相关工作。任何一个应用程序(邮件，浏览器)都可以通过调用该模块来进行域名解析。 并不是每次域名解析都要完整的经历解析过程。DNS Resolver通常有DNS缓存(cache)，用来记录最近使用和查询的域名/IP关系。在进行DNS查询之前，计算机会先查询cache中是否有相关记录。这样，重复使用的域名就不用总要经过整个递归查询过程。 HTTP协议：我们在TCP流通信中说明了，TCP协议实现了数据流的传输。然而，在实践中发现，人们往往习惯以文件为单位传输资源，比如文本文件，图像文件，超文本文档(hypertext document)。超文本文档中包含有超链接，指向其他的资源。超文本文档是万维网(World Wide Web，即www)的基础。 HTTP协议是应用层协议，它随着万维网发展起来。HTTP协议最初只是一套实践标准。其本质目的是，如何在万维网的网络环境下，更好的使用TCP协议(尽管HTTP协议也可以用UDP协议作为底层，但绝大部分都是基于TCP协议)，以实现文件，特别是超文本文件的传输。 早期的HTTP协议主要是传输静态文件，也就是存储在服务器上的文件。随着万维网的发展，HTTP协议被用于传输“动态文件”，这样的文件是服务器上的程序根据HTTP请求即时生成的文件。我们将HTTP的传输对象统称为资源(resource)。 HTTP交互过程HTTP协议的通信是一次request-responce交流。客户端(guest)向服务器发出请求(request)，服务器(server)回复(response)客户端。 通过上图一问一答式的交互，就完成了资源请求与资源响应。HTTP协议规定了请求和回复需要遵循的格式。请求和回复需要满足下面的格式: 12345起始行(start line)头信息(headers)主体(entity body) 起始行只有一行。它包含了请求/回复最重要的信息。请求的起始行表示“想要什么”。回复的起始行表示”响应的简要信息”。 头信息可以有多行。每一行是一对键值对(key-value pair)，比如:Content-type: text/plain 它表示，包含有一个名为Content-type的参数，该参数的值为text/plain。头信息是对起始行的补充。请求的头信息对服务器有指导意义 主体部分包含了具体的资源。上图的请求中并没有主体，因为这是个GET请求 (请求是可以有主体内容的)。回复中包含的主体是一段文本文字(Hello World!)。 请求报文：123我们深入一些细节。先来看一下请求：GET /index.html HTTP/1.1 # 起始行Host:www.example.com # 头信息 起始行 ： GET /index.html HTTP/1.1 GET 方法。用于说明想要服务器执行的操作。 /index.html 资源的路径。这里指向服务器上的index.html文件。 HTTP/1.1 协议的版本。HTTP第一个广泛使用的版本是1.0，当前版本为1.1。 头信息 ：Host:www.example.com 该头信息的名字是Host。HTTP的请求必须有Host头信息，用于说明服务器的地址和端口。HTTP协议的默认端口是80，如果在HOST中没有说明端口，那么将默认采取该端口。 响应报文：服务器在接收到请求之后，会根据程序，生成对应于该请求的回复，比如： 123HTTP/1.1 200 OK # 起始行Content-type: text/plain # 头部字段标识Hello World! # 响应内容 一、起始行： HTTP/1.1 协议版本 200 状态码(status code)。 OK 状态描述 OK是对状态码200的文字描述，它只是为了便于人类的阅读。电脑只关心三位的状态码(status code)，200 表示访问正常 其它常见的状态码还有: 302，重新定向(redirect): 我这里没有你想要的资源，但我知道另一个地方xxx有，你可以去那里找。 404，无法找到(not found): 我找不到你想要的资源，无能为力。 (重新定向时，客户端可以根据302的建议前往xxx寻找资源，也可以忽略该建议。) 二、字段：Content-type：说明了主体所包含的资源的类型。根据类型的不同，客户端可以启动不同的处理程序(比如显示图像文件，播放声音文件等等)。下面是一些常见的资源： text/plain 普通文本 text/html HTML文本 image/jpeg jpeg图片 image/gif gif图片 三、响应内容：服务器响应的内容主要信息都存放于此 HTTP无状态根据早期的HTTP协议，每次request-reponse时，都要重新建立TCP连接。TCP连接每次都重新建立，所以服务器无法知道上次请求和本次请求是否来自于同一个客户端。因此，HTTP通信是无状态(stateless)的。服务器认为每次请求都是一个全新的请求，无论该请求是否来自同一地址。 详细的 HTTP 协议介绍 可以阅读 我之前的爬虫文章： 爬虫Day1-网页基础 DHCP协议：DHCP协议用于动态的配置电脑的网络相关参数，如主机的IP地址，路由器出口地址、DNS域名服务器地址等。一台电脑只要接上网，就可以通过DHCP协议获得相关配置（例如IP地址等配置信息） DHCP协议全称为“动态主机设置协议”（Dynamic Host Configuration Protocol）。通常来说，普通电脑中都内置有DHCP客户端模块。电脑接上网络后，DHCP客户端发现新连通的网络，会在该网络上找DHCP服务器。DHCP服务器将给电脑提供合理的网络配置，并把设置信息传回本机。所谓的DHCP服务器，其实就是一些运行有DHCP服务器端软件的特殊电脑。本机和DHCP服务器之间的通信，都是通过DHCP协议进行的。 地址分配DHCP服务器的首要任务是分配IP地址。分配的IP地址要符合以下原则： 地址合理，即对应该局域网的IP地址和子网掩码。 地址空闲，同一网络下没有其他设备使用该地址。 地址池DHCP服务器上存有一个地址池，里面是可用的IP地址，当有一台主机接入该局域网时，DHCP服务器就会从地址池中取出一个IP地址分配给主机。此外，服务器还会说明IP地址的占用时间，也就是租期： 当然，主机使用网络的时间可能超过租期。如果主机在租期到时都没有联系DHCP服务器，那么DHCP服务器会收回IP地址，再分配给其他主机。可如果主机想继续使用IP地址，就要在中途申请延长租期。收到申请的DHCP服务器通常会答应主机的请求，允许它继续使用现有IP地址。 有了动态分配，DHCP服务器不但简化了网络配置过程，还可以有效利用IP地址资源。利用不断分配收回IP地址，实现地址的灵活分配使用，其利用率大大提高。 通信过程DHCP协议的底层是UDP协议。使用UDP的广播，把UDP数据包发送到网络的广播地址，网络上的每个设备都能收到。因此，DHCP通信主要靠这种广播的形式进行。 DHCP通信分为四步： 客户机发广播，搜寻DHCP服务器（一个网段不一定只有一台DHCP服务器）。 DHCP服务器发出Offer报文，提供一个可用的IP地址。 客户机携带该IP地址发出Request报文请求。 DHCP服务器回复ACK报文进行确认，并提供其他配置参数。 应用层的介绍就结束啦。最近更新可谓非常频繁了，啊哈哈]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day5-UDP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay5-UDP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[UDP协议简介UDP(User Datagram Protocol)传输与IP传输非常类似。你可以将UDP协议看作IP协议暴露在传输层的一个接口。UDP协议同样以数据包(datagram)的方式传输。 当应用程序对传输的可靠性要求不高，但是对传输速度和延迟要求较高时，可以用UDP协议来替代TCP协议在传输层控制数据的转发。UDP将数据从源端发送到目的端时，无需事先建立连接。UDP采用了简单、易操作的机制在应用程序间传输数据，没有使用TCP中的确认技术或滑动窗口机制，因此UDP不能保证数据传输的可靠性，也无法避免接收到重复数据的情况。 UPD的诞生：尽管UDP协议非常简单，但它的产生晚于更加复杂的TCP协议。早期的网络开发者开发出IP协议和TCP协议分别位于网络层和传输层，所有的通信都要先经过TCP封装，再经过IP封装(应用层-&gt;TCP-&gt;IP)。开发者将TCP/IP视为相互合作的套装。但很快，网络开发者发现，IP协议的功能和TCP协议的功能是相互独立的。对于一些简单的通信，我们只需要简单的IP传输就可以了，而不需要TCP协议复杂的建立连接的方式(如果过多的建立TCP连接，会造成很大的网络负担，而UDP协议可以相对快速的处理这些简单通信)。UDP协议随之被开发出来。 UDP运用场景：UDP适合传输对延迟敏感的流量，如语音和视频。在使用TCP协议传输数据时，如果一个数据段丢失或者接收端对某个数据段没有确认，发送端会重新发送该数据段。 让我们设想一下，平时打电话过程中，偶尔出现的信号不好的情况，此时，如果对方在说话，那么我们肯定是听不清楚的，而这些遗失的语音是不会重新出现的（除非你叫对方重新讲一遍） 上面的打电话案例其传输层协议就是使用的UDP，而不是TCP。UDP采用实时传输机制和时间戳来传输语音和视频数据。 使用UDP传输数据时，由应用程序根据需要提供报文到达确认、排序、流量控制等功能。 UDP说的东西不多，只要我们IP协议学的好，UDP并不是很大难题，重点还是TCP的熟练掌握。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day4-TCP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay4-TCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[传输层协议之TCP协议传输层最重要的协议为TCP协议和UDP协议。TCP协议复杂且面向连接，但传输可靠。UDP协议简单且不面向连接，传输不可靠。 TCP比较出众的一点就是提供一个可靠的，流控的数据传输。而传输层的出现有个重要的原因：端口。 IP协议进行的是IP地址到IP地址的传输，这意味者两台计算机之间的对话。但每台计算机中需要有多个通信通道，并将多个通信通道分配给不同的进程使用。一个端口就代表了这样的一个通信通道。网络层在逻辑上提供了端口的概念。一个IP地址可以有多个端口。一个具体的端口需要IP地址和端口号共同确定(我们记为IP:port的形式)。 TCP协议的两个特性：一、双向连接： TCP连接是双工(duplex)的。双向连接实际上就是建立两个方向的TCP传输。因此，主机与服务器的交互可能是双向的~ 二、TCP端口号 TCP允许一个主机同时运行多个应用进程。每台主机可以拥有多个应用端口，每对端口号、源和目标IP地址的组合唯一地标识了一个会话。端口分为知名端口和动态端口。 有些网络服务会使用固定的端口，这类端口称为知名端口，端口号范围为0-1023。动态端口号范围从1024到65535 TCP 头部： TCP通常使用IP作为网络层协议,这时TCP数据段被封装在IP数据包内。TCP数据段由TCP Header（头部）和TCP Data（数据）组成。TCP最多可以有60个字节的头部，如果没有Options字段，正常的长度是20字节。 TCP Header是由上图标识一些字段组成，下面是对字段的介绍： 16位源端口号：源主机的应用程序使用的端口号。 16位目的端口号：目的主机的应用程序使用的端口号。每个TCP头部都包含源和目的端的端口号，这两个值加上IP头部中的源IP地址和目的IP地址可以唯一确定一个TCP连接。 32位序列号：用于标识从发送端发出的不同的TCP数据段的序号。数据段在网络中传输时，它们的顺序可能会发生变化；接收端依据此序列号，便可按照正确的顺序重组数据。 32位确认序列号：用于标识接收端确认收到的数据段。确认序列号为成功收到的数据序列号加1。 4位头部长度：表示头部占32bit字的数目，它能表达的TCP头部最大长度为60字节。 16位窗口大小：表示接收端期望通过单次确认而收到的数据的大小。由于该字段为16位，所以窗口大小的最大值为65535字节，该机制通常用来进行流量控制。 16位校验和：校验整个TCP报文段，包括TCP头部和TCP数据。该值由发送端计算和记录并由接收端进行验证。 TCP 三次握手建立起一个TCP连接需要经过“三次握手”： 第一次握手：客户端发送请求建立连接报文（seq = x，同时SYN置1，表示第一次建立连接）到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到SYN包，回复客户的请求报文（seq =y，ack=x+1），同时回包，即SYN+ACK包（SYN、ACK置1，ACK置1表示同意建立连接），此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手” TCP四次挥手： 我们可以看到，连接终结的过程中，连接双方也交换了四片信息(两个FIN和两个ACK)。TCP并没有合并FIN与ACK片段。原因是TCP连接允许单向关闭(half-close)。 假设Client端发起中断请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说“我client端要发给你了”，但是如果你还没有数据要发送完成，则不必急着关闭Socket，可以继续发送数据。所以所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。 当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，”就知道可以断开连接了”。Client端等待了2MSL(最大报文段生存时间)后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。 Ok，TCP连接就这样关闭了！ “流”通信TCP协议是传输层协议，实现的是端口到端口(port)的通信。更进一步，TCP协议虚拟了文本流(byte stream)的通信。在TCP协议与”流”通信中讨论的TCP传输需要一个前提：TCP连接已经建立。 IP协议和UDP协议采用的是数据包的方式传送，后发出的数据包可能早到，我们并不能保证数据到达的次序。 TCP协议确保了数据到达的顺序与文本流顺序相符。当计算机从TCP协议的接口读取数据时，这些数据已经是排列好顺序的“流”了。比如我们有一个大文件要从本地主机发送到远程主机，如果是按照“流”接收到的话，我们可以一边接收，一边将文本流存入文件系统。这样，等到“流”接收完了，硬盘写入操作也已经完成。如果采取UDP的传输方式，我们需要等到所有的数据到达后，进行排序，才能组装成大的文件。 片段与编号我们知道了TCP是通过报文流进行通信的，那么同时发出的多个报文流，对端是如何按正确的顺序进行组装呢？ TCP片段的头部(header)会存有该片段的序列号(sequence number)。这样，接收的计算机就可以知道接收到的片段在原文本流中的顺序了，也可以知道自己下一步需要接收哪个片段以形成流。比如已经接收到了片段1，片段2，片段3，那么接收主机就开始期待片段4。如果接收到不符合顺序的数据包(比如片段5)，接收方的TCP模块可以拒绝接收，从而保证呈现给接收主机的信息是符合次序的“流”。 TCP 可靠性 TCP的可靠性是这么体现的：在每收到一个正确的、符合次序的片段之后，就向发送方(也就是连接的另一段)发送一个特殊的TCP片段，回复一个ACK给发送方(ACK，acknowledge)：“我已经收到那个片段了。” 这个特殊的TCP片段叫做ACK回复。如果一个片段序号为L，对应ACK回复有回复号L+1，也就是接收方期待接收的下一个发送片段的序号。如果发送方在一定时间等待之后，还是没有收到ACK回复，那么它推断之前发送的片段一定发生了异常。发送方会重复发送(retransmit)那个出现异常的片段，等待ACK回复，如果还没有收到，那么再重复发送原片段… 直到收到该片段对应的ACK回复(回复号为L+1的ACK)。 当发送方收到ACK回复时，它看到里面的回复号为L+1，也就是发送方下一个应该发送的TCP片段序号。发送方推断出之前的片段已经被正确的接收，随后发出L+1号片段。ACK回复也有可能丢失。对于发送方来说，这和接收方拒绝发送ACK回复是一样的。发送方会重复发送，而接收方接收到已接收过的片段，推断出ACK回复丢失，会重新发送ACK回复。 通过ACK回复和重新发送机制，TCP协议将片段传输变得可靠。但我们也看出来了—— 这也太麻烦了吧！ 而TCP 的连接是“可靠的”，这一点还体现在 滑动窗口机制和重传机制上。 TCP 滑动窗口机制：滑窗：上面的工作方式中，发送方保持发送-&gt;等待ACK-&gt;发送-&gt;等待ACK…的单线工作方式，这样的工作方式叫做stop-and-wait。stop-and-wait虽然实现了TCP通信的可靠性，但同时牺牲了网络通信的效率。在等待ACK的时间段内，我们的网络都处于闲置(idle)状态。我们希望有一种方式，可以同时发送出多个片段。然而如果同时发出多个片段，那么由于IP包传送是无次序的，有可能会生成乱序片段。 在stop-and-wait的工作方式下，乱序片段完全被拒绝（序列号对不上，会等待重发），这也很不效率。毕竟，乱序片段只是提前到达的片段。我们可以在缓存中先存放它，等到它之前的片段补充完毕，再将追加在后面。 然而，如果一个乱序片段实在是太过提前(太“乱”了)，该片段将长时间占用缓存。我们需要一种折中的方法来解决该问题：利用缓存保留一些“不那么乱”的片段，期望能在段时间内补充上之前的片段(暂不处理，但发送相应的ACK)；对于“乱”的比较厉害的片段，则将它们拒绝(不处理，也不发送对应的ACK)。 这个机制就是——窗口机制 窗口机制TCP滑动窗口技术可以通过动态改变窗口大小来实现对端到端设备之间的数据传输进行流量控制。 滑窗(sliding window)被同时应用于接收方和发送方，以解决以上问题。发送方和接收方各有一个滑窗。当片段位于滑窗中时，表示TCP正在处理该片段。滑窗中可以有多个片段，也就是可以同时处理多个片段。滑窗越大，越大的滑窗同时处理的片段数目越多(当然，计算机也必须分配出更多的缓存供滑窗使用)。 流量控制主机和服务器之间可以通过滑动窗口来实现流量控制。 TCP协议会根据情况自动改变滑窗大小，以实现流量控制。流量控制(flow control)是指接收方将 window的大小通知给发送方，从而指导发送方修改发送 window的大小。接收方将该信息放在TCP头部的window size区域：发送方在收到window size的通知时，会调整自己滑窗的大小。这样，发送窗口变小，文本流发送速率降低，从而减少了接收方的负担。 TCP重传机制：超时重传机制超时重传机制是指：当发送方送出一个TCP片段后，将开始计时，等待该TCP片段的ACK回复。如果接收方正确接收到符合次序的片段，接收方会利用ACK片段回复发送方。发送方得到ACK回复后，继续移动窗口，发送接下来的TCP片段。如果直到计时完成，发送方还是没有收到ACK回复，那么发送方推断之前发送的TCP片段丢失，因此重新发送之前的TCP片段。这个计时等待的时间叫做重新发送超时时间(RTO, retransmission timeout)。 快速重传机制快速发送机制如果被启动，将打断计时器的等待，直接重新发送TCP片段。 快速重传机制：实现了另外的一种丢包评定标准，即如果连续收到3次 ACK，发送方就认为这个seq的包丢失了，立刻进行重传，这样如果接收端回复及时的话，基本就是在重传定时器到期之前，提高了重传的效率。 TCP的可靠机制虽然保证了数据安全，但是往往也带来某些方面的缺陷：例如TCP慢启动和TCP全局同步现象。 TCP的缺陷：一个概念——慢启动：TCP连接刚建立时，不是一下子就能把握TCP窗口值大小的，会从一个小的窗口值慢慢往上加，最后协商成功，达到稳定的windows大小。 TCP全局同步：当网络发生堵塞时，报文就不能及时送达目的端。对于TCP报文，如果大量的报文被丢弃，将造成TCP超时，从而引发TCP慢启动，使得TCP减少报文的发送。当队列同时丢弃多个TCP连接的报文时，将造成多个TCP连接同时进入拥塞避免和慢启动状态以调整并降低流量，这就被称为TCP全局同步现象。这样多个TCP连接发往队列的报文将同时减少，而后又会在某个时间同时出现流量高峰，如此反复，使网络资源利用率低。 资源消耗：TCP 最大的缺点是那些”确认码”数据包把数量翻了一倍，但并没有传输更多信息。有时候这种代价是不值得的，于是有了UDP这个协议来代替一些用户的其他需求（下一篇我们会讲到） 这一章节我不知不觉就写了四千字（捂脸），可见TCP之重要性… 刚开始学TCP的时候，我还没有意识到问题的严重性…hhh 这几天写的博客有点多，因为热爱，所以无惧！ 当回过头来看这些文章时，更多的是一种自豪感，我会继续坚持下去的。Fighting！]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day3-网络层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay3-%E7%BD%91%E7%BB%9C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[网络层：网络层(network layer)是实现互联网最重要的一层。正是在网络层面上，各个局域网根据IP协议相互连接，最终构成覆盖全球的Internet。更高层的协议，无论是TCP还是UDP，必须通过网络层的IP数据包(datagram)来传递信息。操作系统也会提供该层面的套接字(socket)，从而允许用户直接操作IP包。 IP数据包： IP数据包简称为IP包。它是符合IP协议的0/1序列。信息包含在这一序列中。IP包分为头部(header)和数据(Data)两部分。数据部分是要传送的信息，头部是为了能够实现传输而附加的信息。 与帧类似，IP包的头部也有多个区域。我们将注意力放在源地址(source address)和目的地(destination address)。它们都是IP地址。IPv4的地址为4 bytes的长度(也就是32位)。我们通常将IPv4的地址分为四个十进制的数，每个数的范围为0-255,比如192.0.0.1就是一个IP地址。填写在IP包头部的是该地址的二进制形式。 IP地址是全球地址，它可以识别局域网和主机。这是通过将IP地址分类实现的。下图则为 IP 地址分类： 每个IP地址的32位分为前后两部分，第一部分用来区分局域网，第二个部分用来区分该局域网的主机。 子网掩码(Subnet Mask)告诉我们这两部分的分界线，比如255.0.0.0(也就是8个1和24个0)表示前8位用于区分局域网，后24位用于区分主机。由于A、B、C分类是已经规定好的，所以当一个IP地址属于B类范围时，我们就知道它的前16位和后16位分别表示局域网和主机。 网卡与路由器网卡（NIC）： IP地址是分配给每个计算机的。但这个说法并不精确。IP地址实际上识别的是网卡(NIC, Network Interface Card)。 网卡是计算机的一个硬件，它在接收到网路信息之后，将信息交给计算机的CPU处理。当计算机需要发送信息的时，也要通过网卡发送。一台计算机可以有不只一个网卡，比如笔记本就有一个以太网卡和一个WiFi网卡。计算机在接收或者发送信息的时候，要先决定想要通过哪个网卡。 路由器： 路由器(router)实际上就是一台配备有多个网卡的专用电脑。它让网卡接入到不同的网络中。他专门负责与多个网络之间的寻址与通信。跨网段的数据通信一般都会经过路由器 路由寻址：IP包的传输要通过路由器的接力。每一个主机和路由中都存有一个路由表(routing table)。路由表根据目的地的IP地址，规定了等待发送的IP包所应该走的路线。 始发：比如我们从主机145.17生成发送到146.21的IP数据包，剩下数据部分可以是TCP包，可以是UDP包，我们暂时不关心。注明目的地IP地址(199.165.146.21)和发出地IP地址(199.165.145.17)。主机145.17随后参照自己的路由表，看到路由表中的记录： 145.17 routing table (Genmask为子网掩码,Iface用于说明使用哪个网卡接口) 这里有两行记录： 第一行，如果IP目的地是199.165.145.0这个网络的主机，那么只需要自己在eth0上的网卡直接传送(“本地社区”：直接送达)，不需要前往路由器(Gateway 0.0.0.0 = “本地送信”)。 第二行，所有不符合第一行的IP目的地，都应该送往Gateway 199.165.145.17，也就是中间路由器接入在eth0的网卡IP地址。因为 目的地址和子网掩码全为 0.0.0.0 是缺省路由（当匹配不到路由明细时，就会往这个接口发出） 中转我们的IP包目的地为199.165.146.21，不符合第一行，所以按照第二行，发送到中间的路由器。主机145.17会将IP包放入帧的payload，并在帧的头部写上199.165.145.17对应的MAC地址，这样，就可以在局域网中传送了。 中间的路由器在收到IP包之后(实际上是收到以太协议的帧，然后从帧中的payload读取IP包)，提取目的地IP地址，然后对照自己的路由表： 送达：数据包的目的地符合第二行，所以将IP放入一个新的帧中， 在帧的头部写上199.165.146.21的MAC地址，直接发往主机146.21。 总结： 这样，我们就完成了“路由”的过程。整个过程中，IP包不断被主机和路由封装入帧并拆开，然后借助连接层，在局域网的各个NIC之间传送帧。整个过程中，我们的IP包的内容保持完整，没有发生变化（有变化的是TTL值、FCS校验码等字段）。最终的效果是一个IP包从一个主机传送到另一个主机。利用IP包，我们不需要去操心底层(比如连接层)发生了什么。 路由协议：之前的信息传递基于一个假设：每个人手里都有份准确的地图。用计算机的话来说，就是每个主机和路由上都已经有一张路由表。这个路由表描述了网络上的路径信息。如果你了解自己的网络连接，可以手写自己主机的路由表。但是，一个路由器可能有多个出口，所以路由表可能会很长。更重要的是，周围连接的其他路由器可能发生变动(比如新增路由器或者拓扑变更)，我们就需要路由表能及时将交通导向其他的出口。我们需要一种更加智能的探测周围的网络拓扑结构，并自动生成路由表—— 路由协议 一些比较出名的路由协议有： OSPF、BGP、ISIS 等 这里不过多介绍，不然可以讲一天 =.=]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day2-数据链路层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay2-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82%2F</url>
    <content type="text"><![CDATA[以太网的帧格式帧本身是一段有限的0/1序列。它可以分为以下几部分： Preameble 和 SFD： 帧的最初7个字节被称为前导码，前导码是包括 七个字节的二进制0 1交替的代码 共56位。 定界符是由一个字节的01 交替的 二进制序列 ，他的作用是使接收端对数据帧的第一位进行定位 通常，我们都会预定好以一定的频率发送0/1序列(比如每秒10bit)。如果接收设备以其他频率接收(比如每秒5bit)，那么就会错漏掉应该接收的0/1信息。但是，由于网卡的不同，发送方和接收方即使预订的频率相同，两者也可能由于物理原因发生偏差。这就好像两个人约好的10点见，结果一个人表快，一个人表慢一样。前导码是为了让接收设备调整接收频率，以便与发送设备的频率一致。 dst 和 src 紧跟着后面的是6 字节的目的地和6 字节的源地址。要注意，这里写的是MAC地址。MAC地址是物理设备自带的序号，只能在同一个以太网中被识别 Data 数据一般包含有符合更高层协议的数据，比如IP包。连接层协议本身并不在乎数据是什么，它只负责传输。注意，数据尾部可能填充有一串0(PAD区域)。填充0的原因是，一个帧需要超过一定的最小长度。 数据的字节长限制为 64～1500字节： 最大包长1518字节，其中数据1500字节为MTU，18字节为字段值 最小包长64字节，为了限制冲突，有足够时间检测到冲突（CSMA/CD） Type字段： type字段两个字节，它用于区分两种帧类型，当type字段值小于1500，则是IEEE802.3格式。当type字段值大于或等于1536，帧使用的是以太Ethernet格式。0×0800标识IP，0×0806标识ARP) FCS效验码 接着是校验序列(FCS, Frame Check Sequence)。校验序列是为了检验数据的传输是否发生错误。在物理层，我们通过一些物理信号来表示0/1序列(比如高压/低压，高频率/低频率等)，但这些物理信号可能在传输过程中受到影响，以致于发生错误。 FCS采用了循环冗余校验(CRC, Cyclic Redundancy Check)算法。利用特定的算法得到的余数与实际拿到数据包的FCS进行比较， 如果两者不相符，我们就知道数据在传输的过程中出现错误，不能使用。 以上的数据包封装需要MAC 地址，本端的MAC地址我们知道（由厂商分配写好），那么对端的MAC地址我们如何得知呢？ ARP协议ARP协议介于连接层和网络层之间，ARP包需要包裹在一个帧中。它的工作方式如下：主机会发出一个ARP包，该ARP包中包含有自己的IP地址和MAC地址。通过ARP包，主机以广播的形式询问局域网上所有的主机和路由：我是IP地址xxxx，我的MAC地址是xxxx，有人知道199.165.146.4的MAC地址吗？拥有该IP地址的主机会回复发出请求的主机：哦，我知道，这个IP地址属于我的一个NIC，它的MAC地址是xxxxxx。由于发送ARP请求的主机采取的是广播形式，并附带有自己的IP地址和MAC地址，其他的主机和路由会同时检查自己的ARP cache，如果不符合，则更新自己的ARP cache。 这样，经过几次ARP请求之后，ARP cache会达到稳定。如果局域网上设备发生变动，ARP重复上面过程。 总的来说： APR协议就是通过IP 地址请求对端的MAC地址的过程。 免费arp ： 主机被分配IP地址或者IP地址发生变更之后，检测该网段是否唯一，避免冲突。主机用过发送一个arp request 报文进行检测，将广播报文中目的ip地址设置为自己的IP地址。当收到其他主机的回复时，则表示该网段被占用。 ICMP协议：ICMP(Internet control message protocol)是网络层的的协议，用来在网络设备间传递各种差错和控制信息，他对于收集各种网络信息，诊断和排除各种网络故障具有至关重要的作用。 ICMP重定向，ICMP差错检查:用于诊断源和目的之前的连通性(ICMP echo request查询和ICMP echo reply响应)，ICMP错误报告(各种传输数据失败的原因) ICMP基于IP协议。也就是说，一个ICMP包需要封装在IP包中，然后在互联网传送。ICMP是IP套装的必须部分，也就是说，任何一个支持IP协议的计算机，都要同时实现ICMP。 常见的ICMP包类型目的端不可达目的地无法到达(Destination Unreachable)属于错误信息。如果一个路由器接收到一个没办法进一步接力的IP包，它会向出发主机发送该类型的ICMP包。比如当IP包到达最后一个路由器，路由器发现目的地主机down机，就会向出发主机发送目的地无法到达(Destination Unreachable)类型的ICMP包。目的地无法到达还可能有其他的原因，比如不存在路由表项，比如不被接收的端口号等等。 重定向重新定向(redirect)属于错误信息。当一个路由器收到一个IP包，对照其routing table，发现自己不应该收到该IP包，它会向出发主机发送重新定向类型的ICMP，提醒出发主机修改自己的routing table（“我”不是你的目标，请更换目的地址） ICMP 的两个运用： Ping 和 Ttracertping可以检测网络的连通性，这一点运用很广。ping命令就是利用了该类型的ICMP包。当使用ping命令的时候，将向目标主机发送Echo-询问类型的ICMP包，而目标主机在接收到该ICMP包之后，会回复Echo-回答类型的ICMP包，并将询问ICMP包包含在数据部分。ping命令是我们进行网络排查的一个重要工具。如果一个IP地址可以通过ping命令收到回复，那么其他的网络协议通信方式也很有可能成功。 tracert 用于测试链路的跳数：IPv4中的Time to Live(TTL)会随着经过的路由器而递减，当这个区域值减为0时，就认为该IP包超时(Time Exceeded)。Time Exceeded就是TTL减为0时的路由器发给出发主机的ICMP包，通知它发生了超时错误。traceroute命令用来发现IP接力路径(route)上的各个路由器。它向目的地发送IP包，第一次的时候，将TTL设置为1，引发第一个路由器的Time Exceeded错误。这样，第一个路由器回复ICMP包，从而让出发主机知道途径的第一个路由器的信息。随后TTL被设置为2、3、4，…，直到到达目的主机。这样，沿途的每个路由器都会向出发主机发送ICMP包来汇报错误。traceroute将ICMP包的信息打印在屏幕上，就是路径的信息了。 集线器(Hub) vs. 交换器(Switch)二层设备集线器： 以太网使用集线器或者交换器将帧从发出地传送到目的地。一台集线器或交换器上有多个端口，每个端口都可以连接一台计算机(或其他设备)。 集线器像一个广播电台。一台电脑将帧发送到集线器，集线器会将帧转发到所有其他的端口。每台计算机检查自己的MAC地址是不是符合DST。如果不是，则保持沉默。集线器是比较早期的以太网设备。它有明显的缺陷： 任意两台电脑的通信在同一个以太网上是公开的。所有连接在同一个集线器上的设备都能收听到别人在传输什么，这样很不安全。可以通过对信息加密提高安全性。 不允许多路同时通信。如果两台电脑同时向集线器发信，集线器会向所有设备发出“冲突”信息，提醒发生冲突。可以在设备上增加冲突检测算法：一旦设备发现有冲突，则随机等待一段时间再重新发送（CSMA/CD） 交换机： 交换器克服集线器的缺陷。交换器记录有各个设备的MAC地址。当帧发送到交换器时，交换器会检查DST，然后将帧只发送到对应端口。交换器允许多路同时通信。由于交换器的优越性，交换器基本上取代了集线器。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识之操作系统]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[操作系统的诞生现代计算机系统是由一个或者多个处理器，主存，磁盘，打印机，键盘，鼠标显示器，网络接口以及各种其他输入输出设备组成的复杂系统，每位程序员不可能掌握所有系统实现细节，并且管理优化这些部件是一件挑战性极强的工作。所以，我们需要为计算机安装一层软件，也就是操作系统。操作系统的任务就是用户程序提供一个简单清晰的计算机模型，并管理以上所有设备。 操作系统充当软件和硬件之间的媒介，更具体来说，操作系统提供API来抽象硬件，叫“设备驱动程序” 操作系统使得程序员可以用标准化机制 和 输入输出硬件（I/O）交互。还有就是说，程序员只负责编写程序，如何调用、处理程序、输出内容的任务就交给了操作系统 总结：操作系统是一个用来协调、管理和控制计算机硬件和软件资源的系统程序，其中包括：文件系统、内存管理、设备管理和进程管理。它位于硬件和应用程序之间。需要注意的是，操作系统本质上也是一个软件，它由硬件上的程序进行调用运行。 CPU的资源利用我们知道，CPU的运算是很快的，而 I/O、打印机之类的设备运行很慢，这导致了一个后果，CPU闲置（程序阻塞在I/O上，这很浪费资源） 我们想实现的是：当cpu遇到阻塞时，它会把程序休眠，切换到下一个程序，等到阻塞的任务完成了，再回过头来进行运行处理。 所以我们需要有一种方式来最大限度的利用它，这样使得多个程序可以同时运行，在单个CPU上共享时间——“多任务处理” 内存地址的分配：每个程序都会占用一些内存，当切换到另一个程序时，我们不能丢失数据，需要记录当前状态，以方便回过头来处理程序时不会丢失数据。 解决办法就是： 给每个程序分配专属内存块 把程序A分配到内存地址0~999 把程序B分配到内存地址1000~1999 分配内存地址面临的一个问题： 如果一个程序请求更多内存，操作系统会决定是否同意，以及决定分配具体哪些内存块。这样虽然灵活性很好，但是会遇到个问题，由于更多的内存块是后续申请的，同个程序的内存块可能分配到非连续的情况。 虚拟内存其实真正的程序可能会分配到内存中数十个地方，这对于程序员来说很难跟踪，为了隐藏这种复杂性，操作系统会把内存进行“虚拟化”—— 称为虚拟内存，程序可以假定内存总是从地址0开始，而实际物理地址，被操作系统隐藏和抽象了。 如上图所示，此时程序B的虚拟内存地址为0999，而实际的物理地址，却是10001999。程序员不用关心实际的内存地址，操作系统会自动处理虚拟内存和物理内存之间的映射。 这种功能对于程序A的情况更为有用： 程序A的物理内存是不连续的，而从程序A的角度上看，它是连续的（虚拟内存） 这种机制使程序的内存大小可以灵活增减，叫“动态内存分配”，它的简化，为操作系统同时运行多个程序提供了更大的灵活性 内存保护给程序分配专用的内存范围，还有另外一个好处： “程序之间的隔离” 程序与程序直接互相独立，互不影响。内存保护是操作系统对计算机上的内存进行访问权限管理的一个机制。内存保护的主要目的是限制某个进程访问的只是操作系统配置给它的地址空间。这个机制可以防止某个行程，因为某些程序错误或问题，而有意或无意地影响到其他进程或是操作系统本身的运行状态和数据。 这也是程序与程序之间不能直接通信的原因。 UNIXUNIX操作系统把操作系统的各个任务总的划为两个部分： 如内存管理，多任务和输入/输出处理，这叫内核（Kernel）。操作系统的内核是一个管理和控制程序，负责管理计算机的所有物理资源，其中包括：文件系统、内存管理、设备管理和进程管理。 第二部分是一堆有用的工具（比如程序和运行库） UNIX的另一个特点就是被归类为分时操作系统，所谓分时操作系统，值得就是计算机将CPU的处理以时间段进行划分，优先级高的优先执行，若执行程序优先级降低，则推出执行，转而执行优先级更高的程序和事务。分时操作系统的执行效率是非常高的。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day1-TCP/IP协议]]></title>
    <url>%2F2019%2F07%2F27%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay1-TCP-IP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[引言：感谢！计算机网络可以说是一门计算机的必修课吧，私以为，计算机网络和计算机组成原理是一名计算机专业学生最应该先学习的知识~ 于是我决定展开对于这门学科一些知识总结与感悟。 在这里要感谢一名老师，在刚踏上大学这段旅途的时候，余老师像一盏明灯一样，使得我面临计算机这个钢铁巨兽面前不会感到手足无措，带我踏进了计算机这个门槛、揭开了计算机的奥妙。 实例引入：计算机之所以能够通信，是因为其遵守了相约定好的协议(protocol)。就像人与人之间对话所使用的语言一样，语言不同，自然也就不能实现“通信”。于是，协议这种约定俗成的东西就诞生了。 TCP/IP 模型： 下面的 OSI 七层模型是最早的协议，但由于某些原因，他更多的是一种标准的形式存在，没有实际推广（有兴趣的同学可以自行百度~） TCP/IP 四层模型的网络接口层又被人拆分为 数据链路层和 物理层。（又叫TCP 五层模型） 下面我们针对TCP/IP的各层进行简单介绍 物理层所谓的物理层，是指光纤、电缆或者电磁波等真实存在的物理媒介。这些媒介可以传送物理信号，比如电信号、电压。对于数字应用来说，我们只需要两种物理信号来分别表示0和1，比如用高电压表示1，低电压表示0，就构成了简单的物理层协议。针对某种媒介，电脑可以有相应的接口，用来接收物理信号，并解读成为0/1序列。 数据链路层息以帧(frame)为单位传输。信息是一段有序的0/1序列，而帧，是这个序列中符合特定格式的一小段。连接层协议的功能就是识别0/1序列中所包含的帧。并且负责 物理层和网络层之间的通信。 帧中，有源地址和目的地址，还有能够探测错误的校验序列(FCS)。当然，帧中最重要的最重要是所要传输的数据 通过连接层协议，我们可以建立局域的以太网或者WiFi局域网，并让同一局域网中的两台计算机通信。 网络层数据链路层使得同一个局域网的人可以互相通信，那么不处于同一个局域网如何通信呢？我们需要一个“中间人”。这个“中间人”必须有以下功能: 能从物理层上在两个网络的接收和发送0/1序列， 能同时理解两种网络的帧格式。 路由网关： 路由器(router)就是为此而产生的“中间人”，也称为“网关”。一个路由器有多个网卡(NIC，Network Interface Controller)。每个网卡可以接入到一个网络，并理解相应的连接层协议。在帧经过路由到达另一个网络的时候，路由会读取帧的信息，并改写以发送到另一个网络。 两个不同局域网的计算机是这么通信的： 主机发出数据 -&gt; 发现与对端主机处在不同个局域网 -&gt; 数据交给网关进行转发 -&gt; 送达目的端计算机 而实际情况比这更加复杂，上述内容省略了ARP、ICMP、路由等步骤 在数据链路层，一个帧中只能记录目的地址和源地址两个地址。而完成上面的过程需要经过四个地址(计算机1，网关地址，以太网接口，计算机2)，因此，这也是 IP协议的主要功能——寻址。 传输层上面的三层协议让不同的计算机之间可以通信。但计算机中可能运行了许多个进程，每个进程都可能有自己的通信需求。我们使用网络接口层、网络层实现了主机与主机之间的通信，可是， 软件与软件之间的通信该如何解决呢？ 没错，就是传输层要做的事情了。 端口号： 计算机有多个进程，那么我们如何辨别是某个进程发出的信息请求呢？ 比如我们打开firefox浏览网页，与此同时，又用QQ来接收邮件。我们需要有一个标志来识别，也就是端口号啦。 我们知道IP层的ip地址可以唯一标识主机，而端口号可以唯一标示主机的一个进程。 TCP 和 UDP Internet 在传输层有两种主要的协议：一种是面向连接的协议 TCP ，一种是无连接的协议 UDP。利用端口号，一台主机上多个进程可以同时使用 TCP/UDP 提供的传输服务，并且这种通信是端到端的，它的数据由 IP 传递，但与 IP 数据报的传递路径无关。这俩兄弟的具体情况，我们以后会涉及到。 应用层应用层协议定义了运行在不同端系统上的应用程序进程如何相互传递报文。向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。 常用服务 协议 端口号 HTTP TCP 80 FTP TCP 20、21 Telnet TCP 23 ​ 以上就是这篇文章的全部内容，我们只是介绍了个大概的框架，具体内容会在以后进行详细介绍。 还有一点要说的是，计算机网络是基础，大多数内容都是理论。这些知识点都是需要牢牢记住的~ 因为听说面试大多数都会问到 TCP 如何建立连接、HTTP 的内部实现 等等一些理论性知识点。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day7-Mysql的那些事]]></title>
    <url>%2F2019%2F07%2F26%2F%E7%88%AC%E8%99%ABDay7-Mysql%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[前言：爬虫爬取到的数据最终是要保存下来滴~ 别以为爬下来就完事了，具体流程还有数据去重，数据分析，最后数据可视化等等操作呢… 这一章节，我们聊聊 “数据存储” 在 Python2 中，连接 MySQL 的库大多是使用 MySQLDB，但是此库官方并不支持 Python3，所以在这里推荐使用的库是 PyMySQL。 环境准备： 安装好PyMysql库 本地mysql环境的部署 PyMysql使用介绍：获取连接对象我们要连接到 mysql 这个库，首先应该获取一个connect 对象（输入本地部署的Mysql参数） 1234import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)# 通过 PyMySQL 的 connect() 方法声明了一个 MySQL 连接对象 获取操作游标紧接着，需要实例化一个操作游标对象。 12345import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)cursor=db.cursor()# 注意：cursor 对象是基于db 这个connection对象实例化出来的 cursor 则为mysql 里面的 操作游标，这个概念和光标有些类似。语句的执行操作，都是依靠操作游标的。 我们拿到操作游标后，就很容易了。因为python执行的 sql 语句跟 mysql 里面是一样的…但还是有一丢丢不同的，让我们接着往下看。 插入数据 注意我们这里使用了异常处理，因为在PyMysql 里面是采用事务这个概念进行操作的 插入数据需要使用 commit 进行数据提交；当捕捉到错误时，db对象进行 rollback 事件回滚（只针对插入、更新、删除这些对数据库进行更改的操作，事件回滚就是：要么执行成功，要么不执行） 还有一点要注意的是：里面的参数，不管什么类型，统一使用%s作为占位符 动态插入数据：有时候我们面临的实际情况，你不知道有多少字段插入（上面的情况是我们”写死“的），可以说，插入的字段是不确定因素的，我们应该学会根据实际情况去构造 sql 语句。 在很多情况下，我们要达到的效果是插入方法无需改动，做成一个通用方法，只需要传入一个动态变化的字典给就好了。比如我们构造这样一个字典： 上面这条执行语句， 就替代之前那条 很 low 的写法，这么一来，无论字典有多少个键值对，我们都能进行插入。 数据查询：是的，对于数据库的增删改查，什么最重要？ （大声告诉我！）—— 没错！ 就是查询！！ 旁白君吐槽：“自嗨型作者…” 于是，旁白君大手一挥，说：“这个还不简单，我最拿手了。”于是抢走了作者的键盘，刷刷的写下这么一串代码 12345678910111213import pymysqldb = pymysql.connect(host='localhost',port=3306,user='root',password='root',db='spiders')cursor = db.cursor()table = 'students'sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: results = cursor.execute(sql) print(results)except Exception as e: print(e) 但是，结果却是这样的… what？ 这是什么鬼东西，我的数据呢？？ 作者邪魅一笑，眼中三分不屑三分冷漠四分讥笑，慢吞吞的拿走旁白君的键盘，自信的啪啪啪添加上了这么几条代码: 12345678910sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: cursor.execute(sql) results = cursor.fetchall() # fetchall（）方法，获取查询数据的返回值 for row in results: print(row)except Exception as e: print(e) 看到输出的结果，作者在旁白君羡慕的眼神中满意的点了点头 旁白君对着图仔细看了看说：“刚好四条！原来我刚才输出的4是执行成功的条目数！” “没错，悟性还挺高！” 作者留下潇洒离去的背影，并甩给旁白君一本秘籍——“数据查询的方法” 数据查询的方法： 使用select语句查询后不会直接返回查询的具体数据，而是返回执行成功的条目数，我们需要使用fetchone或者fetchall方法可以将数据读取出来。 fetchone() 方法，这个方法可以获取结果的第一条数据，返回结果是元组形式，元组的元素顺序跟字段一一对应，也就是第一个元素就是第一个字段 id，第二个元素就是第二个字段 name，以此类推。 fetchall() 方法，它可以得到结果的所有数据，然后将其结果和类型打印出来，它是二重元组，每个元素都是一条记录。我们将其遍历输出，将其逐个输出出来。 我们还可以用 while 循环加 fetchone() 的方法来获取所有数据，而不是用 fetchall() 全部一起获取出来，fetchall() 会将结果以元组形式全部返回，如果数据量很大，那么占用的开销会非常高。所以推荐使用如下的方法来逐条取数据 总结这一篇主要是理解如何连接数据库、事务的概念（默认开启事务），以及使用对数据库对字典形式的数据进行操作。然后 使用sql语句进行增删改查（主要是动态数据、插入更新数据部分） 在这一期皮了一下，希望大家喜欢，啊哈哈。 各位，我们下期再见！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day6-Beautiful介绍]]></title>
    <url>%2F2019%2F07%2F24%2F%E7%88%AC%E8%99%ABDay6-BeautifulSoup%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[之前的urllib实战中我们谈到了BeautifulSoup这个解析库，那么…是不是应该来一篇详细介绍呢？ 今天，他来了！！ BeautifulSoup的基本使用：拿到网页数据：一般来说，我们使用解析库的前提，已经是通过请求库拿到了网页的数据，假设现在由下图的 html 变量存储的数据正是我们请求的文本内容，下面我们将使用BeautifulSoup进行加工解析。 获取Soup对象：使用BeautifulSoup方法，第一个参数传入 HTML字符串文本，第二个参数传入的是解析器的类型，在这里我们使用lxml。不指定会使用默认的解析器，但是会发出警告 123from bs4 import Beautifulsoupsoup = BeautifulSoup(html,"lxml")print(soup) 运行后，发现内容如下（发现不完整的HTML字符串被补齐了）： 属性选择调用 soup 的各个方法和属性（选择器）对这串HTML代码解析。这个soup对象支持 “.” 获取节点操作，也可以通过”.”表达嵌套关系。 我们直接拿到的soup打印不美观，使用prettify（）方法可以打印出树形结构的 html 文本 这里我们打印 title 的文本内容，以及 head 节点信息 上面就是BeautifulSoup的使用流程，有时候，我们需要的内容存放于某些节点中，这些节点有它的特征，我们可以通过选择器获取这些节点，最后拿到我们需要的结果。 1对于某些节点，我们使用嵌套选择来选中它非常麻烦，那么，就有了节点选择器，根据其CSS编写的 类、id 这样的属性进行选择。Beautifulsoup 很快就能找出匹配的标签。 节点选择器：选择节点：直接通过调用节点的名称就可以选择节点元素了，这种选择方式速度非常快，如果单个节点结构话层次非常清晰，可以选用这种方式来解析。 注意，我们的选择器是基于soup这个对象的 输出了soup.head的类型，是 bs4.element.Tag 类型.这是 BeautifulSoup 中的一个重要的数据结构，经过选择器选择之后，选择结果都是这种 Tag 类型，它具有着一些属性，比如 string 属性（输出文本信息），可以进行获取属性、嵌套等操作。 不过这种选择方式只会选择到第一个匹配的节点，其他的后面的节点都会忽略——比如我们打印的p节点，只呈现一个信息（html字符串中有三个p节点信息） 获取属性值：每个节点可能有多个属性值，比如id，class 等等，我们选择到这个节点元素之后，可以调用 attrs 获取所有属性。 可以看到 attrs 的返回结果是字典形式，把选择的节点的所有属性和属性值组合成一个字典。因此，可以通过键值形式进行取值传入节点的属性，获取该节点的属性值 contents 属性： 选取到了一个节点元素之后，如果想要获取它的直接子节点的信息 （不包含子孙节点），可以调用contents 属性（和直接选择不同的是，contents返回的列表包含了所有子节点和所有文本信息） 关联选择：我们在做选择的时候有时候不能做到一步就可以选择到想要的节点元素，有时候在选择的时候需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等等。这里列出了获取节点的各个方法： Children 查询某个节点的直接子节点（返回一个迭代器） Descendants 查询某个节点的所有子孙节点（返回生成器） parent 查询某个节点的元素的父节点 Parents 查询某个节点的元素的祖父节点（返回生成器） soup.a.next_sibling 查询某个节点的下一个节点 previous_sibling 查询某个节点的上一个节点 next_siblings 和 previous_siblings 分别返回所有前面和后面的兄弟节点的生成器。 需要注意的是：上面这些节点选择，但凡涉及到多个节点内容，就会返回一个生成器类型。 关于这些节点选择，我们要注意 html 树型结构的排列关系，例如，我们不能跨越父级节点，去拿到另一个子孙节点的内容。 调用 API前面我们所讲的选择方法都是通过属性来选择元素的，这种选择方法非常快，但是如果要进行比较复杂的选择的话则会比较繁琐，不够灵活。所以 BeautifulSoup 还为我们提供了一些查询的方法，这里介绍非常常用的 find_all()、find() 等方法，我们可以调用方法然后传入相应等参数就可以灵活地进行查询。 find_all( )查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，并使用一个列表进行返回。 12# 它的API 如下：find_all (name, attrs, recursicve, text, **kwargs) 我们接下来针对它的 API 下的属性进行介绍 ①name：根据标签名选择节点 方法选择器会返回一个Tag标签的类型，也就是说，其支持嵌套获取元素、层层迭代使用find_all等等操作。 当name=True时，匹配所有标签。 string 属性获取文本： 如果我们想拿到节点中的文本，该怎么做呢？ 使用列表切片方式取值，再调用.string属性，即可获取文本。 ② attrs / id ：根据属性值匹配 可以通过这些attrs、id 这两个关键字对我们需要的节点传入进行匹配 ③ text：用来匹配节点的文本，文本参数 text是用标签的文本内容去匹配，而不是用标签的属性。 传入的形式可以是文本字符串，也可以是正则表达式对象（compile方法生成的对象） 举例：text = “Hello，this is a link” 这样的形式进行匹配节点。 find( ) find() 方法返回的是单个元素，也就是第一个匹配的元素。类型依然是 Tag 类型。（当某个标签只有一个是使用find比较好，因为find返回一个Tag类型，利于我们后续操作。 而find_all返回一个列表，这是最大的区别了，有时候我们需要拿到Tag类型的节点，这样对我们的后续操作更为便利），跟find_all很相似，API都是相同. CSS选择器BeautifulSoup 还提供了另外一种选择器，那就是 CSS 选择器。如果学过前端，那么这个选择器简直是为你量身定做的。 我们只需要调用 select() 方法，传入相应的 CSS 选择器即可（个人比较喜欢这个选择器） CSS选择器是 BeautifulSoup 最大的特性，比较简便，且好理解 例如： id =“xx” 我们就可以写成 ‘#xx’，传给select 方法匹配。 获取文本经过节点选择，我们已经匹配到我们需要的节点了，接下来应该是获取文本内容了，通过Tag 类型的string 属性，我们可以拿到文本内容，还有一个方法那就是 get_text（），同样可以获取文本值。 只需基于 Tag类型，调用 get_text（） 方法即可。不过需要注意的是： .get_text（）会把你正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如你正在处理一个包含许多超链接、段落和标签的大段源代码，那么 .get_text() 会把这些超链接、段落和标签都清除掉，只剩下一串不带标签的文字。 所以，get_text（）方法一般留到节点匹配之后使用，不然会影响我们获取节点 获取属性 get 属性还是比较常用的，例如获取 a 标签的href 属性，我们就能得到链接了： get（”href“） 写在最后： 说一下节点选择的建议：页面布局总是不断变化的。一个标签这次是在表格中第一行的位置，没准儿哪天就在第二行或第三行了。如果想让你的爬虫更稳定，最好还是让标签的选择更加具体。如果有属性，就利用标签的属性。 解析库呢，就像一把锤子，使用的是我们，我们学会如何使用一把锤子的同时呢，也要关注什么时候去使用它，不要手上握着一把锤子，看任何问题都是钉子。使用适当的解析库去拿到我们需要的内容才是我们应该做的，使用什么样的锤子并不重要。 不经意间，爬虫专栏已经走到的Day6了，过阵子应该会更新 爬取网站的实战。还是那句话，学理论归理论，但只有实战才能使我们将其内化，成为我们真正的技能。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day5-requests介绍]]></title>
    <url>%2F2019%2F07%2F19%2F%E7%88%AC%E8%99%ABDay5-requests%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Requests的基本使用我们知道在 Urllib 库中有 urlopen() 的方法，实际上它是以 GET 方式请求了一个网页。那么在 Requests 中，相应的方法就是 get() 方法，是不是很方便（使用什么请求方法就用对应的方法名封装的API） GET 请求下面是使用requests库发出的简单get请求，是不是相对于urllib，步骤变简单了很多呢？嘿嘿~ 响应内容的类型是type，我们可以基于这个对象，使用text方法获取文本，也可以调整编码格式等等。 响应状态码封装在status_code这个属性中，200表示正常访问。 模拟其他请求接下来，在这里分别用post()、put()、delete() 等方法实现了 POST、PUT、DELETE 等请求。requests这些方法时是直接使用的（不像urllib那么隐晦） 附加信息我们知道，使用百度这样的搜素引擎搜索关键字时，会在url生成类似这样的url，比如我们现在搜索的关键词是python，会生成下面这条url——“http://www.baidu.com/s?wd=python” 那么，在requests中，如何模拟这样的关键词查询呢？ 回答：将信息数据存储到字典中，然后利用params这个参数进行传递。 我们可以看到，字典的键值被传递到url中。 注意字典里值为None 的键都不会被添加到 URL 的查询字符串里。 定制请求头 注意：定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。 如果被重定向到别的主机，授权 header 就会被删除。 代理授权 header 会被 URL 中提供的代理身份覆盖掉。 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。 更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 二进制流数据上面例子中，实际上它返回的是一个 HTML 文档，那么如果我们想抓去图片、音频、视频等文件的话应该怎么办呢？我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，所以就需要拿到他们的二进制码。 需要记住的是：用什么保存格式编码就要用对应的解析方式解码 前两行便是 r.text 的结果，最后一行是 r.content 的结果。 前者出现了乱码，由于图片是二进制数据，所以前者在打印时转化为 str 类型（.text属性会猜测 其编码格式，然后自动给你转码），因此这里就是把图片直接转化为字符串，理所当然会出现乱码 而.content会到HTML纯文档中查找Meta标签下的编码，然后进行指定编码。可以发现，打印的结果前面带有一个 b，代表这是 bytes 类型的数据。 得出结论： r.test返回的是字符串类型，如果返回结果是文本文件，那么用这种方式直接获取其内容即可。 如果返回结果是图片、音频、视频等文件，应该使用 r.content，Requests 会为我们自动解码成 bytes 类型，即获取字节流数据。此时，我们就可以通过写入文件（注意编码格式是wb），将其保存为照片格式。 我建议使用 .content ， 然后将二进制数据 编码为 utf-8 关于编码：编码可谓是蛰伏在黑暗中的精灵，如果你不搞懂他，它就会偶尔出来耍一下脾气… Requests 会自动解码来自服务器的内容 。 大多数 unicode 字符集都能被无缝地解码 。 请求发出后 ， Requests 会基于 HTTP 头部对响应的编码作出有根据的推测 。 当你访问 r.text 时 ， Requests 会使用其推测的文本编码 。 不过你也可以找出 Requests 使 用 了 什 么 编 码 ， 并 且 能 够 使 用 r.encoding来更改编码方式。如果你改变了编码格式，每当你访问r.text，Request都会使用r.encoding的新值。 不过每一次设置r.encoding都需要去浏览器中查看相应字符集，这很麻烦。我们可以这样写： 1r.encoding = r.apprent_encoding # apprent_encoding是在响应报文的头部获取字符集，这样设置比较灵活 post 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典（也可以是个元组）给 data 参数。你的数据字典在发出请求时会自动编码为表单形式 是不是觉得好简单？哈哈~ 这就是requests向我们提供的简单接口，然后我们直接拿来调用就行了！ Request的高级用法文件上传我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传。使用files关键字传递文件，就可以达到我们想要的效果。 content-type的值是multipart/from-data，表示是以表单数据提交的，证明请求方法为post 发送到网页，需要以字节流形式传输，所以我们打开文件时必须指定为“wb” 最后使用files关键字指定传入文件，文件需以字典形式传入（键名是什么不重要） 设置Cookies在前面我们使用了 Urllib 处理过 Cookies，写法比较复杂，而有了 Requests，获取和设置Cookies 就比较方便快捷了。而且requests有两种设置方法，我们一起来看看吧~ ① 在请求报文头中设置 ② 通过cookies参数进行传递 代理设置对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会提出登录验证、验证码，甚至直接把IP给封禁掉。（我就曾使用selenium对淘宝网进行爬取，结果没爬几页，发现被淘宝反爬虫给识别出来了，这就很尴尬了…） 那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在 Requests 中需要用到 proxies 这个参数。 超时设置在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能会收到一个响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，我们应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错，或者谁用异常处理来处理这类事情。 设置超时时间需要用到 timeout 参数（默认是None）。这个时间的计算是发出 Request 到服务器返回Response 的时间。 上面就是requests的简单介绍了，是不是发现功能都被封装称为各个API中，我们只需要进行调用就好了，有没有像我当初一样，被requests这个库的魅力所感染到了呢？]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识扫盲篇]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2%E7%AF%87%2F</url>
    <content type="text"><![CDATA[计算机组成基础谈谈汇编语言：机器语言——“计算机运行的基础” 机器语言是机器指令的集合，机器指令展开来讲就是一台机器可以正确执行的命令。电子计算机的机器指令是一列二进制数字。计算机将之转变为一列高低电平，以使计算机的电子器件受到驱动，进行运算。 汇编语言的诞生： 汇编语言是直接在硬件上工作的编程语言，用来替代机器语言麻烦的编程问题（难以辨别和记忆）。 汇编语言的主题是汇编指令，汇编指令和机器指令的差别在于指令的表示方法上，汇编指令是机器指令便于记忆的书写格式 编译器： 但是计算机能读懂的只有机器指令，那么如何让计算机执行程序员用汇编语言编写的程序呢？ 这时，一种能够将汇编语言转成机器指令的翻译程序——编译器 产生了。 汇编语言的组成： 汇编指令：有对应的机器码，决定了汇编语言的特性 伪指令：没有对应的机器码，由编译器执行，计算机并不执行 其他符号：如+、-、*、/ 等，由编译器识别，也没有对应的机器码 计算机组成的各个部分存储器：CPU是计算机的核心部件，它控制整个计算机的运作，要想让CPU工作，就必须向它提供指令和数据。指令和数据在存储器中存放，也就是平时说的内存。 离开了内存，CPU也无法工作。 磁盘是计算机永久性存储数据的空间所在，磁盘上的数据要想被**CPU**执行调用，必须先读到内存当中。 指令和数据：指令和数据是应用上的概念，在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。 CPU在工作时，把有的信息看做指令，有的信息看做数据。（有特殊意义的二进制信息视为指令） 存储单元：存储器又被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127。 微型机存储器的每一个存储单元可以存储一个字节，即8个bit（电子计算机的最小信息单位）。 也就是说，一个存储器有128个存储单元，它可以存储128个字节。 微机存储器的容量是以字节为最小单位来计算的。 CPU对存储器的读写：存储器被划分成多个存储单元，存储单元从零开始顺序编号，这些编号可以看作存储单元在存储器中的地址，就像每个房子的门牌号 CPU要从内容中读取数据，首先要指定存储单元的地址，也就是说它要先确定读取哪一个存储单元中的数据。 另外，在微机中，不当当只有存储器这一种器件，CPU在进行读写数据操作时，需要指明它要对哪一个器件进行操作、进行哪种操作、是从中读出数据，还是写进数据。 可见，CPU要想进行数据的读写，必须和外部器件（可以说成芯片）进行三类信息的交互： 存储单元的地址（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据） 那么CPU是通过什么介质将这些信息传到存储器芯片中呢？ 电子计算机能处理、传输的信息都是电信号，电信号当然要用导线传送。在计算机中，专门连接CPU和其他芯片的导线，称为总线。 从物理上来讲，总线是一根根导线的集合，根据传送信息的不同，又从逻辑上分为三类、即地址总线、控制总线和数据总线。 总线 地址总线： CPU是通过地址总线来指定存储器单元的。地址总线上能传送多少个不同的信息，CPU就可以对多少个存储单元进行寻址。 在电子计算机中，一根导线可以传送的稳定状态有两种，高电平或是低电平。用二进制表示就是1或者0。 有 n 根导线就能表示 2的n次方种信息。假设一个CPU有N跟地址线，则可以说这个CPU的地址总线的宽度为N，这样的CPU最多可以寻找2的N次方个内存单元。 数据总线： CPU与内存或其他器件之间的数据交互是通过数据总线进行传递的。 数据总线的宽度决定了CPU与外界的数据传输速度。8跟数据总线一次可传送一个8位二进制数据（即一个字节）。 控制总线： CPU对外部器件的控制是通过控制总线来进行的。 控制总线是一些不同控制线的集合，有多少根控制总线，就意味着CPU提供了对外部器件的多少种控制，所以，控制总线的宽度决定了CPU对外部器件的控制能力。 内存地址空间：举例来说，一个CPU的地址宽度为10，那么可寻址1024个内存单元，这1024个可寻址的内存单元就构成了这个CPU的内存地址空间。CPU在操纵和控制各类存储器时，都把他们当作内存来对待，把它们总的看做一个由若干存储单元组成的逻辑存储器，这个逻辑存储器就是我们所说的内存地址空间。 主板：每一台PC机中，都会有一个主板，主板上的器件通过总线相连。 这些器件有：CPU、内存、外围芯片组、扩展插槽等。扩展插槽一般插有RAM内存条和各类接口卡. 接口卡：计算机系统中，所有可用程序控制其工作的设备，都归CPU控制。 CPU对于外部设备如显示器、音箱等，是通过控制这些设备直接相连的接口卡，从而实现间接的控制这些外部设备的。简单来说，CPU通过总线向接口卡发送命令，接口卡根据CPU的命令控制外设进行工作。 各类存储器芯片：一台PC机中，装有多个存储器芯片。不同的器件，从读写属性上分为两大类： 随机存储器（RAM）和只读存储器（ROM） 随机存储器可读可写，但必须带电存储，关机后存储的内容丢失； 只读存储器只能读取不能写入，关机后内容也不会消失。 这些存储器从功能和连接上又可分为以下几类： 随机存储器：用于存放供CPU使用的绝大部分程序和数据，主随机存储器一般由两个位置上的RAM组成，装在主板上的RAM和插在插槽上的RAM 装有BIOS（基本输入输出系统）的ROM：BIOS是由主板和给某些接口卡（如：显卡、网卡等）厂商提供的软件系统。 接口卡上的RAM：某些接口卡需要大批量输入输出数据进行暂时存储，因此会在其上装有RAM。最为经典的是显示卡上的RAM，一般称为显存。我们将需要显示的内容写入显存，就会出现在显示器上。 **** 以上就是总结的部分计算机基础知识。我们下篇见，see you ~]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程新手的必修课]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BC%96%E7%A8%8B%E6%96%B0%E6%89%8B%E7%9A%84%E5%BF%85%E4%BF%AE%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[论一名计算机专业学生的修养从事计算机行业，私以为，是最为公平且投入风险小的一件事。我们听过不少这样的话——“只要你们用心学，不要放弃，就一定能学会计算机”。但我认为这句话是片面的，任何没有回报的努力，都是耍流氓。 刚开学，大家学习着学校安排的课程，抱着几本所谓计算机专业的教材一顿猛啃，你也许看懂了里面讲什么，但不明所以，不知道这些有什么用；有的学生学的一头雾水开始怀疑自己准备转专业放弃计算机；有的学生学懂写程序一直报错开始怀疑自己。 以上这几种情况，若没有领悟到学计算机的真正奥妙，最终都要凉凉。 为什么会这样呢？ 我们有没有反省过，究竟是环境问题，还是自驱动力的不足，亦是学习方法一开始就是错的？ 私以为，学计算机的成功离不开下面三点。 努力：计算机虽然是目前最火的专业，但若人人都想分一杯羹，不付出实际行动，一切都是空谈。 视野：视野决定高度，有句话说，站在风口上，猪都能飞来。 职业规划： 计算机虽然被统称为计算机专业，但其包含的方向数不胜数，方向选择是很重要的。拥有一份明确的职业规划，绝对让你披荆斩棘，一路高歌。 1接下来我们针对以上三点，进行描述 投入努力：在计算机专业，有一点是肯定的，那就是 “投入时间占比 = 回报率占比”。课堂所学，是计算机最基础的东西，就算你课堂上听懂了，全弄懂了，那些知识也无法支撑你拿到满意的工作。（这是真的！） 你之所以觉得听不懂，你要弄清楚是哪方面原因，当夜深人静的时候，好好想想，是不是真的对计算机没有兴趣。当遇到的BUG终于解决的时候，心情是激动的，还是仍然一副恼怒的样子。当DEBUG成功时，应该感到开心，这才是兴趣驱动学习。 这是一门全靠自学为主的学科，投入时间最为重要。 视野的打开网上资源一抓一大把，学会如何搜索，等于坐拥了这些资源。对于计算机专业的人来说，更是一笔最为宝贵的财富，编程新手的必修课，就是会用搜索引擎。 走进世界编程的大社区，例如Github、stackoverflow。哪怕什么都不懂，你也能保证所在的社区，是世界程序员的优秀社区，里面接触的东西，更是当下流行的。相信我，你的视野就会在此为起点，快速打开并不断增长，进入一个良性循环。 有多少学生，连stackoverflow、github是什么都不知道，如果我是老师，我教编程的第一节课绝对是教同学们如何搜索、如何利用网上资源、还有社区。 视野是很重要的一点，视野打不开，一身本领就会被扼杀在摇篮中。 职业规划：现互联网处于蓬勃发展的阶段，更是等着我们这一代人去带动他，我推测，互联网的红火还能至少维持二十年。且现在的红利虽不如几年前那么丰富，但现在也是不容小觑的。我们经常听到过，穷人家的孩子学计算机，富人家的孩子学金融就是这么来的。如果已经就读计算机专业，那么恭喜你，已经处于风口上，接下来就看你能不能把握住了。 这里引用一张知乎上k点赞的图片，里面所讲述的，描述了计算机行业中多个方向的选择，我们可以从中挑选出适合我们自己的。 四大理论最为重要，分别是： 数据结构与算法、操作系统、计算机组成原理、计算机网络。就算大学生涯什么都不会，这四大基础一定要学好。 写在最后到了大学，请不要再有“学生思维”，碰到问题时不要立马请教他人，解决问题的过程就是你进步的一种。而且这种解决问题的能力，更是会伴随我们整个职业发展。可以说，这种技能胜过于任何一个知识点，也是自学过程中必须掌握的。 上面这些，是我的这一年来的心得体会，希望能帮到有需要的人。]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day4-urllib实战]]></title>
    <url>%2F2019%2F07%2F15%2F%E7%88%AC%E8%99%ABDay4-urllib%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[urllib爬虫实战之爬取笔趣阁小说前言上一节我们介绍了urllib这个基本库，它的内容还是蛮多的，要想熟练掌握爬虫，必须配合实战进行巩固。一开始小伙伴们可能会觉得无从下手，特别是解析响应内容这一步。这一次，作者将会从小白的角度，讲述如何对网页进行爬取、解析。 环境运行平台： WindowsPython版本： Python 3.xIDE： Pycharm BeautifulSoup简介 BeautifulSoup 就是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据。BeautifulSoup提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据。 解析器 BeautifulSoup 在解析的时候实际上是依赖于解析器的，它除了支持 Python 标准库中的 HTML 解析器，还支持一些第三方的解析器比如 LXML 。 Beautiful方法有两个参数，第一个参数传入待解析的文本，第二个参数指定解析器，如果我们想要使用 LXML 这个解析器，在初始化 BeautifulSoup 的时候指定 lxml 即可，如下： 好了，简单介绍了BeautifulSoup，我们快点进入今天的实战环节吧~ 实战 背景介绍：笔趣看是一个盗版小说网站，这里有很多起点中文网的小说，该网站小说的更新速度稍滞后于起点中文网正版小说的更新速度。并且该网站只支持在线浏览，不支持小说打包下载。因此，本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说。 PS：本实例仅为交流学习，支持耳根大大，请上起点中文网订阅。 URL: ‘https://www.biqukan.com/1_1094/5403177.html&#39; 预备知识： 本文提及的代码并不复杂，如果对BeautifulSoup想要更加了解，可以去官方文档进行翻阅 URL：‘http://beautifulsoup.readthedocs.io/zh_CN/latest/‘ BeautifulSoup环境搭建： pip install bs4 导入使用 – from bs4 import BeautifulSoup 获取内容① 打开url链接，按F12或者右键- 检查 进入开发者工具 ② 在开发者工具中，捕获我们要找到的请求条目信息 将正文中部分内容进行复制 在开发者工具中，使用 ctrl + f 进入全局搜索框，将复制内容进行粘贴 然后会在下方得到条目信息，点击，页面会跳转到加载正文的请求响应条目中。 分析发现，正文部分存储在 id 为 content 和 class 为 showtxt 的 div 中。这就是我们解析网页得到的重要信息啦 ③ 构造url请求 有了上面的信息还不够，现在的网站都有了反爬能力，我们需要模拟一条正常从浏览器中发出的url请求链接。 例如： User-Agent（浏览器标识）、Cookies（标识客户端身份的“钥匙”）、referer等等字段 那么，这些包含在请求头的字段信息，我们应该如何构造这些字段信息呢? 还是开发者工具，点击Headers，就可以看到Request-Response条目明细。 ④ 发出请求： 有了字段的详细内容，我们就可以编写出请求网页的代码… 12345678import urllib.requestdef get_page(url): headers = &#123;'User-Agent':'Mozilla/5.0','Cookie':'UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333'&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() print(html) return html ⑤ 获得相应内容：运行，得到内容如下: 发现是一个二进制流的数据，因此得知，我们在解析时，需要编码 解析响应数据接下来，我们就可以使用BeautifulSoup进行解析 12345678910def parser_page(html): soup = BeautifulSoup(html,'lxml',from_encoding='utf-8') # 传入编码方式，为utf-8 text = soup.find('div', class_='showtxt') # 使用BeautifulSoup的find方法，匹配符合的节点，该节点为我们上文得知的属性 cont = text.get_text() # 输出文本内容 cont = str(cont).split() # 将空白符进行分割，这里的作用是消除 \xa0、\u3000空白符 print(cont) 运行….代码结果如图： 拿到小说数据！ 到这里，不少同学露出这样的微笑，哈哈哈 永久存储数据：我们这里使用最简单的方式，存储写入本地文件 打开result.txt文件，内容如下： 写在最后本次我们实战项目就告一段落啦，最后再为大家做一次梳理总结。 观察网页信息，获取相应内容 构造get请求，拿到响应内容 用解析库对相应内容进行解析 写入文件 全文代码如下： 12345678910111213141516171819202122232425262728import urllib.requestfrom bs4 import BeautifulSoupurl = 'https://www.biqukan.com/1_1094/5403177.html'def get_page(url): headers = &#123;'User-Agent':'Mozilla/5.0','Cookie':'UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333'&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() return htmldef parser_page(html): soup = BeautifulSoup(html,'lxml',from_encoding='utf-8') text = soup.find('div', class_='showtxt') cont = text.get_text() cont = str(cont).split() return contif __name__ == '__main__': html = get_page(url) results = parser_page(html) with open('result.txt','w',encoding='utf-8') as f : for result in results: f.write(result) f.write('\n') print("爬取完毕") 耗费一上午时间，终于把这篇文章给赶出来了~ 爬虫Day系列会持续更新，让我们共同期待吧！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day3-urllib介绍]]></title>
    <url>%2F2019%2F07%2F10%2F%E7%88%AC%E8%99%ABDay3-urllib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Urllib介绍在 Python2 版本中，有 Urllib 和 Urlib2 两个库可以用来实现Request的发送。而在 Python3 中，已经不存在Urllib2 这个库了，统一为 Urllib。 urllib分为四个模块： 第一个模块是request，它是最基本的HTTP请求模块，我们可以用它模拟发送请求，就像在浏览器手动输入网址然后敲击回车一样，只需要给库方法传入URL，以及额外的参数，就可以模拟实现整个过程了。 第二个模块是error，即异常处理，如果出现请求错误，我们可以捕获这个异常，然后进行异常的处理，保证程序的稳定性 第三个模块是paser，它是一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等操作 第四个模块是robotparser，主要是用来识别网站的robots.txt的（反爬虫的文本说明） 下面我就对以上模块进行重点讲解~ request模块1.urlopen方法： 我们从上面知道，request是HTTP请求模块，那么具体的请求方法是什么呢？没错，正是urlopen！ 12urlopen的API：urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 我们通过urllib.request.urlopen（“url链接”）就可以请求到网页，如下图所示 这个请求的返回结果response，就是服务器响应内容。紧接着，我们可以调用这个对象包含的方法对返回的内容进行处理。 response.read（）读取响应内容，它是HTML纯文本，它以字节流的形式返回的，于是要解码成utf-8，得到字符串。 respon.status 返回网页的状态码，如果网页的状态码是200，则表示正常。 利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取。 2.data参数 data 参数是可选的，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。我们就可以使传递data参数，完成模拟登陆等类似功能… 具体如何模拟from表单，后文的urllib.parser会讲解 3. timeout参数 timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。 我们可以通过设置timeout值，从而缩短超时时间。或者是让程序不会一直处于阻塞状态（我们知道，客户端和服务器的交互是一个同步过程，我们可以利用timeout参数，以及异常处理来完成非阻塞的效果，以及让程序不会异常终止，让程序更加稳定） 4.构建Request对象 我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。比如可以构建请求链接的浏览器标识等等 下面是正常的urlopen请求和使用Request构建URL的代码举例： 可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，这样的好处是： 一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活（将参数全部加到Ruquest对象中作为一个整体发送） Request配置举例 之前我们说到过，可以构建url链接，比如关键字查询、模拟登陆。还可以模拟浏览器标识，让爬虫更像是一个人为发出的请求信息。具体内容如下： headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。 data就是我们发出的数据了，他会以表单形式向服务器发出 method是方法的意思，我们可以指定method来声明这是一个什么类型的请求方法。 Error模块前文我们了解了 Request 的发送过程，但是在网络情况不好、被反爬的情况下，出现了异常怎么办呢？这时如果我们不处理这些异常，程序很可能报错而终止运行，所以异常处理还是十分有必要的。 Urllib 的 error 模块定义了由request 模块产生的异常。如果出现了问题，request 模块便会抛出error 模块中定义的异常 1.URLError URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。它具有一个属性 reason，即返回错误的原因。 2.HTTPError 它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。 它有三个属性。 code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。 reason，同父类URLError一样，返回错误的原因。(有时候 reason 属性返回的不一定是字符串，也可能是一个对象—可以用 isinstance() 方法来判断它的类型，做出更详细的异常判断。) headers，返回 Request Headers。 因为 URLError是HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，这样捕获的代码更加精准，且有利于程序猿处理。 请求到响应信息不足为奇，也不是什么难事，最主要的，是让程序可以持久的”跑起来”，所以，异常处理是爬虫工程师的必修课，需好好掌握。 parse模块这是一个工具模块，用的地方挺多的，一些常用操作都封装在里面，如下面介绍的几个函数方法。 1.urlparse（） urlparse() 方法可以实现 URL 的识别和分段,返回六个部分的内容，字段分别是 scheme、netloc、path、params、query、fragment。 2.urlencode（） 常用的 urlencode() 方法，它在构造 GET 请求参数的时候非常有用——字典转序列化 我们知道，使用GET发送请求信息，它的参数是 写在URL中的，那么字典一样的数据（爬取数据大多数为字典）如何传递到URL中呢—— 正是上面讲的序列化了。 3.parse_qs（） 有了序列化必然就有反序列化，如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典 4.quote（） quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码 python3 默认编码格式是 Unicode，URL默认不接收这种编码格式，因此我们发送中文字符时，就应该使用相应编码。 5.unquote（） quote() 方法的逆操作，它可以进行 URL 解码（默认是以utf-8编码） robotparser模块在介绍rebotparser模块之前呢，我们需要对robots协议进行了解 Robots 协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。 robotparse：用来解析rebots文件的类。 robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。 123接口API：import urllib.robotparserurllib.robotparser.RobotFileParser(url='') 使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。这样我们就拿到了一个 robot 对象，再调用can_fetch() 就可以返回是否可用爬取网页的布尔值了。 好了，对urllib库的介绍就到这儿了，下次作者会对第三方请求库requests讲解，这是一个当前的流行库，让我们一起期待吧~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day2_爬虫基础]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%88%AC%E8%99%ABDay2-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[爬虫基础：简单来说，爬虫就是请求网页并提取数据和保存信息的自动化程序。 人们常常把爬虫比做蜘蛛爬网，把网的节点比做一个个网页，爬虫爬到这就相当于访问了该页面获取了其信息，节点间的连线可以比做网页与网页之间的链接关系，这样蜘蛛通过一个节点后可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，这样网站的数据就可以被抓取下来了。 简单流程介绍： 下面我们说说爬虫的具体流程，一般分为四步： 发起请求， 通过请求库对目标站点发起请求，我们可以模拟携带浏览器信息、cookie值、关键字查询等信息，从而访问服务器端，然后等待服务器的响应 解析内容： 服务器响应的内容有可能是HTML纯文本，也有可能是json格式的数据、或者是二进制（如图片视频），我们可以利用相关的解析库进行解析、保存或进一步处理。 处理信息：一般来说，我们使用解析库解析出来的内容，是冗杂晦涩的，我们通过用format或者切片等技术，使得内容清晰明了，这时，才算得上的有用的信息。 接下来，我们可以将信息存储到本地、数据库或者远程服务器等等。 爬虫优化：爬虫优化的方面有很多，例如，控制爬虫速率、使用高匿代理服务器、使用分布式爬取、处理异常… 接下来作者针对爬虫最重要的部分做出详细介绍： 获取网页爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。 在前面我们讲到了 Request 和 Response 的概念，我们向网站的服务器发送一个 Request，返回的 Response 的 Body 便是网页源代码。 所以最关键的部分就是构造一个 Request 并发送给服务器，然后接收到 Response 并将其解析出来。在python3 中，相关的库有：urllib（python自带的库）、requests（优雅的第三方库） 提取信息我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。 另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。 能抓取怎样的数据： 常规网页， 也就是html源代码。有些网页返回的不是不是 HTML 代码，而是返回一个 Json 字符串（使用Ajax渲染的网页），API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。 二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。 我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。 JavaScript 渲染页面：有时候我们在用 Urllib 或 Requests 抓取网页时，得到的源代码实际和浏览器中看到的是不一样的。 这个问题是一个非常常见的问题，现在网页越来越多地采用 Ajax、前端模块化工具来构建网页，整个网页可能都是由 JavaScript 渲染出来的，意思就是说原始的 HTML 代码就是一个空壳。 因此在用 Urllib 或 Requests 等库来请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中看到的内容了。 这也解释了为什么有时我们得到的源代码和浏览器中看到的是不一样的。 所以使用基本 HTTP 请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台的Ajax接口，通过模拟Ajax请求得到渲染的页面，或者也可以使用Selenium这样的库来实现模拟JavaScript。 本期对爬虫基础原理的讲述就到这里了~ 下期预告：“urllib请求库的使用”]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day1-网页基础]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%88%AC%E8%99%ABDay1-%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[网页基础URLURL：统一资源定位符，通俗的讲，它是一个链接，它能使我们从互联网上找到我们访问的资源。 例如： http://www.4399.com/flash/58508.htm 这样的一个网站 它包含三个部分： 访问协议 http 访问路径 “www.4399.com” 资源名称 “flash/58508” 网页交互过程：我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。实际上这就是和服务器做交互行为， 通过发送请求，再到服务器响应服务，这就完成了我们获取资源的目的。 客户端请求–Request： Request，即请求，由客户端向服务端发出。可以将Request分为四部分： 请求方法（Request Method） 请求链接（Request URL） 请求头（Request Headers） 请求体（Request Body） 接下来，我们就分别来谈谈这四个部分的具体内容… Request Method请求方式，请求方式常见的有两种类型，GET 和 POST。 Get： 我们在浏览器中直接输入一个URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里（GET 方式请求提交的数据最多只有 1024 字节） Post： Post 请求大多为表单提交发起，如一个登录表单，输入用户名密码，点击登录按钮，这通常会发起一个 POST 请求，其数据通常以 Form Data 即表单的形式传输，不会体现在 URL 中。（POST 方式请求提交的数据没有限制） 而且Post请求在Request Headers中的Content-Type标识——只有设置为application/x-www-form-urlencoded 才是Post请求。 其他请求方式 我们平常遇到的绝大部分请求都是GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下： Request URL：顾名思义，就是请求的链接网址，我们发起的请求，必须包含URL。这是发起网站请求最重要的信息。 Request Headers：请求头，用来说明客户端要使用的附加信息，比较重要的字段有：Cookie，Referer、User-Agent 以及 Content-Type等等。 Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。 因为python爬虫脚本，默认的请求头参数会表明 这是由 python 发出的请求，而不是浏览器。 Request Body：请求体，一般承载的内容是Post请求的Form Data，即表单数据，其一般承载着发送往服务器的信息，例如账号、密码等 而对于Get请求中，Request Body的内容为空。 所有信息存储在 URL中。 服务器响应–Response Response，即响应，由服务端返回给客户端。 Response 可以划分为三部分，Response Status Code、Response Headers、Response Body。 Response Status Code：响应状态码，此状态码表示了服务器的响应状态，如 200则代表服务器正常响应，404 则代表页面未找到，500则代表服务器内部发生错误。 Response Headers：响应头，其中包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等，下面对一些常用的头信息说明： Date：日期，标识 Response 产生的时间。 Last-Modified，指定资源的最后修改时间。 Content-Encoding，指定 Response 内容的编码。 Server，包含了服务器的信息，名称，版本号等。 Content-Type，文档类型，指定了返回的数据类型是什么，如text/html 则代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回了图片。 Set-Cookie，设置Cookie，Response Headers 中的 Set-Cookie即告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。 Response Body：响应体，响应的正文数据都存放于此，如果请求一个网页，它的响应体就是网页的HTML代码，或者是Json格式的数据；请求一张图片，它的响应体就是图片的二进制数据。 爬虫的解析都是根据响应体进行操作的。 好了，以上就是简单的讲解了爬虫相关的网页基础内容，下期作者会讲解爬虫的原理基础，以及实现过程。Bye~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
