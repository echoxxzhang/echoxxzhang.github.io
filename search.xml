<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Servlet学习—Day1-C/S基础]]></title>
    <url>%2F2019%2F12%2F01%2FServlet%E5%AD%A6%E4%B9%A0%E2%80%94Day1-C-S%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[前言好久没更新博客了，我没有偷懒哈哈哈，因为最近在学习Java基础，其他学习方向就放置了。有过一门编程基础之后，学习其他语言真的是可以为所欲为（其实没有！！） python是脚本语言，跟Java还是有很多地方不一样的。 好了，先回归正题，接下来更新的系列是：Serlvet 。这个系列要求读者有一些Java基础。（嗯，下面这些基本够了） Serlvet 是很久之前的技术吧，先谈谈我为什么还要学他。现如今的JavaEE，Spring一家独大，一门新技术的出现可能意味着其他内容的消亡。但是有一句话说的话——万变不离其宗。在我调研了几天发现 Serlvet还是被依赖的技术。所以，不仅要学，还要学深。 但是例如SSH框架、JSP，就没有学习的必要了。为什么？ 没有为什么，听我的，全听我的。 知识铺垫HTTP基础哈哈哈，之前说的这两篇就很详细啦！！没看过的旁友迅速瞄一下。 计算机网络Day6-应用层协议 爬虫Day1-HTML基础 C/S架构 这是个老生常谈的东西啦，看图就能看懂的（get！） 从浏览器地址行输入一个URL这是一个面试很常考的问题：“在地址栏输入一个网址，浏览器实际上做了什么？”我们知道，要在互联网上访问另一台主机（服务器也是主机），需要通过IP地址形式。但是，IP地址不便于人类记忆，因此我们采用域名形式进行映射（baidu.com），通过域名可以找到该主机。 首先浏览器会访问电脑上的hosts文件。 12- 如果找得到，则直接解析地址，例如localhost，映射为127.0.0.1- 如果没找到，则向DNS发出解析请求 DNF服务器收到后，会在DNS缓存中寻找该域名对应的IP地址映射。 12- 如果找到了，返回给浏览器- 如果没找到，接着向上级DNS请求解析，直到拿到解析内容为止 这时，浏览器就拿到了网址的IP地址。 浏览器向该IP地址发出HTTP请求连接，可能直接请求服务器的静态资源，或者是个动态的请求信息。 服务器解析浏览器发来的请求（假设一切正常） 12- 如果是静态资源则直接返回- 如果是动态请求，例如url后面拼接的（？username=xx）。则会运行服务器端的程序，获得响应内容后再回送给浏览器 浏览器拿到响应内容后进行渲染，呈现页面给用户。 图解： Just so cool，对吧~ 概况Servlet（Server Applet）是Java Servlet的简称，称为小服务程序或服务连接器，用Java编写的服务器端程序。 Servlet带给我们最大的作用就是能够处理浏览器带来HTTP请求，并返回一个响应给浏览器，从而实现浏览器和服务器的交互。 环境准备： IDEA（好用） 去Apache官网下载tomcat JDK8 关于环境搭建，网上教程一大把，由于文章篇幅问题，我就不写在这里面了。 简单的Servlet程序一、继承HttpServlet 二、复写方法 service方法中定义的内容，是我们每一次请求Servlet服务所运行的方法。也就是说，当我们在url上请求我们的项目时，它就会运行一次service方法。 在上面我们重写的是Servlet的接口，要实现5个方法。这样太麻烦了！而HttpServlet类已经实现了Servlet接口的所有方法，编写Servlet时，只需要继承HttpServlet，然后重写你需要的方法即可，所以上面的方法不一定每一次都要重写，我只是介绍了有这些方法而已。 一般我们都是重写了doPost（）或doGet（）方法，当Servlet被访问时，会先去找service方法，如果没有，会继续找到我们复写的get、post方法（划重点） 三、配置注解我们需要在web.xml中配置项目映射 servlet-name由我们自己分配，只要两个servlet-name相同，即可对应上。 url-pattern就是我们在url上请求的资源路径，他会经过当前项目的web.xml文件进行过滤，当本项目文件没有遭到规则拦截，会去全局的web.xml继续匹配。再没有则报出404错误。 当我们的servlet的版本是3.0以后时，我们可以通过更简便的方法——注解 简单多了有没有！！！ 四、运行Servlet实例 可以看到，在控制台输出了“HelloWorld”。我们入了个门，不是么，嘻嘻~ Servlet生命周期 加载Servlet。当Tomcat第一次访问Servlet的时候，Tomcat会负责创建Servlet的实例 初始化。当Servlet被实例化后，Tomcat会调用init()方法初始化这个对象 处理服务。当浏览器访问Servlet的时候，Servlet 会调用service()方法处理请求 销毁。当Tomcat关闭时或者检测到Servlet要从Tomcat删除的时候会自动调用destroy()方法，让该实例释放掉所占的资源。 卸载。当Servlet调用完destroy()方法后，等待垃圾回收。 简单总结：只要访问Servlet，service()就会被调用。init()只有第一次访问Servlet的时候才会被调用。 destroy()只有在Tomcat关闭的时候才会被调用。而加载与卸载这两部分不用我们关心，这是Servlet容器自动处理的。 后记Servlet第一节就到这了，后面还有很多知识没讲，我们下篇再见，peace！]]></content>
      <categories>
        <category>Servlet</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>JavaEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶之标识与别名]]></title>
    <url>%2F2019%2F10%2F20%2FPython%E8%BF%9B%E9%98%B6%E4%B9%8B%E6%A0%87%E8%AF%86%E4%B8%8E%E5%88%AB%E5%90%8D%2F</url>
    <content type="text"><![CDATA[前言在python中，原本一句简简单单的赋值语句，竟然也可以牵扯出复杂的内存空间的关系。 为什么这么说呢？ 快来看看今天对python标识、别名的剖析吧。 变量名不是盒子“变量名不是盒子，而是标识” 这句话形象的揭露了变量的本质。 举个栗子： 上图中，我们将 a 的列表赋值给变量b，发现 b 的内容也发生了改动。 为什么会这样呢？ 虽然我们用 “变量存储着可变的数据” 概括了变量的作用。 但我们不能理解为“变量是个盒子，装着数据”；而是要将其理解为 “存储的数据指向了这个变量名”。 说着有点拗口，看图吧！ 所以，a跟b都指向了同一块地址，b 也能看出内部数据被修改的痕迹咯。 为了理解 Python 中的赋值语句，应该始终先读右边。对象在右边创建或获取，在此之后左边的变量才会绑定到对象上，这就像为对象贴上标注。 别名通过等号赋值，当一个变量赋值给另一个变量后，就可以说有了多个别名。 别名相当于多个标识指向了同一片数据，is 运算符和 id 函数确认了这一点。 从各个别名上对此所绑定的对象上做出的改动，都会互相影响到对方 is 运算符比较两个对象的标识； id() 函数返回对象标识的整数表示。 新对象绑定与浅复制如果是重新定义的对象绑定，就不是同一个标识了。什么意思呢？ 我们可以理解为，使用的是新创建的对象、或者说使用浅复制的对象绑定，就不是同一个标识。 my_list3 和 new_list 虽然内容和 my_list1 一样，但是指向的并不是同一片内存地址。 共享传参的后果Python 唯一支持的参数传递模式是共享传参（call by sharing） 共享传参指函数的各个形式参数获得实参中各个引用的副本。也就是说，函数内部的形参是实参的别名。 这样会导致：函数可能会修改作为参数传入的可变对象，不过无法修改那些对象的标识（即不能把一个对象替换成另一个对象） 发现不可变类型的数据类型定义的值不会发生改变； 而可变的数据类型定义的值经过函数运行后发生改变。 Ps：之所以导致这种情况，是因为整数的+=自增操作符和列表的区别。之前有说过 Python进阶之序列的其他姿势 不要使用可变类型作为参数的默认值将可变参数当作默认值可能还会导致一系列问题，我们举个栗子看看吧： 123456789class HauntedBus: def __init__(self, passengers=[]): self.passengers = passengers def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 功能很简单，创建一个类，其构造函数默认是一个列表。 功能函数有两个，一个是增加传进去的参数到列表中，另一个是从列表中删除。 一切都如我们想象的那么美好，但实际情况却是这样的： 哈？bus2竟然有两个参数？？明明我们只是添加了一个啊… 让我们看看究竟是什么原因： 这是因为 self.passengers 变成了 passengers 参数默认值的别名。出现这个问题的根源是，默认值在定义函数时计算（通常在加载模块时），因此默认值变成了函数对象的属性。 另一个使用默认值的空列表时，它就不会重新创建一个新列表，而是使用已经创建过的列表进行初始化。 因此，如果默认值是可变对象，而且做出了修改，那么后续的函数调用都会受到影响。 所以，我们知道了，最好不要使用可变参数作为默认初始值，或者保证初始化的列表都是新的空列表。 所以我们应该这么写： 123456789101112131415class NewBus: def __init__(self, passengers=None): if passengers is None: # 如果，默认值为空，即不传参数，就生成一个空列表 self.passengers = [] # 确保了创建的是新的空列表 else: # passengers = passengers # 这么写会导致“别名”，也就是说，会更改外部传进来的列表（上面有说） self.passengers = list(passengers) # 改为传入一个副本 def pick(self, name): self.passengers.append(name) def drop(self, name): self.passengers.remove(name) 效果验证 写在最后我认为在python中，引用的概念是很重要的。弄清楚什么是别名、什么是标识，有利于识破python的“魔法” okk，此篇就讲到这里了，拜拜！]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶之装饰器]]></title>
    <url>%2F2019%2F10%2F19%2FPython%E8%BF%9B%E9%98%B6%E4%B9%8B%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言上一章我们讲了函数的本质，今天我们谈谈python的装饰器，这是一个现实开发中很常用的东西，快快学起来吧~ 需要函数本质作为理论基础，忘了的童鞋翻一翻：Python进阶之函数的那些事 装饰器基础知识装饰器定义：装饰器是可调用的对象，其参数是另一个函数（被装饰的函数）。 装饰器会处理被装饰的函数，然后把它返回，或者将其替换成另一个函数或可调用对象。 但是装饰器是有限制的：装饰器不能修改被装饰函数的源代码逻辑、装饰器不能修改被装饰函数的调用方式。 接下来，我们先看看学习装饰器需要的功能储备吧： 函数即变量（之前讲过了，这点略过） 高阶函数 嵌套函数 + 闭包 高阶函数当函数可以满足以下这两个条件之一，就可以称为高阶函数。 ①把一个函数名当作实参传给另一个函数 如上图，就是高阶函数的一种，他将hello函数传给了show_time，完成了函数运行计时功能 它实现了不修改另一个函数源代码的前提下为其添加功能,但是改变了函数的调用方式 ②函数的返回值是另一个函数 这也是高级函数的一种，它实现了不修改函数的调用方式，为其函数添加新功能这一特点。 但却有局限性，添加的功能全部在运行功能函数的上方，不灵活。 这两个高级函数都实现了装饰器一部分的功能，我们需要其他知识点来完善它。 嵌套函数嵌套函数的定义是，在一个函数内又有用def 声明的另一个函数。 嵌套函数是这样的： 123456789def foo(): print('in the foo') def bar(): print('in the bar') bar() # 处于foo函数体中，如果要运行，就需要先将bar函数定义foo() 闭包函数的内部引用了函数外部的变量，这就是闭包。 假如有个名为 avg 的函数，它的作用是计算不断增加的系列值的均值；例如，整个历史中某个商品的平均收盘价。每天都会增加新价格，因此平均值要考虑至目前为止所有的价格。 我们使用高阶函数进行编写这个函数的功能： 可以看到，我们实现了功能，但是这里面隐藏了个细节——闭包！ 运行此函数时，返回嵌套函数的引用，那么 agv=make_averager()就得到了一个对象（averager的内存地址）,问题来了，为什么函数内部还可以调用已经运行完毕的外部函数体中的series列表？不应该是函数运行之后，该函数体就跟着销毁吗？ 原因是这样的：在 averager 函数中，series 是自由变量（free variable）。自由变量指未在本地作用域中绑定的变量。 闭包它会保留定义函数时存在的自由变量的绑定，这样调用函数时，虽然定义作用域不可用了，但是仍能使用那些绑定。 一个装饰器123456789101112131415161718192021import timedef foo(func): def bar(name): start_time = time.time() time.sleep(3) func(name) # run函数实际的运行，是在这里 stop_time = time.time() run_time = stop_time - start_time print("运行耗费时间：", run_time) return bar@foo # 相当于run = foo(run)def run(name): print("I am %s" % name)run("zihao") 运行结果如下： run是功能函数的函数名，表示下面的函数将作为实参传入foo。 @后面跟的是添加新功能的函数（也就是装饰器），这个就类似于我们上面高阶函数第二点写过的 run = foo(run)，它使得我们不更改函数调用方式的前提下，添加新功能。 这一次，我们既没有修改被装饰函数的源代码逻辑、也没有修改被装饰函数的调用方式。一个简单的装饰器就被我们做出来啦！ 写在最后今天讲解了装饰器的内部实现过程，那么这篇文章就到这里了，期待下回与你们相遇。 see you~]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶之函数的那些事]]></title>
    <url>%2F2019%2F10%2F14%2FPython%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%87%BD%E6%95%B0%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[前言函数，学过编程语言基础的人都会使用，但是它背后的原理，以及其他扩展特性，不知道你们是否知道呢？ 让我们进入今天文章的内容，Python函数的那些事~ 函数即变量什么叫函数即变量呢？有两个理由支撑这个概念： ① 函数可以像变量一样当作参数传递 ② 函数的内存分布跟变量是一样的 接下来，我们一一讨论。 函数的内存分布我们知道，变量的赋值本质上做的就是两件事情： ① 在内存中生成一个新的地址空间，然后压栈； ② 为这片地址空间命名，将变量对应的内存地址绑定到变量名上； 例如：我们声明一个变量—— var = “Hello”，它的内存空间的分布是这样的… 要记住一点是：变量名是内存地址的引用。 而函数也是大同小异，当我们使用一下代码块声明函数时： 12def func(a,b): return a+b; 它的底层操作是这样的： ① 看到def关键字，会在内存空间中创建一个新空间，存储着函数体 ② 将func函数名指向了这片新空间的引用 函数可以当作参数传递 我们将函数体赋值给另一个变量var，发现var（）也可以运行成功。 我们了解了上面的内存分布后，就很容易理解这个行为了：函数体的内存空间多了一个新变量的引用。 我们将len函数当成一个参数传给了sort（）方法中的key关键字，让sort依据列表中参数的字符长度进行排序 关于key关键字，任何单参数函数都能作为 key 参数的运行依据（自定义的函数也可以） 所以，我们发现，函数有着跟变量非常相似的行为与功能，这就是函数即变量的推理了。 函数的可调用性函数的可调用性是如何实现的呢？ 可以看到，我们只是运行了c1这个类的实例，它就自动调用了speak方法； 原因是我们重构了__call__这个特殊方法，这也说明小括弧就是在调用__call__的过程 判断是否可调用 Python中，可调用对象有很多，判断对象是否能调用，最安全的方法是使用内置的callable()函数： 有了这一章做铺垫以后，无论是魔法函数还是装饰器等等，我们都可以轻易的套用知识去理解它。 下一次更新预知：函数装饰器和闭包 期待跟你一起傲游 python语言，拜拜~]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客置顶篇]]></title>
    <url>%2F2019%2F10%2F10%2F%E5%8D%9A%E5%AE%A2%E7%BD%AE%E9%A1%B6%E7%AF%87%2F</url>
    <content type="text"><![CDATA[谈谈这个博客这是由node.js+hexo+github 技术搭建的私人博客 里面分享的是我对现阶段的技术记录以及总结，还有一些随心所致的文章，用以记录生涯遇到的一些事情以及反思。 这一篇置顶文章是用来介绍本博客文章系列的主要内容的，具体介绍看下方： 博客内容概况 计算机网络（计算机中必须掌握的理论知识） 通过自底向上的介绍，了解网络协议的具体内容，明白网络通信中所发生的详细情况：计算机网络系列 python爬虫系列文章（面向小白，所见即所爬） 只要掌握了python的基础知识，即可进入python爬虫系列进行学习 真正做到所见即所爬~：Python爬虫系列 python进阶技术（掌握pythonic风格的代码的以及学习python中罕为人知的秘密） 这是正在连载中的系列文章，这也是我对python学习以来很多心得的总结 看完之后绝对能感受到python的魅力所在~：python进阶系列 后端Flask框架 基于python中web开发框架Flask的使用介绍，其中包含很多笔记 从零搭建一个属于自己的博客：Flask后端开发 python网络编程 包含很多科班学生必须掌握的知识点。比如：进程线程、并行并发、全局解释锁GUI、socket编程等等… 网络编程 Linux操作系统 Linux操作系统的常见命令，基础理论知识：Linux笔记 …… 未完待续！！ 版权与协议若无特殊说明，本站所有文章均为原创文章。你可以自由地对博客中内容进行分享、直至商业性的使用，但必须在文章末尾注明文章的出处。 更多了解我，请戳以下链接跳转：关于博主]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python进阶之字典背后的秘密]]></title>
    <url>%2F2019%2F10%2F09%2FPython%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%AD%97%E5%85%B8%E8%83%8C%E5%90%8E%E7%9A%84%E7%A7%98%E5%AF%86%2F</url>
    <content type="text"><![CDATA[前言除了我们上节讨论过元组、列表外，python基本数据类型中，还有字典这种性能非常出众的序列。 我们这一篇将讨论字典的本质，如何实现的、以及字典性能如此出众的原因。 字典的键在python的标准库里的所有映射类型都是利用 dict 来实现的，因此它们有个共同的限制——即只有可散列的数据类型才能用作这些映射里的键（只有键有这个要求，值并不需要是可散列的数据类型）。 什么是可散列的数据类型： 可散列的数据类型是指原子不可变的数据类型，如：str、bytes 和整数类型 都是可散列类型，包括frozenset 也是可散列的，因为根据其定义，frozenset 里只能容纳可散列类型。也就是说，字典的键只能是数据不可变的值。 所以，列表不能作为字典的键。那么不可变的元组能否作为键呢？ 答案是：只有当元组的内部元素都是可散列类型的值的情况下，元组才是可散列的。 字典是 Python 语言的基石。模块的命名空间、实例的属性和函数的关键字参数中都可以看到字典的身影。 跟它有关的内置函数都在__builtins__.__dict__模块中。正是因为字典至关重要，Python 对它的实现做了高度优化，而散列表则是字典类型性能出众的根本原因。 散列表散列表其实是一个稀疏数组（总是有空白元素的数组称为稀疏数组）。在一般的数据结构教材中，散列表里的单元通常叫作表元（bucket）。 在 dict 的散列表当中，每个键值对都占用一个表元，每个表元都有两个部分，一个是对键的引用，另一个是对值的引用。因为所有表元的大小一致，所以可以通过偏移量来读取某个表元。 Python 会设法保证大概还有三分之一的表元是空的，当快要达到这个阈值的时候，原有的散列表会被复制到一个更大的空间里面。（始终维持三分之一这个阀值） 如果要把一个对象放入散列表，那么首先要计算这个元素键的散列值。Python 中使用hash() 方法来做这件事情。 散列表算法为了获取 my_dict[search_key] 背后的值 ①Python 首先会调用hash(search_key) 来计算 search_key 的散列值，把这个值最低的几位数字当作偏移量，在散列表里查找表元（具体取几位，得看当前散列表的大小）。 ②若找到的表元是空的，则抛出 KeyError 异常；若不是空的，则意味着表元里会有一对 found_key:found_value。 ③这时候 Python 会检验 search_key == found_key 是否为真，如果它们相等的话，就会返回 found_value（也就是字典的值）。 散列冲突：如果 search_key 和 found_key 不匹配的话，这种情况称为散列冲突。发生这种情况是因为搜寻表元时，是使用偏移量进行查找的，所以有一定小概率上出现散列冲突。 解决方法是这样的：散列表算法会使用退避算法（另取偏移量）重新当作索引进行寻找表元，直到找到value，或者发生KeyError。 表元扩充：我们可能遇到过这种情况，在循环字典时，试图对字典进行修改，比如： 然后望着报错怀疑人生… 解决方法有以下两种： 将字典临时转为列表； 遍历字典时将要修改、删除的值放到另一个字典中，遍历结束后再对原字典进行更新 但我们要研究的永远不是怎么做，而是为什么要这么做~ 字典遍历过程中进行修改发生报错的原因： 往字典里添加新键或修改字典内容时，Python 解释器都可能做出为字典扩容的决定。扩容导致的结果就是要新建一个更大的散列表，并把字典里已有的元素添加到新表里。这个过程中可能会发生新的散列冲突，导致新散列表中键的次序变化。 如果你在迭代一个字典的所有键的过程中同时对字典进行修改，那么这个循环很有可能会跳过一些键——甚至是跳过那些字典中已经有的键。（所以python并不允许一边遍历一边修改字典） 字典查询的本质字典的查询速度是很快的，有人就做过这么一个实验，填充十倍递增的数据，看看不同容器（字典、集合、列表）的查询速度是多久。 可以看到，当数据达到一千万的时候，字典的查询速度依旧很快 为什么有这个优势呢？ 我们知道，字典维持着三分之一空表元，也就是说，字典一直有着空出来的内存。 dict 的实现是典型的空间换时间：字典类型有着巨大的内存开销，但它们提供了无视数据量大小的快速访问。 并且，字典的散列表算法也让其在短时间内得出结果（利用hash值进行搜寻匹配即可） 得到什么，就意味着失去什么，这是很公平的一件事。 在实际情景中，只有最合适的数据结构，没有最好的数据结构。 字典推导式最后我们谈谈生成字典的强悍方式——字典推导，这个是跟列表推导很相似的东西。 普通举例： 所谓推导式，就是使用for循环遍历可迭代对象，然后对每次循环中的变量进行更改，最后用不同容器进行存放的过程。 字典推导式也可以从任何以键值对作为元素的可迭代对象中构建出字典。对于一个承载成对数据的列表，它可以直接用在字典的构造方法中。 这涉及到了我们上篇Python进阶之序列的其他姿势中谈到的元组拆包以及结合了推导式这两个知识点。 推导式还能快速的完成一些小功能： 如果我们想要达成键值对反转的情景，可以这么做： 如果我们想在字典中对字符统计进行大小写统一，我们可以这么做： 关于python中的字典，我觉得是一个很强大的数据类型，反正我是很喜欢，哈哈~]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进阶之序列的其他姿势]]></title>
    <url>%2F2019%2F10%2F08%2FPython%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%BA%8F%E5%88%97%E7%9A%84%E5%85%B6%E4%BB%96%E5%A7%BF%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[前言开始连载python进阶系列的博文~ 这个系列是属于掌握python基础之后的一些进阶知识，从底层出发，探索怎么实现的过程。同时，还要掌握 pythonic 的书写方式，理解python独特的魅力~ come on！ PS：因为是进阶python，因此假定读者都是具有一定python基础知识。 Python的序列类型python 基本数据类型中，涉及序列概念的有：元组、列表、字典、字符串等等。对于不同的序列类型，我们可以将其分为两大类型：容器序列和扁平序列。 容器序列： list、tuple 和 collections.deque 这些序列能存放不同类型的数据。 扁平序列： str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。 接下来我们用多个例子来展示这两种不同序列的区别. *操作符带来的局面我们知道 * 这个操作符可以对序列进行复制几份然后拼接。这对于不同类型的序列有着不同的效果。 从第一个代码块可以观察发现：我们只修改了一处地方，却导致所有子列表的数值都发生了改变。这是因为容器序列存放着对象的内存地址的引用，连续乘上N次，就导致了一个问题，这些生成的元素，有了同一块内存地址的引用。可以理解为，子列表中都是对同一块内存地址的引用，当修改了引用内容时，其他指向该引用的变量也会发生改变。 但是扁平序列却不会发生这种情况，这是因为扁平序列存储的数据不是引用，而是实体数据，它对于*操作符是这么执行的：复制所要的内容，然后去内存空间新创建一片空间，然后将复制后的内容存放进去。 区别总结： 容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是实体数据而不是引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。 序列的增量赋值+= 背后的特殊方法是 __iadd__。但是如果一个类 没有实现这个方法的话，Python 会退一步调用 __add__。 不过特殊方法的具体实现不在我们这篇讨论的范围之内，我们要知道的，是增量赋值对于不同序列所产生的结果。 可以发现，整数在增量赋值之后，它的id号发生改变，而列表没有。 对于可变序列 对可变序列（例如 list、bytearray 和 array.array）来说，增量赋值会导致该序列会就地改动，就像调用了my_list.extend([1]) 一样。 对于不可变序列 而如果是不可变序列的话，从内存地址角度上看，是重新创建了一个新的对象，内容进行复制、修改，然后再指向映射。 结论：对不可变序列进行重复拼接操作的话，效率会很低，因为每次都有一个 新对象，而解释器需要把原来对象中的元素先复制到新的对象里，然后再修改新的元素。 但有一个例外是 str，因为对字符串做 += 实在是太普遍了，所以 CPython 对它做了优化：为 str 初始化内存的时候，程序会为它留出额外的可扩展空间，因此进行增量操作的时候，并不会涉及复制原有字符串到新位置这类操作。 讨论完不同序列的区别后，我们展开描述 python基础数据类型中元组和列表的其他姿势。 元组拆包元组的拆包就像一把利器，会对了地方就会感觉很 pythonic。有时候，别人会告诉你：元组是不可变的列表，但元组比这有用得多。 元组其实是对数据的记录：元组中的每个元素都存放了记录中一个字段的数据，外加这个字段的位置。正是这个位置信息给数据赋予了意义。 有了位置信息，我们可以用与元组中相同元素个数的变量进行平行赋值，这就是拆包了。 我们还可以利用*号来表示多个参数（就像函数形参定义的*args一样），可以同时接收多个值 拆包的实际使用： os.path.split() 函数就会返回以路径和最后一个文件名组成的元组 (path,last_part): 在进行拆包的时候，我们不总是对元组里所有的数据都感兴趣，_ 占位符能帮助处理这种情况 列表切片切片也是一把瑞士军刀，适用于很多种场景。它可以用来一个可迭代对象中部分的切取，拿到我们需要的内容。 对 seq[start:stop:step] 进行求值的时候，Python 在底层会调用 seq.__getitem__(slice(start, stop, step))。 所以当一个对象没有实现__getitem__特殊方法时，是无法完成切片操作的。 切片的设计思想： 我们知道，切片是左闭右开的（包含左边，不包含右边），可为什么它的设计思想是左闭右开的呢？或者说，这么设计有什么独特的好处？ python是以0作为起始下标的，当只有最后一个位置信息时，我们可以快速看出切片和区间里有几个元素，比如：range(2) 和 list[:2] 都是返回三个元素。（而不用每次都 -1） 当起始跟结束位置都可见，我们可以快速计算出切片和区间的长度个数，用后一个数减去第一个下标（stop - start）即可。 当需要列表分割时，可以利用任意一个下标来把序列分割成不重叠的两部分，只要写成 my_list[:x] 和my_list[x:] 就可以了（如上图行2和行4表示） 列表排序和内置排序列表排序： 有时候我们会发现，打印排序的列表，返回的却是None： list.sort 方法会对列表本事做出排序（列表发生实际修改）。这也是这个方法的返回值是 None 的原因，这个设计是在提醒你该方法不会新建一个新列表。（实际上如果一个函数或者方法对对象进行的是就地改动，那它就应该返回None，好让调用者知道传入的参数发生了变动，而且并未产生新的对象） 内置排序： 而内置排序却恰好相反，内置函数 sorted 它会新建一个列表作为返回值。这个方法可以接受任何形式的可迭代对象作为参数，甚至包括不可变序列或生成器。 而不管 sorted 接受的是怎样的参数，它最后都会返回一个列表。 sorted是这么操作的：将可迭代对象进行复制，然后基于复制体进行排序，最后将结果返回，这意味着内置排序并没有修改可迭代对象本身。 写在最后对于序列的一小部分总结就说到这里啦，ok，下回见！]]></content>
      <categories>
        <category>Python进阶</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask开发Day4-数据库]]></title>
    <url>%2F2019%2F09%2F23%2FFlask%E5%BC%80%E5%8F%91Day4-%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[前言在Flask中，其本身不支持数据库。如果想要使用数据库，则需要安装插件。绝大多数的数据库都提供了Python客户端包，它们之中的大部分都被封装成Flask插件以便更好地和Flask应用结合。数据库被划分为两大类：关系数据库和非关系数据库。 在Flask中，关于数据库的插件是Flask-SQLAlchemy，这个插件为流行的SQLAlchemy包做了一层封装以便在Flask中调用更方便，类似SQLAlchemy这样的包叫做Object Relational Mapper，简称ORM。 ORM允许应用程序使用高级实体（如类，对象和方法）而不是表和SQL来管理数据库。 意思就是说：ORM的工作就是将高级操作转换成数据库命令，我们只需编写高级的逻辑就行了。 SQLAlchemy不只是某一款数据库软件的ORM，而是支持包含Mysql、SQLite、PostgreSQL等数据库软件。很多教学都是采用SQLite，而我想使用Mysql 进行开发（不过也可以在开发的时候使用简单易用的SQLite，需要部署应用到生产服务器上时，则选用更健壮的MySQL或PostgreSQL服务） Flask-SQLAlchemy配置首先，安装插件： 1(venv) $ pip install flask-migrate 然后，我们在config.py文件中添加以下配置项： 123456import osclass Config(object): # ... SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://root:root@localhost/flask_db2' SQLALCHEMY_TRACK_MODIFICATIONS = False # SQLALCHEMY_TRACK_MODIFICATIONS配置项用于设置数据发生变更之后是否发送信号给应用，这里设置为Flask Flask-SQLAlchemy插件从SQLALCHEMY_DATABASE_URI配置变量中获取应用的数据库的位置。 我们这里直接指定了mysql，然后键入账号密码@主机 / 数据库名 由于使用的是Mysql数据库，在应用运行之前，需要先将数据库（这里的flask_demo2）进行创建。 接着，app/__init__.py文件变更如下： 1234567891011from flask import Flaskfrom config import Configfrom flask_sqlalchemy import SQLAlchemy # Flask的数据库插件from flask_migrate import Migrate # 数据库迁移引擎app = Flask(__name__)app.config.from_object(Config)db = SQLAlchemy(app)migrate = Migrate(app, db)from app import routes, models # model是用来定义数据库结构的脚本，后面我们会编写 SQLAlchemy类传入一个app（可以是多个数据库，所以要指明具体是哪一个应用），然后实例化出来的db是数据库对象。 migrate是数据库迁移对象，我们以后在数据库中的变更都是基于它进行操作的。 数据库模型定义数据库中一张表及其字段的类，通常叫做数据模型。ORM(SQLAlchemy)会将类的实例关联到数据库表中的数据行，并翻译相关操作。接下来完成我们之前说的数据库模型代码实现： 12345678910from app import dbclass User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True) email = db.Column(db.String(120), index=True, unique=True) password_hash = db.Column(db.String(128)) def __repr__(self): return '&lt;User &#123;&#125;&gt;'.format(self.username) User类继承自db.Model，它是Flask-SQLAlchemy中所有模型的基类。User被称为”模型“，而不是表明。 这个类将表的字段定义为类属性，字段被创建为db.Column类的实例，它可以传入字段类型以及其他可选参数 该类的__repr__方法用于在调试时打印用户实例。（更友好的打印，如下图） 数据库迁移技术在实际情况中，随着应用的不断增长，很可能会新增、修改或删除数据库结构。如果没使用数据库迁移技术的话，我们只能使用删表重建的方法，这导致了一个严重的后果：表数据清零。 这在实际使用肯定是无法忍受的缺陷，于是我们需要有一种技术来替代这种”笨方法“。 Alembic（Flask-Migrate使用的迁移框架）将以一种不需要重新创建数据库的方式进行数据库结构的变更。它是这么实现的： 每当对数据库结构进行更改后，都需要向存储库中添加一个包含更改的详细信息的迁移脚本。 当应用这些迁移脚本到数据库时，它们将按照创建的顺序执行。 初始化数据库 运行迁移初始化命令之后，你会发现一个名为migrations的新目录。该目录中包含一个名为versions的子目录以及若干文件。从现在起，这些文件就是你项目的一部分了，其负责代码版本管理（类似git的机制，详见我写的另一篇文章：Git入门指引）。 数据库迁移包含映射到User数据库模型的用户表的迁移存储库生成后，是时候创建第一次数据库迁移了。 -m可选参数为迁移添加了一个简短的注释。（相当于log的作用） 生成的迁移脚本现在是你项目的一部分了，需要将其合并到源代码管理中。 flask db migrate命令不会对数据库进行任何更改，只会生成迁移脚本。 要将更改应用到数据库，必须使用flask db upgrade命令。 应用到数据库 数据库关系对于关系型数据库来说，更难的是表关系的定义。我们已经有了User表，如果想要定义一张post表，同时想要实现类似查询一个用户发布了多少文章的语句，应该如何实现呢？ 莫慌， Flask-SQLAlchemy有助于实现这种查询。post表将具有必须的id、用户动态的body（内容）和timestamp（发布日期）字段。 除了这些预期的字段之外，我还添加了一个user_id字段，将该用户动态链接到其作者。 这种关系被称为一对多，因为“一个”用户写了“多”条动态。 修改后的app/models.py如下： 123456789101112131415161718192021from datetime import datetimefrom app import dbclass User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), index=True, unique=True) email = db.Column(db.String(120), index=True, unique=True) password_hash = db.Column(db.String(128)) posts = db.relationship('Post', backref='author', lazy='dynamic') def __repr__(self): return '&lt;User &#123;&#125;&gt;'.format(self.username)class Post(db.Model): id = db.Column(db.Integer, primary_key=True) body = db.Column(db.String(140)) timestamp = db.Column(db.DateTime, index=True, default=datetime.utcnow) user_id = db.Column(db.Integer, db.ForeignKey('user.id')) def __repr__(self): return '&lt;Post &#123;&#125;&gt;'.format(self.body) User类有一个新的posts字段，用db.relationship初始化。这不是实际的数据库字段，而是用户和其动态之间关系的高级视图，因此它不在数据库图表中。对于一对多关系，db.relationship字段通常在“一”的这边定义，并用作访问“多”的便捷方式。 db.relationship的第一个参数表示代表关系“多”的类。 backref参数定义了代表“多”的类的实例反向调用“一”的时候的属性名称。 这将会为用户动态添加一个属性post.author，调用它将返回给该用户动态的用户实例。 这里，我们变动了数据库的结构，所以要进行数据库迁移操作：migrate和upgrade 写在最后数据库的相关操作就是这些啦，主要的还是关系数据库中表的创建和字段的引用。其他的学会之后套用即可。]]></content>
      <categories>
        <category>后端开发</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之链接那些事]]></title>
    <url>%2F2019%2F09%2F20%2FLinux%E4%B9%8B%E9%93%BE%E6%8E%A5%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[链接是平时很常用的东西，它可以快速定位到文件真正的所在位置。运行起来也跟运行真正的所在文件一般无二，Linux下的链接分为软链接和硬（实体）链接。但是介绍链接之前，我们需要对inode号有一定的了解，我们先开看看inode是什么吧！ Linux的EXT文件系统可以分为以下三部分： data block （数据区块） inode table （inode 表格） Superblock （超级区块） 一个文件/目录一旦被创建，就有相关信息存在（权限信息，属主属组、创建日期等等）——我们称为元数据，而这些元数据，就存放于inode block中。 可以这么说：一个文件，具有两种信息，一种是文件数据，另一个是元数据。 每当创建一个文件，就会分配一个inode编号，inode号指向每一个文件。 命令 作用 Ls -I [路径] 查看inode_block df -I 查看inode信息（inode编号） 软链接链接的源文件必须写成能找到该链接文件的路径，这样，才能形成链接。我们可以把Linux下的软链接视为Widows的快捷方式，其作用也相似。 命令 Ln -s [源文件] [链接文件] 创建软链接，等同于快捷方式。 软链接有以下特点： 修改源文件，链接文件里面的内容会更改，反之亦然 删除链接文件，对源文件没影响；删改源文件，链接文件会失效。 软链接是指向文件名的 软链接的源文件和所创建的链接文件inode号并不相同。 Ln命令参数 注意 -s 参数的介绍： 如果没有添加s参数，默认是创建硬连接 硬链接从上面 ln 命令中的 -s 参数，我们知道了还有硬链接这么一个东东，我们可以将它理解为一个“指向原始文件inode的指针”，系统不为它分配独立的inode和文件。所以，硬链接文件与原始文件其实是同一个文件，只是名字不同。 我们每添加一个硬链接，该文件的inode连接数就会增加1；而且只有当该文件的inode连接数为0时，才算彻底将它删除。换言之，由于硬链接实际上是指向原文件inode的指针，因此即便原始文件被删除，依然可以通过其他硬链接文件来访问。 可以发现该文件的硬连接数为2，证明我们创建的是硬连接 硬连接的特点： 硬链接的源文件和所创建的链接文件inode号相同 修改源文件，链接文件里面的内容会更改，反之亦然 删除链接文件，对源文件没影响；删改源文件，对链接文件也无影响。 这么看来，硬连接好像比软链接强很多，有一句话这么说：存在即合理。所以啊，其实硬连接是有他的不到之处的，比如：不能跨文件系统创建、不能链接目录。 所以日常生活中，软链接使用的频率不低于硬链接，我们可以从需求的角度思考，使用什么方式的链接比较好。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之文件的特殊权限]]></title>
    <url>%2F2019%2F09%2F20%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E7%9A%84%E7%89%B9%E6%AE%8A%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[文件的特殊权限有时候在某需求方面上，单纯设置文件的 rwx 权限无法满足我们对安全和灵活性的需求，因此便有了SUID、SGID与SBIT的特殊权限位。这是一种对文件权限进行设置的特殊功能，可以与一般权限同时使用，以弥补一般权限不能实现的功能。比如让普通用户执行一个对该用户没有执行权限的文件等等。 一般这些特殊命令是结合某些修改、查看文件的命令使用的：如ps、passwd等等 SUID当s这个标志出现在文件所有者的x权限上时，例如文件权限状态“-rwsr-xr-x”，此时就称为 Set UID，简称为SUID的特殊权限。SUID有这样的限制和功能： SUID权限仅对二进制程序有效； 执行者对于该程序需要具有x的可执行权限； 本权限仅在执行该程序的过程中有效； 执行者将具有该程序所有者的权限。 举个例子：所有账号的密码记录在/etc/shadow这个文件中，并且只有root可以读和强制写入这个文件。但是，我们却可以看到，一般用户可以修改自己的密码，这是为什么呢？ etc/shadow是存放用户密码的文件 /bin/passwd命令修改密码（平时输入的passwd就是执行的这个文件，一切皆文件！敲黑板。） 由上可知：我们查看passwd命令属性时，发现所有者的权限由rwx变成了rws，其中x改变成s就意味着该文件被赋予了SUID权限。那么SUID是什么呢，简单来说就是，让普通用户暂时拥有root用户的权限。 ps：如果原先权限位上没有x执行权限，那么被赋予特殊权限后将变成大写的S。 SGID当s标志出现在文件所有者的x权限时称为SUID，那么s出现在用户组的x权限时称为SGID。（U表示user，G表示group）。SGID有如下功能： SGID对二进制程序有用； 程序执行者对该程序需具备x权限； 让执行者临时拥有属组的权限（对拥有执行权限的二进制程序进行设置） 在某个目录中创建的文件自动继承该目录的用户组（只可以对目录进行设置）。 针对规则：文件的属主属组一般会自动归属于执行这个操作的用户。设置了SGID之后，会归属于其扩展属组。 SBITSBIT 只针对目录有效，对于文件已经没有效果了。 SBIT对目录的作用是： 当用户对此目录具有w和x权限时，亦即具有写入权限时； 当用户在该目录下创建新文件或目录时，仅有自己和root才有权力删除。 当目录被设置SBIT特殊权限位后，文件的其他人权限部分的x执行权限就会被替换成t或者T，原本有x执行权限则会写成t，原本没有x执行权限则会被写成T。 想对其他目录来设置SBIT特殊权限位，用chmod命令就可以了。对应的参数o+t代表设置SBIT粘滞位权限 文件的隐藏权限Linux系统中的文件除了具备一般权限和特殊权限之外，还有一种隐藏权限，即被隐藏起来的权限，默认情况下不能直接被用户发觉。隐藏权限是为了更细分权限方面的东西而存在的。 chattr命令chattr命令用于设置文件的隐藏权限，格式为“chattr [参数] 文件”。如果想要把某个隐藏功能添加到文件上，则需要在命令后面追加“+参数”，如果想要把某个隐藏功能移出文件，则需要追加“-参数”。 比较常见的有a、i参数，加上了a、i 参数后，文件就无法删除（root用户也是无法删除！只能将文件权限改回来） lsattr命令lsattr命令用于显示文件的隐藏权限，格式为“lsattr [参数] 文件”。在Linux系统中，文件的隐藏权限必须使用 lsattr 命令来查看，平时使用的ls之类的命令则看不出端倪： 实例：]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker入门]]></title>
    <url>%2F2019%2F09%2F18%2FDocker%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[前言：以下是Docker的官方介绍： Docker 是一种最流行的容器化实现方案，和虚拟化技术类似，它极大地方便了应用服务的部署；又与虚拟化技术不同，它以一种更轻量的方式实现了应用服务的打包。使用 Docker，可以让每个应用彼此相互隔离，在同一台机器上同时运行多个应用，不过它们彼此之间共享同一个操作系统。Docker 的优势在于，它可以在更细的粒度上进行资源管理，也比虚拟化技术更加节约资源。 从上面的话中，我们可以提取出简要信息出来：方便应用服务部署，可以实现应用服务打包，多应用隔离….等等 其实我使用docker的初衷是由于之前学习的爬虫，后面接触到框架内容时，就有各种各样的环境部署问题，还记得那时候，一下子就花了大半天时间，去部署相关环境，弄得我后面抓狂了哈哈哈。 后来就听说有这么个虚拟化容器技术，然后我就上手使用了，后来…嗯，真香！ Docker总览 容器：Docker可以同时运用多个服务（存储在容器中），容器是Docker最重要的概念，容器承载着镜像，而镜像可以理解为跟虚拟机里面的镜像文件类似的东西 可以将应用服务打包成tar文件（就像把当前镜像写成一个文件，也可以由文件还原成镜像） 仓库存储着很多镜像文件、可以下载的软件等等，从仓库下载速度很快 Dockerfile是配置文件，可以构建镜像 我们今天这篇Docker入门介绍主要就是围绕这个图进行展开的。 现在我们动手实战吧：https://labs.play-with-docker.com/ 这个网站提供了Docker网页版，我们可以不用下载Docker，就可以在上面试试手。 Docker实战下载与运行： pull命令可以从仓库获取我们需要的镜像，这里使用nginx示例 可以看到，上面有个80的字样，我们就配置好了nginx服务（是不是超级简单！！） 网页效果： 查看当前运行服务 可以发现一些命令和Linux是有共同之处的 服务由几部分组成： ①container ID：服务运行的ID号 ② Name ：名称，可以使用–name 进行指定 ③ image：镜像文件 ④ 后面还有一些该服务的相关信息，比如创建使用的命令、运行的服务时长、什么时候创建的等等 关闭进程 首先，使用docker container ls 或者 docker ps 查找到进程的ID号 然后使用 rm -f id号 进行删除 commit镜像 我们可以将当前运行的服务，生成为镜像文件 命令为：docker commit 进程id 将要生成的镜像名 运行commit的镜像文件 我们利用镜像文件生成了另一个nginx服务，映射成81的外部端口 镜像与 tar文件的交互第一步，写入到tar文件： docker save 镜像名 &gt; 文件名.tar （&gt;的意思是，使用覆盖的方式） 第二步，需要先删除镜像文件run的服务，不然无法删除该镜像文件（会提示正使用中） docker images查看当前的镜像文件。 第三步，使用load命令加入镜像文件 Docker 内部IP示意图 宿主机在这里我们举例为eth0口（容器与宿主机交互通过docker网卡转发路由） Docker安装后会生成一个docker0网卡（IP通常是127.27.0.1），而容器的IP会等于 Docker网卡IP +1+1… 主要实现：如果想要访问容器1的Mysql服务，我们需要在宿主机输入：127.27.0.2:3306 写在最后Docker的介绍就到这里啦~我觉得掌握了这个，就免去了很多环境部署的烦恼… 而且建议大家都在Linux系统下面部署开发环境。对！（一把辛酸泪）]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask开发Day3-Web表单]]></title>
    <url>%2F2019%2F09%2F17%2FFlask%E5%BC%80%E5%8F%91Day3-Web%E8%A1%A8%E5%8D%95%2F</url>
    <content type="text"><![CDATA[前言Web表单是所有Web应用程序中最基本的组成部分之一。表单实现的功能可以有：用户发表动态、登陆认证等 在Flask框架中，有一个专门的插件来处理Web表单，插件是Flask生态中的很重要的一部分，Flask故意设计为只包含核心功能以保持代码的整洁（包括第二章说的jinja2，它就是一个插件，只不过内置了而已），并暴露接口以对接解决不同问题。 Flask插件都是常规的Python三方包，可以使用pip安装： 1(venv) $ pip install flask-wtf 配置Flask_WTF表单SECRET_KEY对于新引入的插件，我们需要配置它，才能够对接使用——你需要决定传入什么样的配置变量列表到框架中。最基本的解决方案是使用app.config对象，它是一个类似字典的对象，可以将配置以键值的方式存储其中。 由于松耦合思想，我们可以在项目的顶层目录创建一个config.py文件专门来存储这些配置信息。 1234import osclass Config(object): SECRET_KEY = os.environ.get('SECRET_KEY') or 'this is my blog' SECRET_KEY是我添加的配置选项，对大多数Flask应用来说，它都是极其重要的。Flask及其一些扩展使用密钥的值作为加密密钥，用于生成签名或令牌。用于抵御CSRF攻击，并且，这个密钥是必配项。（不然打不开网页，它会提示你相关信息） 密钥被定义成由or运算符连接两个项的表达式。第一个项查找环境变量SECRET_KEY的值，第二个项是一个硬编码的字符串。（由你自己定义，什么都行，但是要保密一些）。这种首先检查环境变量中是否存在这个配置，找不到的情况下就使用硬编码字符串的配置变量。 应用配置拥有配置文件之后，我们要需要启动它（在__init__.py文件中修改如下）： 1234567from flask import Flaskfrom config import Config # new1app = Flask(__name__)app.config.from_object(Config) # new2from app import routes 导入了config这个配置文件的内容 使用app.config.from_object( )方法来通知Flask读取并使用配置信息 用户登陆表单Flask-WTF插件使用Python类来表示Web表单。表单类只需将表单的字段定义为类属性即可。我们将表单类单独存储到名为app/forms.py的模块中。 123456789from flask_wtf import FlaskFormfrom wtforms import StringField, PasswordField, BooleanField, SubmitFieldfrom wtforms.validators import DataRequiredclass LoginForm(FlaskForm): username = StringField('Username', validators=[DataRequired()]) password = PasswordField('Password', validators=[DataRequired()]) remember_me = BooleanField('Remember Me') submit = SubmitField('Sign In') Flask-WTF插件本身不提供字段类型，所有表示表单字段的类都存放于wtforms模块中，这里通过几个类就定义了表单类型，作用与我们在HTML中书写标签是类似的。 可选参数validators用于验证输入字段是否符合预期（用于控制表单），DataRequired验证器仅验证字段输入是否为空。（表示“被要求的”，即不能为空） 渲染表单下一步是将表单添加到HTML模板以便渲染到网页上。还记得我们之前第二章介绍过的做法吗，道理是一样的！ 我将把登录模板存储在文件app/templates/login.html 中 123456789101112131415161718&#123;% extends &quot;base.html&quot; %&#125;&#123;% block content %&#125; &lt;h1&gt;Sign In&lt;/h1&gt; &lt;form action=&quot;&quot; method=&quot;post&quot; novalidate&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &lt;p&gt; &#123;&#123; form.username.label &#125;&#125;&lt;br&gt; &#123;&#123; form.username(size=32) &#125;&#125; &lt;/p&gt; &lt;p&gt; &#123;&#123; form.password.label &#125;&#125;&lt;br&gt; &#123;&#123; form.password(size=32) &#125;&#125; &lt;/p&gt; &lt;p&gt;&#123;&#123; form.remember_me() &#125;&#125; &#123;&#123; form.remember_me.label &#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; form.submit() &#125;&#125;&lt;/p&gt; &lt;/form&gt;&#123;% endblock %&#125; 我们以前可能没见过这个模板中引用的语法：form.hidden_tag（）、form.username.label、form.username( )，接下来我会对其一一介绍。 form.hidden_tag()模板参数生成了一个隐藏字段，其中包含一个token。 对于保护表单（通俗的讲，这里的token和我们配置的SECRET_KEY变量就保护了表单免受CSRF攻击）。 这里引用的form，其实就是LoginForm表单类的实例，那么继承于LoginForm的实例，就有其相关的类属性（定义的字段类型）。 那么有了字段类型的属性之后，我们就可以引用它，使其渲染到页面啦。form.username.label的作用就在于此，这就应用了我们在form.py定义的 username 字段。 而form.username(size=32 )的意思是：此模板中的username和password字段将size作为参数，将其作为属性添加到&lt;input&gt; HTML元素中。同时最重要的是：form.field_name(）是体现字段特性的一个方法。 表单视图完成这个表单的最后一步就是编写一个新的视图函数来渲染上面创建的模板。 12345678910from flask import render_templatefrom app import appfrom app.forms import LoginForm# ...省略的代码@app.route('/login')def login(): form = LoginForm() return render_template('login.html', title='Sign In', form=form) 注意这里参数传递的使用：form=form，第一个form是模板中引用的变量名称，第二个form是实例化的对象。（python中参数传递都经常使用这种方式） 添加登陆链接在基础模板templates/base.html的导航栏上添加登录的链接，以便访问： 12345&lt;div&gt; Microblog: &lt;a href=&quot;/index&quot;&gt;Home&lt;/a&gt; &lt;a href=&quot;/login&quot;&gt;Login&lt;/a&gt;&lt;/div&gt; 页面效果： 接收表单数据点击提交按钮，浏览器将显示“Method Not Allowed”错误。要知道，我们只实现了如何显示表单，但没有逻辑处理表单数据，这也是报错的原因所在。 Flask-WTF可以轻松完成这部分工作， 以下是视图函数的更新版本，它接受和验证用户提交的数据： 12345678910from flask import render_template, flash, redirect@app.route('/login', methods=['GET', 'POST'])def login(): form = LoginForm() if form.validate_on_submit(): flash('Login requested for user &#123;&#125;, remember_me=&#123;&#125;'.format( form.username.data, form.remember_me.data)) return redirect('/index') return render_template('login.html', title='Sign In', form=form) 装饰器中传入 methods 参数，它告诉Flask这个视图函数接受GET和POST请求（客户端向服务端发送数据一般都为POST请求，通过传入methods参数，你就能告诉Flask哪些请求方法可以被接受。） form.validate_on_submit()实例方法会执行form校验的工作。当浏览器发起GET请求的时候，它返回False，这样视图函数就会跳过if块中的代码，直接转到视图函数的最后一句来渲染模板。 当用户在浏览器点击提交按钮后，浏览器会发送POST请求。form.validate_on_submit()就会获取到所有的数据，运行字段各自的验证器，全部通过之后就会返回True，这表示数据有效。 如果我们想要将某些内容显示给用户看，就可以使用flash（）。当你调用flash()函数后，Flask会存储这个消息（但不会直接出现在页面上，模板需要将消息渲染到基础模板中，才能让所有派生出来的模板都能显示出来。） redirect()这个函数指引浏览器自动重定向到它的参数所关联的URL。当前视图函数使用它将用户重定向到应用的主页。 接下来我们更新一下base.html模板，让其接受我们编写的逻辑代码，内容如下： get_flashed_messages()函数将flash存储的信息获取出来。我们使用for循环将其打印输出。 页面逻辑实现： 写在最后这样我们就完成了Web表单的基本功能了（填写数据、提交数据）。 让我们回顾一下流程操作： 引入插件、配置插件 应用配置 编写表单字段信息 渲染表单到页面 接受表单数据的逻辑]]></content>
      <categories>
        <category>后端开发</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask开发Day2-模板页面]]></title>
    <url>%2F2019%2F09%2F15%2FFlask%E5%BC%80%E5%8F%91Day2-%E6%A8%A1%E6%9D%BF%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[前言在上一篇 Flask开发Day1-Hello world中，我们已经创建好了一个Flask应用，接下来，我们就会针对他进行功能的添加，实现出一个中小型项目的web应用。 动态用户显示这个功能想要实现的是：根据不同用户显示出不同的欢迎界面 假设我们现在需要有一个欢迎用户的标题。虽然目前的应用程序还没有实现用户概念（需要与数据库打交道），但这不妨碍我使用一个Python字典来模拟一个用户。 1user = &#123;'username': 'Tom'&#125; 代码编写如下： 1234567891011121314151617181920from app import app@app.route('/')@app.route('/index')def index(): user = &#123;username:'Tom'&#125; return ''' &lt;html&gt; &lt;head&gt; &lt;title&gt;My home &lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello, ''' + user['username'] + ''' !&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; ''' 注意，要在html 代码中嵌入其他内容，需要使用三引号进行包裹。 页面效果： 那么问题来了，功能的确是实现了，但是以后网页的维护怎么办？ 我是指，当以后网页的用户变多了，我们总不可能在一张网页里写出所有用户的信息吧？而且如果哪天我决定更改这个应用的布局，那就不得不更新每个视图函数的HTML字符串。显然，随着应用的扩张，这种“写死”的方式完全不可行。 模板的实现模板有助于实现页面展现和业务逻辑之间的分离。脚本这块，负责逻辑的实现，而页面渲染的内容，交个html文件，这样就实现了前后端分离操作。 一张模板就相当于一个函数，我们可以不断调用它，从而实现渲染多个页面的效果，减轻了代码的重复性，也便于后期的修改（更改一处，处处改变）。 在Flask中，模板被编写为单独的文件，存储在应用程序包内的templates文件夹中。 在确定你在microblog目录后，创建一个存储模板的目录： 1(venv) $ mkdir app/templates 在下面可以看到你的第一个模板，它的功能与上面的index()视图函数返回的HTML页面相似。 把这个文件写在app/templates/index.html中： 12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;&#123;&#123; title &#125;&#125; - Microblog&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello, &#123;&#123; user.username &#125;&#125;!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 双花括号里面包含的内容是动态获取的，通过这种形式，我们可以获取到函数传入的参数。 渲染模板网页渲染转移到HTML模板之后，视图脚本（routes.py）就能被简化： 12345678from flask import render_templatefrom app import app@app.route('/')@app.route('/index')def index(): user = &#123;'username': 'Miguel'&#125; return render_template('index.html', title='Home', user=user) 将模板转换为完整的HTML页面的操作称为渲染。 我们需要从Flask框架中导入一个名为render_template()的函数。 render_template()函数调用Flask框架原生依赖的Jinja2模板引擎。它需要传入渲染的页面，还有变量参数列表（会替换模板中双花括号里面的代码块） 对比起之前return的一大串html代码，这样是不是更简洁呢？ 更重要的是，这个模板是可以多次使用的 jinja2在渲染过程中使用实际值替换占位符，只是Jinja2在模板文件中支持的诸多强大操作之一。 条件判断：123456789101112&lt;html&gt; &lt;head&gt; &#123;% if title %&#125; &lt;title&gt;&#123;&#123; title &#125;&#125; - Microblog&lt;/title&gt; &#123;% else %&#125; &lt;title&gt;Welcome to Microblog!&lt;/title&gt; &#123;% endif %&#125; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello, &#123;&#123; user.username &#125;&#125;!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 以上通过条件判断书写了两种情况的 title 显示（开始有了动态的感觉） 在jinja2中，通过特定格式传入控制语句，其他的还有 for循环、过滤器、分隔符等等，其方法格式在官网上都有，有兴趣的可以去官网学习：http://docs.jinkan.org/docs/flask/ 模板继承我们可能在平常的网页中可能会看到这么个情况：例如豆瓣评分，页面的框架大多相同，变化的只是其中呈现的内容，那么框架相同则意味着相同的代码，我们有没有一种方法，使得模板中也可以互相共用代码？ Jinja2有一个模板继承特性，专门解决这个问题。从本质上来讲，就是将所有模板中相同的部分转移到一个基础模板中，然后再从它继承过来。 例如，我们现在编写一个base.html基础文件： 123456789101112131415&lt;html&gt; &lt;head&gt; &#123;% if title %&#125; &lt;title&gt;&#123;&#123; title &#125;&#125; - Microblog&lt;/title&gt; &#123;% else %&#125; &lt;title&gt;Welcome to Microblog&lt;/title&gt; &#123;% endif %&#125; &lt;/head&gt; &lt;body&gt; &lt;div&gt;Microblog: &lt;a href=&quot;/index&quot;&gt;Home&lt;/a&gt;&lt;/div&gt; &lt;hr&gt; &#123;% block content %&#125;&#123;% endblock %&#125; &lt;/body&gt;&lt;/html&gt; 这里比较陌生的代码可能就是那个block关键字了。在jinja2中 block被赋予一个唯一的名称，派生的模板可以在提供其内容时进行引用。（就像HTML中link标签用来引入css文件一样） block content 是一种固定写法，它让Jinja2知道如何将这模板合并成在一起。 有了基础文件之后，我们就可以修改index.html，在其中渲染base.html文件 123456&#123;% extends &quot;base.html&quot; %&#125;&#123;% block content %&#125; &lt;h1&gt;Hi, &#123;&#123; user.username &#125;&#125;!&lt;/h1&gt; &#123;% endblock %&#125; extends语句用来建立了两个模板之间的继承关系，后面跟着的渲染模板的文件 这里block content里面出现的HTML代码就会当成附加内容，渲染到base.html里的block代码块中。 页面效果： 写在最后需要注意的是，如果网页出现以下内容： 可能就是你的代码书写错误，好好回想自己干了什么，然后去代码中看看 大多数情况下，都是jinja2语法错误，在templates/xxx.html 各种文件中，它的注释有独特的写法（详见官方文档），如果使用HTML的&lt;!-- --&gt;就会出现以上报错（碰到的坑）]]></content>
      <categories>
        <category>后端开发</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask开发Day1-Hello world]]></title>
    <url>%2F2019%2F09%2F15%2FFlask%E5%BC%80%E5%8F%91Day1-Hello-world%2F</url>
    <content type="text"><![CDATA[前言打算自己独自做一个小项目：半年时间内，用python+python周边库搭建出一个网站，这个网站服务端运用爬虫技术爬取一些数据，可以提供网页点播音乐功能。 其中 Flask作为Web服务主库，BeautifulSoup、requests作为爬虫主库，Mysql作为数据库。 所以，这个系列是我的笔记，也记录了这个项目中磕磕碰碰的坑。 环境搭建 python3.x windows 10 需要的库：Flask、python-dotenv Python在python官网即可下载适合自己操作系统的python版本，当在终端输入python时，有以下字样（可能有些不同），就表示 python 已安装成功： Flask我们知道，pip安装非常简单快捷，但是也有一个缺陷： pip工具会安装到全局Python目录下，即刻起，所有Python脚本都可以访问到这些包。但是如果你需要不同版本的库时，这时候麻烦就找上门了，版本问题会纠缠在一起，前期为了便捷安装留下的隐患就爆发了。 为了解决维护不同应用程序对应不同版本的问题，Python使用了虚拟环境的概念。 虚拟环境是Python解释器的完整副本。在虚拟环境中安装三方包时只会作用到虚拟环境，全局Python解释器不受影响。这就避免多个项目中版本纠缠的问题了。 项目开始 创建项目目录（我将这个项目命名为flask_demo1） 12$ mkdir flask_demo1$ cd flask_demo1 创建虚拟环境（基于python3.4及以上版本） 1$ python3 -m venv venv 使用这个命令来让Python运行venv包，它会创建一个名为venv的虚拟环境。 命令中的第一个“venv”是Python虚拟环境包的名称，第二个是要用于这个特定环境的虚拟环境名称。 注意，整个项目文件中，不要出现中文路径 有了虚拟环境这个目录之后，我们就可以启动虚拟环境了。 记得每次需要启动虚拟环境的时候都需要进入虚拟环境的文件夹的Scripts的目录下 activate命令启动虚拟环境、deactivate退出虚拟环境。 安装Flask 成功创建和激活了虚拟环境之后，就可以安装Flask了，命令如下： 1(venv) $ pip install flask “Hello World” Flask 应用应用包创建一个名为app的文件夹（即 Flask 的包）。你可以理解为这个文件夹存放着Flask的应用 注意，你这时候应该处于项目的顶部，也就是你所创建的项目文件（我的是”flask_demo1“） 1(venv) $ mkdir app __init__文件在其下创建文件__init__.py，输入如下的代码： 12345from flask import Flaskapp = Flask(__name__)from app import routes 前后带有双斜杆的 __init__.py 文件，当引入app这个包时，因为import包的本质，就是执行该包中名叫__init__.py的文件 这里有两个实体名为app。 app包由app目录和__init__.py脚本来定义构成，并在from app import routes语句中被引用。 app变量被定义为__init__.py脚本中的Flask类的一个实例。 routes模块是在底部导入的，而不是在脚本的顶部。 在最下面的导入是解决循环导入的问题，这是Flask应用程序的常见问题。 在后面，我们可能会在routes模块导入在这个脚本中定义的app变量，因此将routes的导入放在底部可以避免由于这两个文件之间的递归引用而导致的错误。 编写 app/routes.py 脚本在routes模块中有些什么？ 路由是应用程序实现的不同URL。 在Flask中，应用程序路由的处理逻辑被编写为Python函数，称为视图函数。 视图函数被映射到一个或多个路由URL，以便Flask知道当客户端请求给定的URL时执行什么逻辑。 通俗的说，这里定义了网页中 URL 所访问的资源文件。 123456from app import app@app.route('/')@app.route('/index')def index(): return "Hello, World!" # 回显的内容 ＠app.route修饰器在作为参数给出的URL和函数之间创建一个关联。 在Flask里面就是这么写的。 上面有两个装饰器，它们将URL的 /和/index索引关联到这个函数。 也就是说，当Web浏览器请求这两个URL中的任何一个时，都可以访问到这个文件。 应用程序要完成应用程序，你需要在定义Flask应用程序实例的顶层（也就是flask_demo1目录下）创建一个命名为start.py的Python脚本。 1from app import app 从app包导入其成员app变量。 如果操作正确，我们看到的是这么一个文件树： 123456flask_demo1/ venv/ app/ __init__.py routes.py start.py 以上，我们就把一个简单的”Hello World“的Flask小应用做好了！ flask命令由 Flask 安装，而不是你的应用。为了可以使用，它必须被告知可 以在哪里找到你的应用。FLASK_APP` 环境变量用于定义如何载入应用。 在运行之前，需要通过设置FLASK_APP环境变量告诉Flask如何导入它： 1(venv) $ set FLASK_APP=start.py 调试程序在终端键入flask run命令可以启动开发服务器，它在大多数情况下替代 Flask run方法。 运行如下命令来运行你的第一个Web应用吧： 123(venv) $ flask run * Serving Flask app "start" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) flask run`的输出表明服务器正在运行在IP地址127.0.0.1上，这是本机的回环IP地址。 我们访问http://127.0.0.1:5000/ 或 http://localhost:5000/index 这个地址，就可以看到我们的web应用！（在routes.py定义的路由） 运行结果 环境变量设置还记得在环境准备说的python-dotenv这个包吗，上面在终端会话中直接设置的环境变量不会永久生效，因此你不得不在每次新开终端时设定 FLASK_APP 环境变量，利用python-dotenv，我们可以”一劳永逸“。 1(venv) $ pip install python-dotenv 此时，在项目的根目录下新建一个名为 .flaskenv 的文件（建议在文本编辑器里面创建），写以下下内容： 1FLASK_APP=start.py 通过此项设置，FLASK_APP就可以自动加载了。 写在最后当学会基础的语法后，我们就可以进入项目实战，编程就是这么一回事。这也是我学习Flask的目的。 编程绝对是动手大于理论的一门学科。]]></content>
      <categories>
        <category>后端开发</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day18-反JS加密]]></title>
    <url>%2F2019%2F09%2F06%2F%E7%88%AC%E8%99%ABDay18-%E5%8F%8DJS%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[前言之前说过反爬机制有很多种，其中JS加密是最为常见的，今天我将介绍有道翻译的JS加密混淆的反爬。当遇到JS加密的时候，就会用到一些综合性比较强的知识点，以及如何去打断点等等，这都是我们应该了解学习的知识。 不然怎么说：一入爬虫深似海… URL = ‘http://fanyi.youdao.com/’ 正常模拟我们之前学习了 爬虫Day5-requests介绍 和 爬虫Day9-requests实战 知道了如何模拟发出请求，下面我们尝试一下使用 requests 发出请求，看能不能拿到数据。 老规矩，当爬取的网页是动态网页时，先使用抓包分析数据： 抓包分析 输入待翻译的文字 点击翻译 发现在抓包条目中，发现交互了一条 translate 报文 拿到翻译的URL 查看Form表单我们可以从上面的抓包中看出，这是个POST请求，那么必定有Form Data 表单，客户端与服务器端才能交互数据。 我们往下拉，看看所谓的Form Data 是什么： 发现 i 参数就是我们输入的待翻译文字 发现 salt、sign、ts、bv等神秘数字，其实这就是经过JS加密的参数了，我们后面会说。 其他的参数皆为常量，正常模拟发送就好了。 模拟请求接下来使用requests发送模拟请求就不细说了，直接贴图！ 我们使用requests 模拟headers、data 发送 post 请求 发现返回的结果却是 “errorCode”，证明我们请求失败了，已经被服务器识别出是爬虫脚本 问题出在哪呢？ 我们用浏览器能看到的东西，使用requests模拟应该也能拿到一样的数据啊，这就证明了一些类似“有效时间超时”、“蜜獾陷阱”等等一些反爬机制限制了我们制作的爬虫。 那么，如何如何识别这些反爬虫机制呢？ 我们先说说常见的反爬机制 出现乱码。关键信息不是正常字符，而是通过图片或者乱码来展示（通过渲染让乱码成为可阅读的正常字符） 判断是不是请求了网页上用户看不见的数据，而网页源码能看到的节点（蜜獾陷阱） 给予假数据（也叫投毒） 根据一定行为特征，封IP或者弹验证码。 某些请求参数进行JS加密，一般是随机数、时间戳等 那么，按以往我们说的正常套路肯定是搞不到翻译的内容了，我们需要进行反爬，今天将的有道翻译刚好是参数JS加密混淆，我们需要找出它如何加密，然后模拟加密的过程，最终的数据才能够被提交到服务器上。 JS加密反爬分析还记得我们上面说的那些神秘数字吗？问题就出在这上面。 第一次翻译： 第二次翻译： 可以看到，salt、sign是不断变化的，就这么看是很难看出规律的，这时候，我们就应该去 javascript 里面看看参数对应的数据是怎么来的 javascript脚本分析① 按住 ctrl + shift + f 进入search搜索框，输入salt 关键字 。可以看到这是由一个fanyi.min.js文件得到的。 ② 进入这个js文件，看看到底是何方神圣 ③ 格式化输入后，我们定位salt的位置, 这就是我们加密参数所处的位置了，有了这个，我们就可以观察所谓参数是如何生成的，以及打断点。 js参数断点我们在相应参数的行头点击一下，这样就打上了断点。打上断点之后我们就可以看到函数运行时相应参数的值分别是多少，根据JS语法推断出各个参数的对应关系 ‘e’ 是我们待翻译的值； ‘bv’ 对应 ‘t’ ，它是一个经过md5加密的参数； ‘ts’ 对应 ‘r’，它是一个字符串的时间戳； ‘i’ 由 ’r‘ 和一个十以内的随机数组成的； ‘sign’ 也是由md5 加密的组合字符串； 对于一些 js 的函数，我们可以到Console 运行一下，看看它返回的值是什么。 ① 在‘bv’ 参数中使用md5加密的 navigator.appVersion 可以发现结果是不变的，那么这是一个常量 ② r 参数中的 js 函数：(new Date).getTime() ③ i 参数就不用试了，有编程基础的都能看出端倪，这就是一个十以内的随机数 python脚本模拟知道参数怎么来的之后，我们就可以使用python进行模拟啦~ 发送请求时，携带上这些参数，就ok了！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import hashlibimport timeimport randomimport requestsimport jsondef get_sign(salt): first = 'fanyideskweb' e = KEYWORD i = salt last = 'n%A-rKaT5fb[Gy?;N5@Tj' str_result = first + e + str(i) + last md5 = hashlib.md5() md5.update(str_result.encode('utf-8')) sign = md5.hexdigest() return signdef get_bv(): version = "5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36" md5 = hashlib.md5() md5.update(version.encode('utf-8')) bv = md5.hexdigest() # 生成md5摘要 return str(bv)KEYWORD = "pear" # 翻译关键字ts = int(time.time() * 1000) # 形成时间戳salt = str(ts) + str(random.randint(0, 9)) # salt 参数url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'data = &#123; "smartresult": "dict", "to": "AUTO", "from": "AUTO", "doctype": "json", "salt": salt, "i": KEYWORD, "ts": str(ts), "sign": get_sign(salt), "action": "FY_BY_REALTlME", "keyfrom": "fanyi.web", "bv": get_bv(), "client": "fanyideskweb", "version": "2.1"&#125;headers = &#123; "Cookie": "OUTFOX_SEARCH_USER_ID=1266029150@10.169.0.82; OUTFOX_SEARCH_USER_ID_NCOO=1075804823.494775; DICT_UGC=be3af0da19b5c5e6aa4e17bd8d90b28a|; JSESSIONID=abcY7olcNKLe6b9xzie0w; ___rl__test__cookies=1567748207291", "Referer": "http://fanyi.youdao.com/", "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36"&#125;response = requests.post(url, data=data, headers=headers)result = json.loads(response.content.decode("utf-8")) # 使用json.loads，使其变成python的字符串print(result['smartResult']['entries'][1].strip()) # 取出翻译结果 运行结果： 写在最后如何打断点是个很重要的技能，这是一把利器，就看你能不能用好它。 这个JS混淆加密还是比较简单的反爬，还有一些比较难的比如CSS字体加密，就得需要其他方面的知识了，总的来说，爬虫真的是一门综合性很强的课。 一起努力吧~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之文件和目录的权限管理]]></title>
    <url>%2F2019%2F08%2F28%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E5%92%8C%E7%9B%AE%E5%BD%95%E7%9A%84%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言在Linux系统之后，有一个非常非常重要的概念就是：一切皆文件。也就是说，无论在Linux中看到的东西，都是由一个个文件构成的，例如说：命令、用户、各种配置等等。 尽管在Linux系统中一切都是文件，但是每个文件的类型不尽相同，因此Linux系统使用了不同的字符来加以区分。 查看文件各种详细属性 每一部分的意思依次为(从左往右看)： 权限信息 硬连接数（可以理解为存在 n个相同的文件） 属主：该用户所拥有该文件； 属组：该组具有哪些用户 文件大小 文件创建时间 文件名 共计七部分的内容，不过文件的属性还不止这些，还有特殊权限、隐藏权限等。 那么，先进入到今天的主题：权限信息 文件权限信息我们从上面知道，文件的详细信息的第一部分就是权限信息了。但这权限信息的内容，又得分四部分看： 第1位：指明该文件的类型 标识 文件类型 - 普通文件类型 d 代表目录 l 软链接（快捷方式） b 块设备 p 管道文件 c 字符设备文件 第2-4位：文件的属主对该文件的权限（没有该权限则使用-代表） 描述文件的权限共为三个标志位，分别为：读、写、执行。 标志位 作用 读 r（read） 表示能够读取文件的实际内容 写 w（write） 表示能够编辑、新增、修改、删除文件的实际内容 执行 x（execute） 表示能够运行一个脚本程序 注意：目录文件的权限和文件的权限控制并不相同，等下我们后面会说。 第5-7位：文件的属组对该文件的权限（group） 第8-10位：其他用户对该文件的权限（other） 注意： 一般来说，文件由谁创建的，属主和属组就是该用户，只有文件的属主才能修改权限（root 用户不受权限限制） 目录的权限管理目录的权限管理和文件的权限管理的标志位含义并不相同。 对目录下的文件操作，只跟目录的权限有关；对于目录下的文件内容操作，那是与文件操作相关的。比方说：文件能否被删除并不取决于自身的权限，而是看其所在目录是否有写入权限。 r：可以查（ls）看目录所拥有的文件 w：可以在该目录下创建、删除或重命名（修改文件不是，因为作用于文件本身，是属于文件操作的） x： 可以使用cd命令切换该目录下 属主属组的权限管理对于相同的一个文件来说，打开的人不同，其权限也大不相同。我们可以把文件的权限（rwx）看做属性，既然是属性，那么我们就可以对其进行修改。 修改文件属主：chmod [参数] 权限设置 文件或目录名称 修改文件属组：chown [参数] 所有者:所属组 文件或目录名称 修改属主： +：表示加入权限 -：表示去掉权限 =：表示设定权限 修改属组 这样，我们就把文件的属组从普通用户更改为 root 了 基于数字级的修改rwx权限，也可以使用数字进行表示（4、2、1） 以上的664就代表 “-rw-rw-r–”这样的权限信息描述。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之用户及用户组]]></title>
    <url>%2F2019%2F08%2F28%2FLinux%E4%B9%8B%E7%94%A8%E6%88%B7%E5%8F%8A%E7%94%A8%E6%88%B7%E7%BB%84%2F</url>
    <content type="text"><![CDATA[用户及用户组的相关概念UIDLinux系统的管理员之所以是root，并不是因为它的名字叫root，而是因为该用户的身份号码即UID（User IDentification）的数值为0。在Linux系统中，UID就相当于我们的身份证号码一样具有唯一性，因此可通过用户的UID值来判断用户身份。 管理员UID为0：系统的管理员用户。 系统用户UID为1～999： Linux系统为了避免因某个服务程序出现漏洞而被黑客提权至整台服务器，默认服务程序会有独立的系统用户负责运行，进而有效控制被破坏范围。 普通用户UID从1000开始：是由管理员创建的用于日常工作的用户。 Group：为了方便管理属于同一组的用户，Linux系统中还引入了用户组的概念。通过使用用户组号码（GID，Group IDentification），我们可以把多个用户加入到同一个组中，从而方便为组中的用户统一规划权限或指定任务。 另外，在Linux系统中创建每个用户时，将自动创建一个与其同名的基本用户组，而且这个基本用户组只有该用户一个人。如果该用户以后被归纳入其他用户组，则这个其他用户组称之为扩展用户组。一个用户只有一个基本用户组，但是可以有多个扩展用户组，从而满足日常的工作需要。 用户及用户组相关文件： 文件 作用 /etc/passwd： 显示所有用户详细信息。（冒号为分隔符） /etc/shadow： 存放用户密码的可 执行文件 /etc/group： 显示组信息 /etc/passwd： 七段信息分别为——用户名：密码：useid：组id：描述信息：家目录：用户的shell（标识是否可用登陆的可执行文件） /bin/bash是指可以登录，/sbin/nologin指不可以登陆 此时密码为x，不是说密码就是这个，而是被存储到shadow文件当中了。 /etc/group： 分为四段：组id：组密码：组id：组成员 属于某个组，就代表拥有这个组的权限 用户及成员组相关操作添加用户 命令 作用 useradd [选项] 用户名 增加某个用户名（并设置密码） adduser [选项] 用户名 在文件系统中创建用户 可以使用useradd命令创建用户账户。使用该命令创建用户账户时，默认的用户家目录会被存放在/home目录中，默认的Shell解释器为/bin/bash，而且默认会创建一个与该用户同名的基本用户组。 adduser 和 useradd 的区别是：useradd 只创建用户，创建完了需要用 passwd 命令去设置新用户的密码。adduser 会创建用户，创建目录，创建密码（提示你设置），做这一系列的操作。其实 useradd 更像是一种命令，执行完了就返回。而 adduser 更像是一种程序，需要你输入、确定等一系列操作。 删除用户 命令 作用 Usedel -r [usename] 删除某个用户名 -r：在删除该用户名的同时，删除这个用户的家目录等文件 -f：强制删除用户 创建用户组 命令 作用 groupadd [选项] 群组名 在工作中常常会把几个用户加入到同一个组里面，这样便可以针对一类用户统一安排权限。 用 groupadd 命令添加的用户，都处于扩展用户组中。普通的加组都是修改的扩展组列表，对所属组无影响。 修改用户属性 命令 作用 Usemod（修改） 可以修改已经创建的用户信息，诸如用户的UID、基本/扩展用户组、默认终端等 参数： 实例： 修改用户密码 命令 作用 passwd [选项] [用户名] passwd命令用于修改用户密码、过期时间、认证信息i锁定以及解锁等等 参数：]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之Shell变量和环境变量]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8BShell%E5%8F%98%E9%87%8F%E5%92%8C%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[Shell 变量所谓变量就是计算机中用于记录一个值（不一定是数值，也可以是字符或字符串）的符号，而这些符号将用于不同的运算处理中。通常变量与值是一对一的关系，可以通过表达式读取它的值并赋值给其它变量，也可以直接指定数值赋值给任意变量。 我们可以用 declare 预声明一个变量，或者即用即创建： Name=zihao（变量名不需要加美元符号$，PHP语言中变量需要） 需要特别注意的是：变量名和等号之间不能有空格。 同时，变量名的命名须遵循如下规则： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线（_）。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 变量的使用 建议使用变量时，将变量名加上花括号。有利于帮助解释器识别变量的边界，这是个好习惯。 只读变量在变量被创建了之后，使用 readonly 命令可以将变量定义为只读变量，只读变量的值只能被删除、但是不能被修改。 格式：“readonly 变量名” 删除变量使用 unset 命令可以删除变量。 环境变量 Shell 的环境变量作用于自身和它的子进程。在所有的 UNIX 和类 UNIX 系统中，每个进程都有其各自的环境变量设置，且默认情况下，当一个进程被创建时，除了创建过程中明确指定的话，它将继承其父进程的绝大部分环境设置。 也就是说，Shell 中运行的大部分命令都将以 Shell 的子进程的方式运行。 通常我们会涉及到的变量类型有三种： 当前 Shell 进程私有用户自定义变量，只在当前 Shell 中有效。 Shell 本身内建的变量。 从自定义变量导出的环境变量。 注： 为了与普通变量区分，通常我们习惯将环境变量名设为大写。 命令的查找顺序环境变量的作用是什么呢？或者说，当我们在 Shell 中输入一个命令，Shell 是怎么知道去哪找到这个命令然后执行的呢？这是通过环境变量 PATH 来进行搜索的。 在Linux系统中一切都是文件，Linux命令也不例外。那么，在用户执行了一条命令之后，Linux系统中到底发生了什么事情呢？简单来说，命令在Linux中的执行分为4个步骤。 第1步：判断用户是否以绝对路径或相对路径的方式输入命令（如/bin/ls），如果是的话则直接执行。 第2步：Linux系统检查用户输入的命令是否为“别名命令”，即用一个自定义的命令名称来替换原本的命令名称。可以用alias命令来创建一个属于自己的命令别名，格式为“alias 别名=命令”。若要取消一个命令别名，则是用unalias命令，格式为“unalias 别名”。我们之前在使用rm命令删除文件时，Linux系统都会要求我们再确认是否执行删除操作，其实这就是Linux系统为了防止用户误删除文件而特意设置的rm别名命令。 第3步：Bash解释器判断用户输入的是内部命令还是外部命令。内部命令是解释器内部的指令，会被直接执行；而用户在绝大部分时间输入的是外部命令，这些命令交由步骤4继续处理。可以使用“type 命令名称”来判断用户输入的命令是内部命令还是外部命令。 第4步：系统在多个路径中查找用户输入的命令文件，而定义这些路径的变量叫作PATH，输入命令时Bash解释器就会在这些位置中逐个查找。PATH是由多个路径值组成的变量，每个路径值之间用冒号间隔，对这些路径的增加和删除操作将影响到 Bash 解释器对 Linux命令的查找。 自定义环境变量其实变量是由固定的变量名与用户或系统设置的变量值两部分组成的，我们也可以自行创建变量，来满足工作需求。 但是，这样的环境变量不具有全局性，作用范围也有限，默认情况下不能被其他用户使用。如果工作需要，可以使用export命令将其提升为全局变量，这样其他用户也就可以使用它（全局变量：多用户都能使用） 永久生效的环境变量当关机后，或者关闭当前的 shell 之后，环境变量就会清空。如何让环境变量永久生效呢？ 按变量的生存周期来划分，Linux 变量可分为两类： 永久的：需要修改配置文件，变量永久生效； 临时的：使用 export 命令行声明即可，变量在关闭 shell 时失效。 关于配置文件 两个关于变量的重要文件： /etc/bashrc 和 /etc/profile ，它们分别存放的是 shell 变量和环境变量。 这个 .profile 只对当前用户永久生效。而写在 /etc/profile 里面的是对所有用户永久生效，所以如果想要添加一个永久生效的环境变量，只需要打开 /etc/profile，在最后加上你想添加的环境变量就好了。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之Shell脚本进阶]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8BShell%E8%84%9A%E6%9C%AC%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[前言之前的 Linux之Shell脚本初识 只是介绍了如何实现批处理的功能，但为了让Shell脚本程序更好地满足用户的一些实时需求，以便灵活完成工作，必须要让脚本程序能够像之前执行命令时那样，接收用户输入的参数。 接收用户参数的脚本假设，我们编写了这么一个脚本： 1234#!/bin/bashecho "当前脚本名称为$0"echo "一共有$#个参数，分别是$*。"echo "第一个参数为$1, 第五个参数是$5。" 运行结果： 可以看到，我们在脚本后面跟了几个参数，分别是英文的1~6 会看到脚本中，$n 就是取第n个参数，脚本名为第0个参数。 $# 可以统计参数个数，$* 将所有参数列出（命令通配符） 条件测试Shell脚本中的条件测试语法可以判断表达式是否成立，若条件成立则返回数字0，否则便返回其他随机数值。 注意：条件表达式两边都有一个空格 按照测试对象来划分，条件测试语句可以分为4种： 文件测试语句； 逻辑测试语句； 整数值比较语句； 字符串比较语句。 下面我们就针对以上四种情况进行介绍： 文件测试语句 实例： 我们使用文件测试语句来判断/etc/passwd 是否为一个文件 然后通过Shell解释器的内设 $? 变量显示上一条命令执行后的返回值。 如果返回值为0，则目录存在；如果返回值为非零的值，则意味着目录不存在。 逻辑测试 逻辑与：在Shell终端中逻辑“与”的运算符号是&amp;&amp;，它表示当前面的命令执行成功后才会执行它后面的命令 逻辑或：在Linux系统中的运算符号为||，表示当前面的命令执行失败后才会执行它后面的命令 逻辑非：第三种逻辑语句是“非”，在Linux系统中的运算符号是一个叹号（！），它表示把条件测试中的判断结果取相反值。也就是说，如果原本测试的结果是正确的，则将其变成错误的；原本测试错误的结果则将其变成正确的。 综合实例： 从左到右执行推断： 条件测试里面是逻辑非部分，得出结果是False（因为此时的USER就是 root） 接下来遇到逻辑与，我们知道前面是False，证明后面的东西不会输出 再往右看下去，是逻辑或：当前面命令执行失败时，执行||后面的命令。 于是，输出root 。 整数值比较整数比较运算符仅是对数字的操作，不能将数字与字符串、文件等内容一起操作。 字符串比较字符串比较语句用于判断测试字符串是否为空值，或两个字符串是否相同。它经常用来判断某个变量是否未被定义（即判断内容是否为空值）]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之Shell脚本初识]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8BShell%E8%84%9A%E6%9C%AC%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Shell 脚本是什么可以将Shell终端解释器当作用户与Linux系统内部的通信媒介，除了能够支持各种变量与参数外，还提供了诸如循环、分支等高级编程语言才有的控制结构特性。要想正确使用Shell中的这些功能特性，准确下达命令尤为重要。Shell脚本命令的工作方式有两种：交互式和批处理。 交互式（Interactive）：用户每输入一条命令就立即执行。（之前我们遇到的都是） 批处理（Batch）：由用户事先编写好一个完整的Shell脚本，Shell会一次性执行脚本中诸多的命令。 构建Shell 脚本编写Shell脚本，可以通过vim 进行文本编辑，输入批处理的命令集，然后保存并退出。vim 编辑器是所有Unix及Linux 系统下标准的编辑器，它的强大不逊色于任何最新的文本编辑器。 其实使用Vim编辑器把Linux命令按照顺序依次写入到一个文件中，这就是一个简单的脚本了。 Shell 脚本举例 Shell脚本文件的名称可以任意，但为了避免被误以为是普通文件，建议将 .sh 后缀加上，以表示是一个脚本文件。 在上面的这个example.sh脚本中实际上出现了三种不同的元素： 第一行的脚本声明（#!）用来告诉系统使用哪种Shell解释器来执行该脚本； 第二行的注释信息（#）是对脚本功能和某些命令的介绍信息，使得自己或他人在日后看到这个脚本内容时，可以快速知道该脚本的作用或一些警告信息； 第三、四行的可执行语句也就是我们平时执行的Linux命令了。 Vim 编辑器vim 编辑器的三种模式： 命令模式（默认)：刚进入vim的时候，默认就是 命令模式，可以复制行，删除行等。 输入模式：可以输入内容。 末行模式：在最下边，除编辑模式，可以输入诸多管理员命令 三种模式间的转换： 12345678命令模式→输入模式： i：在当前光标所在字符的前面，转为输入模式 a：在当前光标所在字符的后面，转为输入模式 o：在当前光标所在行的下方，新建一行，并转为输入模式输入模式→命令模式 ESC键命令模式→末行模式 输入：即可转为末行模式 使用举例： “vim + 路径/文件名” 进入到文件中， 如果文件不存在，那么会新创建一个空文件。 使用 a、i、o 进入编辑模式，然后对Shell 脚本的编写 编写完成后，键入ESC，回到命令模式 键入 “：”，并输入 wq （写入退出） 下面介绍一下三种模式一些指令。 命令行模式 组合键（命令） 含义 dd 删除光标所在的那一行 u 撤销上一步操作 Ctrl+r 恢复上一步操作 ndd n为数字，删除光标所在的向下n行 yy 复制光标所在的那一行 nyy n为数字，复制光标所在的向下n行 p 将已复制的数据在光标的下一行粘贴 实现剪切操作： dd+p G 光标移动到这个文件的最后一行 nG 移动到文件的第n行 gg 移动到文件的第一行 编辑模式： 组合键（命令） 含义 i： 在当前光标所在字符的前面，转为输入模式 I： 在当前光标所在行的行首转换为输入模式 a： 在当前光标所在字符的后面，转为输入模式 A： 在光标所在行的行尾，转换为输入模式 o： 在当前光标所在行的下方，新建一行，并转为输入模式 O： 在当前光标所在行的上方，新建一行，并转为输入模式 s： 删除光标所在字符 r： 替换光标处字符 末行模式： 脚本编写之后，我们就可以运行它啦！ 运行Shell 脚本 其返回的结果，就是我们在脚本中输入的命令啦。这就实现了批处理的效果。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之命令行那些事]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[输入输出重定向输入重定向是指把文件导入到命令中，而输出重定向则是指把原本要输出到屏幕的数据信息写入到指定文件中。 在日常的学习和工作中，相较于输入重定向，我们使用输出重定向的频率更高，所以又将输出重定向分为了标准输出重定向和错误输出重定向两种不同的技术，以及清空写入（&gt;）与追加写入（&gt;&gt;）两种模式。 标准输入输出默认情况下，command &gt; file 将 stdout（标准输出） 重定向到 file； command &lt; file 将stdin （标准输入）重定向到 file。 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息 错误输出重定向 对于重定向中的标准输出模式，可以省略文件描述符1不写，而错误输出模式的文件描述符2是必须要写的。 使用场景：当用户在执行一个自动化的Shell脚本时，这个操作会特别有用，因为它可以把整个脚本执行过程中的报错信息都记录到日志文件中，便于后期的排错工作。 命令通配符假设想要批量查看所有日记文件的相关权限属性，我们应该如何做呢？难道一个个去查吗？ 当然不是的，日志文件命名格式大多相同，我们可以利用命令通配符将其选中，然后排列查看。 顾名思义，通配符就是通用的匹配信息的符号，比如星号（*）代表匹配零个或多个字符，问号（?）代表匹配单个字符，中括号内加上数字[0-9]代表匹配0～9之间的单个数字的字符，而中括号内加上字母[abc]则是代表匹配a、b、c三个字符中的任意一个字符。（有没有发现很像正则！） 实例： 可以看到现在有多个日志文件，其中混入了几个命名格式与其他大多数不同的文件。假设我们要将其删除，我们如何使用一句命令进行批处理删除呢？ 实现了删除某些不同日期文件格式文件的需求 Shell解释器 会拿着你编写的命令通配符，与路径中的文件一一匹配，然后返回匹配成功的文件。 实例2： 如果我们想创建的文件，它的名字都类似:file0.txt,file1.txt … … file9.txt等等。 如何实现一个命令批量创建？ 转义字符关于命令通配符，有时候会有特殊情况发现，比如说，文件的命名中有特殊字符（元字符、通配符等等），这时候就需要转义字符出马了。 4个最常用的转义字符 反斜杠（\）：使反斜杠后面的一个变量变为单纯的字符串。若 \ 后跟的是非元字符，与没有加\的效果一样 单引号（’）：转义其中所有的变量为单纯的字符串。 单引号也是转义字符—硬转义（引号内部所有的shell元字符、通配符、都会被关掉） 双引号（”）：保留其中的变量属性，不进行转义处理。软转义，引号内部只允许出现特定的 shell 元字符, $用于参与代换 用于代替命令。 反引号（``）：将其包裹的命令执行后返回结果。(使用情景就像用括号包起来的运算式，会优先计算) 实例： 对于管道符、元字符等等这些特殊符号，我们当成字符串输出的时候，记得要使用转义字符 记得一个单引号里面不能继续出现单引号，shell 解释器会不知道你要干嘛。（解决办法：双包单或者单包双） 对于命令行我们有时候的需求是：上一个命令的结果被下一个命令调用，一般来说我们需要创建一个临时存储的东西，但如果我们掌握了管道符操作，就可以省略这个步骤了。 管道符管道命令符的作用也可以用一句话来概括“把前一个命令原本要输出到屏幕的标准正常数据当作是后一个命令的标准输入” 其执行格式为“命令A | 命令B”。 以上就通过管道符实现了将example.sh文件输出结果进行行数统计的功能 &amp;&amp;和 || 命令格式 作用 cmd1 &amp;&amp; cmd2 1. 若 cmd1 执行完毕且正确执行（$?=0），则开始执行 cmd2。 2. 若 cmd1 执行完毕且为错误 （$?≠0），则 cmd2 不执行。 cmd1 || cmd2 1. 若 cmd1 执行完毕且正确执行（$?=0），则 cmd2 不执行。 2. 若 cmd1 执行完毕且为错误 （$?≠0），则开始执行 cmd2。 对于命令行的使用，要掌握的往往是平时所经常使用的命令，学习这些知识时，需要理清符号的作用，当以后需要某些功能时，能想起来就可以了。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之文件目录管理命令]]></title>
    <url>%2F2019%2F08%2F27%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[touch 命令touch命令用于创建空白文件或设置文件的时间，格式为“touch [选项] [文件]”。 如果创建的文件已存在，并不会进行覆盖操作，而是什么都不执行。 Touch 还可以修改属性：比如修改设置文件内容的修改时间（mtime）、文件权限或属性的更改时间（ctime）与文件的读取时间（atime）。 选项： 实例： 使用 echo 输出了 “hello world” 追加到 new_log 文件中，会让文件时间发生改变。 然后再使用 touch -d 命令将文件改为原来的时间，别人就看不出来你修改过文件。 mkdir 命令mkdir命令用于创建空白的目录，格式为“mkdir [选项] 目录”。 在Linux系统中，文件夹是最常见的文件类型之一。除了能创建单个空白目录外，mkdir 命令还可以结合 -p 参数来递归创建出具有嵌套叠层关系的文件目录。 实例： 嵌套叠层关系的文件目录类似树型结构的问题，如果某一段树枝还没“长”出来，那么就无法生出树叶。 但是使用了 -p 后，如果检测到 某个文件还未创建，它会自动帮你创建，直到长出“树叶”。 cp 命令cp命令用于复制文件或目录，格式为“cp [选项] 源文件 目标文件”。大家对文件复制操作应该不陌生，在Linux系统中，复制操作具体分为3种情况： 如果目标文件是目录，则会把源文件复制到该目录中； 如果目标文件也是普通文件，则会询问是否要覆盖它； 如果目标文件不存在，则执行正常的复制操作。 选项： mv 命令mv命令用于剪切文件或将文件重命名，格式为“mv [选项] 源文件 [目标路径|目标文件名]”。 剪切操作不同于复制操作，因为它会默认把源文件删除掉，只保留剪切后的文件。如果在同一个目录中对一个文件进行剪切操作，其实也就是对其进行重命名： 可以看到，当前目录已经没有 old_log ，new_log 已经取而代之了。 rm 命令rm命令用于删除文件或目录，格式为“rm [选项] 文件”。 在Linux系统中删除文件时，系统会默认向您询问是否要执行删除操作，如果不想总是看到这种反复的确认信息，可在rm命令后跟上-f参数来强制删除。另外，想要删除一个目录，需要在 rm 命令后面加一个 -f 参数才可以，否则删除不掉。 可以看到，如果不带 -r 参数，如果删除的东西是个目录，它会提示无法删除目录的信息 如果删除的是个目录，那么里面的所有内容都会跟着删除，使用这个 rm 命令要慎用。 file 命令file命令用于查看文件的类型，格式为“file 文件名”。 在Linux系统中，文本、目录、设备等所有这些一切都统称为文件，而我们又不能单凭后缀就知道具体的文件类型，这时就需要使用file命令来查看文件类型了。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之打包压缩与搜索命令]]></title>
    <url>%2F2019%2F08%2F26%2FLinux%E4%B9%8B%E6%89%93%E5%8C%85%E5%8E%8B%E7%BC%A9%E4%B8%8E%E6%90%9C%E7%B4%A2%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[tar 命令tar命令用于对文件进行打包或解包，格式为“tar [选项] [文件] [可选：目标文件]”。 在Linux系统中，常见的文件格式比较多，其中主要使用的是.tar或.tar.gz或.tar.bz2格式，不过不用担心格式太多而记不住，其实这些格式大部分都是由tar命令来生成的。 选项： -c参数用于创建打包文件，-x参数用于解包文件，因此这两个参数不能同时使用。 -z参数指定使用Gzip格式来打包或解包文件，-j参数指定使用bzip2格式来打包或解包文件。 用户使用解压时则是根据文件的后缀来自动决定应使用何种格式参数进行解压。 -f 代表要打包或解包的软件包名称。（后面跟文件名，严格来说-f参数应该放到最后面） 实例： 使用 “tar -czvf 压缩包名称 .tar.gz 要打包的目录” 相应的解压命令为 “tar -xzvf 压缩包名称.tar.gz” 压缩压缩：将一个大的文件通过一些压缩算法得到一个小文件。（主要是缩小文件的大小，节省磁盘空间，便于网络传输） gzip：将后面的文件进行压缩 gunzip：将后面的gz压缩包进行解压缩 grep 命令grep命令用于在文本中执行关键词搜索，并显示匹配的结果，格式为“grep [过滤参数] [目标文件]”。 选项： 比较常用的参数： -n参数用来显示搜索到信息的行号； -v参数用于反选信息（即没有包含关键词的所有信息行）。 实例： ps -aux 输出所有进程 | 称为管道符，可以把前一个命令原本要输出到屏幕的标准正常数据当作是后一个命令的标准输入 以上命令的意思：将ps -aux 查看到的内容交给 grep 进行过滤，将 Firefox 的相关进程过滤出来。 find 命令find命令用于按照指定条件来查找文件，格式为“find [匹配参数] [寻找条件后执行的操作]”。 “Linux系统中的一切都是文件”。在Linux系统中，搜索工作一般都是通过find命令来完成的，它可以使用不同的文件特性作为寻找条件（如文件名、大小、修改时间、权限等信息），一旦匹配成功则默认将信息显示到屏幕上。 匹配参数： 实例： 使用 -name 匹配名称，搜索名叫 new_log 的文件 可以看到终端输出了 “./new_log”，也就是说，这个文件是在当前目录。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之文件型相关命令]]></title>
    <url>%2F2019%2F08%2F26%2FLinux%E4%B9%8B%E6%96%87%E4%BB%B6%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言Linux系列文章的相关内容大多数都是一些我所记笔记的输出，同时也是对一些命令的心得，因此是纯技术文章。 话不多说，进入今天的主题！ 文件型相关命令pwd 命令pwd命令用于显示用户当前所处的工作目录，格式为“pwd [选项]”。 工作目录指的是用户当前在系统中所处的位置。 选项： -L 目录连接链接时，输出连接路径 -P 输出物理路径 cd 命令cd命令用于切换工作路径，格式为“cd [目录名称]”。 这个命令应该是最常用的一个Linux命令了。可以通过cd命令迅速、灵活地切换到不同的工作目录。 所谓的[目录名称] 还有两种路径区分： 绝对路径/相对路径： 绝对路径：必须以斜杆开始，绝对路径包括从文件系统的根节点开始要查找的对象（目录或文件），所必须遍历的每一个目录的名字，它是文件位置的完整路标，因此在任何情况下都可以使用绝对路径找到所需文件。 相对路径：不是以斜杆开始，相对路径可以包含从当前目录到要查找的对象（目录或文件）所必需遍历的每一个目录的名字。 实例演示： 在Linux系统中，.. 表示父级目录， . 表示当前目录（这一点和Windows一致） 于是 “cd ..” 就是返回到上层目录。 ls 命令ls命令用于显示目录中的文件信息，格式为“ls [选项] [文件] ”。所处的工作目录不同，当前工作目录下的文件肯定也不同。 使用ls命令的“-a”参数看到全部文件（包括隐藏文件），使用“-l”参数可以查看文件的属性、大小等详细信息。将这两个参数整合之后，再执行 “ls -al” 命令即可查看当前目录中的所有文件并输出这些文件的属性信息： 选项： cat 命令cat命令用于查看纯文本文件（当文件内容较少时），格式为“cat [选项] [文件]”。 如果在查看文本内容时还想顺便显示行号的话，在cat命令后面追加一个-n参数： 为什么建议当查看的文件内容较少时才使用 “cat” 呢？ 因为有更棒的命令啦~ 我们接着往下看！ more 命令more命令用于查看纯文本文件（内容较多的），格式为“more [选项]文件”。 more命令会在最下面使用百分比的形式来提示您已经阅读了多少内容。使用空格键或回车键向下翻页。（按q 退出） tail 命令tail 命令用于查看纯文本文档的后N行，格式为“tail [选项] [文件]”。 假设我们添加了新用户（普通用户UID从1000开始），所以查看新建的用户应该在文档最后方，为避免使用more命令然后一直翻一直翻的情况出现，这时候 tail 就派上用场了。 tail命令还可以持续刷新一个文件的内容，命令格式为“tail -f 文件名” 这个参数可以实现不停地读取某个文件的内容并显示。这可以让我们动态查看日志，达到实时监视的目的。 wc 命令wc命令用于统计指定文本的行数、字数、字节数，格式为“wc [选项] 文本”。 选项： 如果不输入选项，命令 “wc” 显示的内容 默认是 行数、单词数、字节数 diff 命令diff命令用于比较多个文本文件的差异，格式为“diff [参数] 文件”。 在使用diff命令时，不仅可以使用–brief参数来确认两个文件是否不同，还可以使用-c参数来详细比较出多个文件的差异之处，这绝对是判断文件是否被篡改的有力神器。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git入门指引]]></title>
    <url>%2F2019%2F08%2F24%2FGit%E5%85%A5%E9%97%A8%E6%8C%87%E5%BC%95%2F</url>
    <content type="text"><![CDATA[前言Git是目前最先进的版本控制系统，拥有最多的用户数量并管理着数量庞大的实际软件项目；之前在 Github 上慢慢摸索，逐渐了解 Git 这个工具的好处还有实用性。 这篇文章即将介绍相关概念和简单的Git用法。 版本控制工具所谓版本控制，就是对代码修改的记录。 我们知道，Github 上面的项目是开源的，意味着人人都可以对代码进行修改，我们一般把修改的代码提交到分支上面去，当项目的主人——master 看到你提交的代码时，他如果觉得此处修改是有作用的，就可以将分支与主线进行合并。 假设如果这时候，代码合并之后代码逻辑混乱，多个BUG爆出，而又不记得合并之前的代码是如何编写的，他就可以通过版本控制工具，进行版本回退。回到合并代码之前的样子，就避免了源码逻辑错误这种尴尬的事情发生。 还有，对于代码的修改，Git 可以实现每一次修改的记录。当修改多次之后，我们看着版本记录，就很清楚之前做过什么、修改了什么代码。 Git是什么Git 是一个版本控制软件。在进行软件开发时，一个团队的人靠使用Git，就能轻松管理好项目版本，做项目的追踪和辅助进度控制。Git 具有“集中式版本控制“和”分布式版本控制“的特点。 Git是“集中式版本控制“，将代码开源之后，人人都可以从 Git 上面获取代码仓库，就好比这一个团队中，版本库都集中在一台服务器上，每个开发者都要从服务器上获取最新的版本库后才能进行开发，开发完了再把新的版本提交回去。 同时，Git 又是”分布式版本控制“，则是这个团队中每个人的电脑上都会有一份完整的版本库，将源码Fork到本地之后，版本库就在你自己的电脑上。团队之间的成员各自修改了代码逻辑之后，只需把各自的修改推送给对方，就可以互相看到对方的修改了。（多人干活） Git 的使用在实际应用中，Git有非常多的用法，下面举一下简单的例子： 比如，在刚才建好的版本库中，我新建了README文件（项目说明文档）。写好后想给项目做个版本，就需要这样： 12$ git add README$ git commit -m &quot;add README&quot; 第一个命令是告诉 Git 要处理什么操作 第二个命令是进行提交，并对此次提交做个简答说明。然后 Git会自动为此次提交生成一个16进制的版本号。 如果此时查看本地的版本库，就会发现最新的一次提交是在刚才，提交说明为：add README。 分支的概念分支是版本控制里面的一个概念：在项目做大了之后，如果要在原基础上进行扩展开发，最好新建一个分支，以免影响原项目的正常维护，新的分支开发结束后再与原来的项目分支合并。 Git 常用命令：]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux之常用系统工作命令]]></title>
    <url>%2F2019%2F08%2F24%2FLinux%E4%B9%8B%E5%B8%B8%E7%94%A8%E7%B3%BB%E7%BB%9F%E5%B7%A5%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[前言最近开始了 Linux 的学习，打算将每天学习的东西记下来，将知识内化输出。这个系列估计会更新很久，敬请期待~ 我使用的是 Centos 7，是当今主流操作系统。Linux系统是一款优秀的软件产品，具有类似UNIX的程序界面，而且继承了UNIX的稳定性，能够较好地满足工作需求。 为什么学Linux？ 一部分是因为招聘需求大多数有要求是Linux开发；另一部分是因为…windows系统很那啥….嗯你懂的。 还有就是，我觉得使用命令行各种唰唰唰然后调兵遣将的样子很厉害~ 所以， Linux 我来啦！！ 命令的语法格式Linux的命令由三部分组成：“命令 [选项] [参数]” 命令：告诉 操作系统 执行的什么命令 选项：说明命令运行的方式（一般是以 “-”开始的） 参数：说明命令作用于什么（如一个文件、目录、文字等等） 记住这个公式，有利于理解一大串命令的作用。 还有就是，一般在命令最前头，是有用户提示符的，因为 Linux是多用户进行管理（就像多台电脑进行同一台电脑的使用），我们可以使用ctrl + alt +F[1~6] 进行切换用户视角。 那么，所谓的用户提示符是这么规定的： 普通用户提示符：$ root 用户提示户：# root 表示电脑最高权限者，也就是说，对于这台电脑来说，root账号的人就像上帝一般，无所不能。 查看帮助文档这个可谓是学习 Linux 开发人员人手必备的技能了，因为命令那么多，如果不常用，那么就会遗忘。这时候，我们就可以翻阅命令的相关帮助文档，唤起我们的回忆。 查看帮助文档有两个常用方法： ① –help： 例如 free，你如果不知道怎么使用，或者后面携带的选项，那么就使用 “–help” ② man： 一般来讲，使用man命令查看到的帮助内容信息都会很长很多。导致了终端输出的内容无法全部一次性显示出来，那么就涉及到了翻页等技巧，我们看看 man 所包含的常用操作按键及其用途： 使用了 man 命令之后，你就会进入一个类似说明文档的界面中，下面对man命令的帮助信息的结构进行说明： 在学会了如果使用命令之后呢，我们进入这篇文章的主题：常用系统工作的命令 常用系统工作命令echo 命令echo命令用于在终端输出字符串或变量提取后的值，格式为“echo [字符串 | $变量]”。（相当于打印出来） 学过编程的同学，会很容易理解这个命令的作用。就像 python 中的 print 函数嘛， data命令date 命令用于显示及设置系统的时间或日期，格式为“date [选项] [+指定的格式]”。 date 还能自定义时间格式，输入以“+”号开头的参数，即可按照指定格式来输出系统的时间或日期，这样在日常工作时便可以把备份数据的命令与指定格式输出的时间信息结合到一起。（比较常用） 具体使用： 这样一来，我们只需要看一眼文件名称就能大概了解到每个文件的备份时间了。 date 参数说明： 用户相关命令① reboot命令 reboot 命令用于重启系统 ②poweroff命令 oweroff 命令用于关闭系统 该命令与reboot命令相同，会涉及硬件资源的管理权限，因此默认只有root管理员才可以关闭电脑 ③ shutdown命令 用来关机或重启的命令，取决于后面的参数： shutdown -r [now] 重启系统； shutdown -h [now] 关机 shutdown命令的工作原理为：shutdown命令会发送请求给系统的 init 进程将系统调整为合适的运行级别 0 表示关机；6表示重启 ④ logout / exit 命令 用于退出当前用户登陆状态。 ⑤ 用户查看、用户切换 命令 作用 whoami 查看当前正在操作的用户 who 查看当前登陆的用户 su - [usename] 用于直接切换用户 wget命令wget命令用于在终端中下载网络文件，格式为“wget [参数] 下载地址”。 用法举例： 1wget http://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz 这样就通过 wget + [URL能访问到的资源]，实现了本地下载功能。 ps 命令ps命令用于查看系统中的进程状态，格式为“ps [参数]”。有时候，一些服务卡住了，我们可以通过ps 命令进行查看该服务的状态，然后进行下一步的处理。 使用公式： ps -aux Linux系统中时刻运行着许多进程，如果能够合理地管理它们，则可以优化系统的性能。在Linux系统中，有5种常见的进程状态，分别为运行、中断、不可中断、僵死与停止。 R（运行）：进程正在运行或在运行队列中等待。 S（中断）：进程处于休眠中，当某个条件形成后或者接收到信号时，则脱离该 状态。 D（不可中断）：进程不响应系统异步信号，即便用kill命令也不能将其中断。 Z（僵死）：进程已经终止，但进程描述符依然存在, 直到父进程调用wait4()系统函数后将进程释放。 T（停止）：进程收到停止信号后停止运行。 Top命令top命令用于动态地监视进程活动与系统负载等信息。 我们可以将它看作Linux中的“强化版的Windows任务管理器”。 第1行：关于系统的相关信息 第2行：进程的状态，分别对应上面我们说的五个状态，通过数量进行标识 第3行：CPU的使用状况 第4行：内存的使用状况 第5行：虚拟内存的使用状况 pidof 命令pidof命令用于查询某个指定服务进程的PID值，格式为“pidof [参数] [服务名称]”。 每个进程的进程号码值（PID）是唯一的，因此可以通过PID来区分不同的进程。 命令效果 即可得出该服务的进程id（这时候的火狐对应着81904进程号） kill 命令kill命令用于终止某个指定PID的服务进程，格式为“kill [参数] [进程PID]” 命令效果 输入 kill 命令之后，前台运行着的火狐浏览器瞬间就被终结掉了 killall 命令killall 命令用于终止某个指定名称的服务所对应的全部进程，格式为：“killall [参数] [服务名称]” 。 通常来讲，复杂软件的服务程序会有多个进程协同为用户提供服务，如果逐个去结束这些进程会比较麻烦，此时可以使用killall命令来批量结束某个服务程序带有的全部进程，可以将叫做[进程名]的进程全部杀死（不精准） 另一种关闭进程的方法： 如果我们在系统终端中执行一个命令后想立即停止它，可以同时按下Ctrl + C组合键，这样也会立即终止当前命令的进程。 对于 kill 和 killall 的使用建议： 如果知道进程id 使用 kill （这样终结的进程比较精准） 如果知道服务名称，可以使用 killall （有时候会“误伤”，慎用） 写在最后这第一篇关于Linux操作系统的文章就到此结束啦，涉及到知识点还是挺多的，希望读者能好好吸收消化。 对于Linux 命令来说，用熟悉了，自然就记住了，不需要刻意去背的，费时费力的一种行为。 旁友们，下回见~Bye!]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day17-解析库Xpath]]></title>
    <url>%2F2019%2F08%2F22%2F%E7%88%AC%E8%99%ABDay17-%E8%A7%A3%E6%9E%90%E5%BA%93Xpath%2F</url>
    <content type="text"><![CDATA[前言我们在之前介绍如何解析响应数据时讲到了 BeautifulSoup 这个解析库，传送门： 爬虫Day6-Beautiful介绍 但学习之后我们发现，BeautifulSoup 是依赖解析器的，在实际开发环境中，我们常常遇到一些特殊情况，例如编码格式导致解析时发现页面数据缺失等等情景。 这时候，除了更换解析器这个办法之外，我们还可以使用其他的解析库，例如 Xpath。 Xpath介绍XPath，全称 XML Path Language，即 XML 路径语言，它是一门在XML文档中查找信息的语言（XML也是一种标签化语言）。XPath 最初设计是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索。 Python 标准库中自带了 xml 模块，但是性能不够好，而且缺乏一些人性化的 API，相比之下，第三方库 lxml 是用Cython 实现的，而且增加了很多实用的功能，可谓爬虫处理网页数据的一件利器。lxml 大部分功能都存在lxml.etree中。 Xpath 常用规则 每一条 / 都表示一层嵌套关系，我们需要对HTML结构进行一定的了解，使用Xpath 才更加得心应手 // 匹配的节点好比喻成文件系统的绝对路径，如果有相符的节点名称，那么就会被匹配到 @ 这个符号选取属性，我们一般用来获取 URL ，例如：@href 下面列出用法举例： LXML 库 的使用读入HTML 文本1234567891011121314151617from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''html = etree.HTML(text) # 初始化生成一个XPath解析对象result = etree.tostring(html)print(type(result)) # bytes类型print(result.decode('utf-8')) # 将二进制内容解码成str类型的字符串 传入 HTML 文本，会自动修正（补齐缺漏的节点标签）且生成一个XPath解析对象，后续的解析都是根据解析对象来调用xpath方法进行节点选择 由于初始化 HTML 文本返回的结果是bytes类型，我们打印出来时，需要转为utf-8。 读入HTML 纯文件1234567from lxml import etreehtml = etree.parse('test.html', etree.HTMLParser())result = etree.tostring(html) # 解析成字节# etree.tolist（html） 解析成列表print(result.decode('utf-8')) test.html是我们创建的html文件，里面存放一些html文本 跟直接读取html文本不同的是，文件读取会多出 DOCTYPE 的声明，但是对内容解析没有影响 tostring 方法 可以实现 将 内容结构化打印出来（比较直观） xpath 选择节点关于怎么选择节点，除了上面讲的 xpath 常用规则之外，还有一些关于 属性匹配、属性获取的使用方法。 123456789101112131415161718192021222324252627from lxml import etreetext = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;&lt;a href="link1.html"&gt;first item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-inactive"&gt;&lt;a href="link3.html"&gt;third item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt; &lt;/ul&gt; &lt;/div&gt;'''# //代表获取子孙节点，*代表获取所有result = html.xpath('//*') # 指定节点名称，比如要获取所有li节点result2 = html.xpath('//li')# 指定li标签下的直接子节点aresult3 = html.xpath('//li/a') # @符号的过滤 选取 class 为 item-1的li节点result5 = html.xpath('//li[@class="item-1"]')# 获取指定节点父节点的类值result4 = html.xpath('//a[@href="link4.html"]/../@class') 属性获取：@href 即可获取节点的 href 属性 属性匹配：使用中括号，@属性名 = 值 的方式来限定某个属性 获取文本用 XPath 中的 text() 方法可以获取节点中的文本 12result6 = html.xpath('//li[@class ="item-0"]/a/text()') #获取a节点下的内容result7 = html.xpath('//li[@class ="item-0"]//text()') #获取li下所有子孙节点的内容 这里要注意，text（）方法要结合着前面的”/“或“//” 标签看，如果是 “/” 的话，就输出当前子节点的文本；如果是 “//” 的话，就输出当前节点的所有子孙节点的文本 模糊查询如果 HTML 文本中的 li 节点的 class 属性有两个值 ，例如 “class =li li-first” 遇到这种情况，我们可以用contains()函数或者将多个值写全，才能匹配到该节点。 多属性匹配如果需要根据多个属性才能确定一个节点，这是就需要同时匹配多个属性才可以，那么这里可以使用运算符 and 来连接（xml还支持其他运算符） and 表示 “与” 的关系，只有同时满足两个属性匹配表达式，该节点才会被选中。 按序选择有时候我们在选择的时候可能某些属性同时匹配了多个节点，但是我们只想要其中的某个节点，如第二个节点，或者最后一个节点，这时该怎么办呢？ 这时可以利用中括号传入索引的方法获取特定次序的节点 类似于列表的切片，不过需要注意： 这里的切片的索引是从1开始的 支持last、position等函数 还支持+-&lt;&gt;的推算 写在最后这一篇还是干货满满的，哈哈哈，慢慢吸收吧~ 关于节点选择的编写，写熟了自然就会了。 下回见，peace~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day16-文件数据存储]]></title>
    <url>%2F2019%2F08%2F21%2F%E7%88%AC%E8%99%ABDay16-%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[前言关于数据存储，我们之前已经讲了数据库的存储： 爬虫Day7-Mysql的那些事 ，还有 爬虫Day13-Mongodb存储 但是对于一些小型的文件，我们可以使用多种格式类型的文件保存到本地，相对于数据库来说，比较便捷。今天介绍的是有关文件数据存储的相关内容。 文本存储将数据保存到 TXT 文本的操作非常简单，而且 TXT 文本几乎兼容任何平台，但是有个缺点就是不利于检索，所以如果对检索和数据结构要求不高，追求方便第一的话，可以采用 TXT 文本存储。 12345678910111213141516171819202122import requestsfrom pyquery import PyQuery as pqurl = 'http://www.zhihu.com/explore'headers = &#123; 'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:67.0) Gecko/20100101 Firefox/67.0'&#125;html = requests.get(url,headers=headers).text # html纯文本内容doc = pq(html)items = doc('.explore-tab .feed-item').items()# 所有的回答内容都包含在此div中，我们使用items（）将其变成一个迭代器for item in items: # for 遍历这个迭代器 question = item.find('h2').text() # 提问的内容 author = item.find('.author-link-line').text() # 答主 answer = pq(item.find('.content').html()).text() # 将回答的内容提取 file = open('explore2.txt', 'a', encoding='utf-8') # 创建文件句柄 file.write('\n'.join([question,author,answer])) # 将各部分内容依次写入文件 file.write('\n'+'='*50+'\n') # 分割每一个回答 file.close() 我们在上面做了以下事情： 请求网页（设置headers头部） 发出get请求，获得网页响应内容 观察网页内容，调用解析库，传入获取的html文本，生成doc对象 使用选择器进行内容匹配 写入文件（优化数据） 需要注意的是，构造文件句柄的时候，需要注意编码问题，同时，文件的处理类型也要指明（默认是“r”） 媒体文件存储存储媒体文件的两种方式： 只获取文件 URL 链接 直接把源文件下载下来 对于爬虫来说，如果我们要爬取的东西很多，那么建议使用第一种方式。使爬虫运行得更快，耗费的流量更少，因为只要链接，不需要下载文件。 那么，如果通过URL链接，下载我们想要的媒体文件呢？ urllib的urlretrieve方法 在python3 中， urllib.request.urlretrieve 可以根据文件的 URL 下载文件： 12345678910from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com')bsObj = BeautifulSoup(html, 'lxml')imageLocation = bsObj.find("a", &#123;"id": "logo"&#125;).find("img")["src"] # 获取一张图片的URL路径result = urlretrieve(imageLocation, "logo.jpg")print(result) urlretrieve方法传入两个参数： 下载的URL链接，以及所保存的文件名 注意文件后缀名要和保存的数据相符合，例如 图片是jpg格式，那么我们传入的文件名后缀也要是 jpg 格式 Json 文件存储Json，全称为 JavaScript Object Notation, 也就是 JavaScript 对象标记，通过对象和数组的组合来表示数据，构造简洁但是结构化程度非常高，它是一种轻量级的数据交换格式。 Python 为我们提供了简单易用的 json 库来供我们实现 Json 文件的读写操作，我们可以调用 json 库的 loads() 方法将已编码的 JSON 字符串解码为 Python 对象，可以通过 dumps()方法将 Python 对象编码成 JSON 字符串。 值得注意的是 Json 的数据需要用双引号来包围，不能使用单引号。 并且，如果存储的数据中包含中文，我们需要指定一个参数 ensure_ascii 为 False，另外规定文件输出的编码（不指定文件输出的编码会乱码） 读取Json 读取：使用 loads（） 方法，将Json对象解码为 Python 对象 存储Json 调用 dumps() 方法将 Python 对象编码成 JSON 字符串。 CSV文件存储CSV，全称叫做 Comma-Separated Values，中文可以叫做逗号分隔值或字符分隔值。通俗的讲，他就是就是以特定字符分隔的纯文本，我们可以使用 Excel 打开 .csv的文件。单调、简朴是这种文件类型的风格。 CSV的写入 首先打开了一个 data.csv 文件，然后指定了打开的模式为 w，即写入，获得文件句柄。 随后调用 csv 库的 writer() 方法初始化一个写入对象，传入该文件句柄，然后调用 writerow() 方法传入每行的数据即可完成写入。 第一行通常是存储字段的字段名，这起到标识字段值的作用。 文件打开效果如下： CSV的读取 关于如何打开.CSV后缀的文件，并不是在 open()返回的 File 对象上调用 read()或 readlines()方法，而是将 File 对象传递给 csv.reader()函数。这将返回一个 Reader对象。我们将基于此对象进行文件读取。 同时，我们可以使用 for 循环遍历出文件内容。 线上CSV Python 的 csv 库主要是面向本地文件，就是说你的 CSV 文件得存储在你的电脑上。而我们进行网络数据采集的时候，很多文件都是在线的。不过有几种方法可以解决这个问题： 手动把 CSV 文件下载到本机，然后用 Python 定位文件位置； 写 Python 程序下载文件，读取之后再把源文件删除； 从网上直接把文件读成一个字符串，然后转换成一个 StringIO 对象，使它具有文件的属性。 虽然前两个方法也可以用，但是既然你可以轻易地把 CSV 文件保存在内存里，就不要再下载到本地占硬盘空间了。可以直接把文件读成字符串，然后封装成*StringIO对象，让Python把它当作文件来处理，就不需要先保存成文件了。 代码实现： 跟读取本地CSV 文件步骤相似，只是多了使用 StringIO( )方法将在线的CSV文件转为StringIO对象。 这样就实现了线上 CSV 的文件读取，是不是很方便！ 写在最后ok了，这一篇我们介绍了关于文件存储的大部分流行技术，快快用起来吧~ 朋友们，下回见啦。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day15-识别图形验证码]]></title>
    <url>%2F2019%2F08%2F19%2F%E7%88%AC%E8%99%ABDay15-%E8%AF%86%E5%88%AB%E5%9B%BE%E5%BD%A2%E9%AA%8C%E8%AF%81%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言上一篇的模拟登陆 爬虫Day14-模拟登陆Github 提到，具体的反爬手段还有验证码，而验证码又分为很多类：图形验证码、极验滑动验证码、点触验证码等等。对于简单的验证码，我们可以利用脚本识别，如果是比较复杂的验证码，建议外接到打码平台，省时省力。 今天我们要讲的是图形验证码。 环境准备 识别图形验证码需要库 tesseract 和 Pillow。请自行百度安装。 Pillow 是从 Python 图像库（Python Imaging Library，PIL）分出来的；而关于tesseract，在python中它的库为tesserocr。 有人分不清为什么安装了软件还要安装模块，其实是这样的：在python 中的库相当于软件的包装器，可以帮助我们调用软件。 tesseract 的使用我们先保存一张验证码到本地，用来测试使用。名为 code.jpg 如下图： 第一种方法： 12import tesserocrprint(tesserocr.file_to_text('code.jpg')) 通过两行代码我们就将验证码识别出来，是不是很棒！ 第二种方法： 123456import tesserocrfrom PIL import Imageimage = Image.open('code.jpg') # 使用open方法打开验证码的图，获得一个image对象result = tesserocr.image_to_text(image) # 调用 image_to_text（）方法将图片转字符串print(result) 运行结果如下： 可以看到多出来个空白格，对于一些复杂的线条，tesseroct库会识别失效，这个我们后面会说。 并且，第二种方法是基于打开图片后的image对象进行操作的，而且识别效果更高。建议采用第二种写法噢~ 转灰度、二值化上面的方法虽然快捷、但是一旦遇到有复杂线条干扰的验证码，识别准确度就会大大下降。我们可以通过转灰度、二值化进行加工处理，让图片变得更容易电脑识别。 先了解一下这个convert 方法： convert（）：将当前图像转换为其他模式，并且返回新的图像。 代码实现： 12345678910111213141516171819import tesserocrfrom PIL import Imageimage = Image.open('code.jpg')image = image.convert('L')threshold = 127 # 二值化阈值 数值越低接近白，越高越黑table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')image.show()result = tesserocr.image_to_text(image)print(result) 利用Image对象的convert（）方法参数传入’L’，即可将图片转化为灰度图像 传入数字”1” 即对图片进行二值化处理，convert()方法采用的是默认阈值为127。不过我们不能直接转化原图，要将原图先转为灰度图像，然后再指定二值化阔值 threshold 变量存储的二值化阈值的规律是：数值越低接近白，越高越黑（并且是从左到右渲染的） images.point方法：返回给定查找表对应的图像像素值的拷贝。变量table为图像的每个通道设置256个值。 它会为输出图像指定一个新的模式（mode指定的模式） 转灰度、二值化的变化过程： 可以看到，一些复杂的线条就被我们清理掉了，剩下白色背景、黑色字体的图片（tesseroct最喜欢这个了） 运行结果： 经过后期处理之后，现在的识别准确度大大提高，对于此张验证码，我们做到了完美识别！ 写在后面验证码绝对是反爬工程师手中的一把利器，可以阻挡住多数低级爬虫。 这篇只是介绍了下图形验证码，对于验证码，还需要其他方面知识的铺垫、例如机器学习、一定的数学基础等等。 当你学会识别大多数的验证码时，就可以自信的去敲BAT级别公司的门了。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day14-模拟登陆Github]]></title>
    <url>%2F2019%2F08%2F15%2F%E7%88%AC%E8%99%ABDay14-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86Github%2F</url>
    <content type="text"><![CDATA[前言爬虫中比较重要的一环，当然是模拟登陆了，期间可能会遇到验证码等反爬行为，今天要讲的模拟登陆Github，是比较简单的类型：构建请求参数。但这也是最为常见的登陆形式。 我们知道，要想登陆到站点，肯定交互了账号密码、Cookies值的设定等等 为了安全性考虑，大多数请求形式使用POST方式进行交互，账号信息包含在一个From Data 的表单中，服务器拿到了信息之后，会到数据库进行比对，是否有此人的个人信息，如果有，就登陆成功。 模拟登陆首先要分析登录的过程，需要探究后台的登录请求是怎样发送的，登录之后又有怎样的处理过程。 如果已经登录 GitHub，先退出登录，同时清除 Cookies。 打开 GitHub 的登录页面，链接为 https://github.com/login 。 然后输入GitHub 的用户名和密码，打开开发者工具，将 Preserve Log 选项勾选上，这表示显示持续日志（preserve log的作用是，保持下面的所有的网络请求不被冲掉。因为当网页刷新的时候，如果没有选中这个按钮，之前的访问url记录会被冲掉。） 如下图所示： 点击登陆之后 发现交互了一些信息，我们查看session请求响应条目，它是个POST 请求信息（就是它了！） 状态码是302，为重定向，说明我们登陆的时候，经过了一个页面。 往下拉，查看Request Headers 和 From Data 信息： 发现有个from Data ，这对应 POST请求发送的Data表单 。 里面字段信息有 login、password （账号密码），另外还有utf8、commit、authenticity_token 这些其他字段。 有些字段可以直接构建，一些字段参数不知道其含义，只能一个一个测试。比如这里的 authenticity_token 通过观察，我们发现在登录之前我们会访问到一个登录页面，此页面是通过 GET 形式访问的。输入用户名密码，点击登录按钮，浏览器发送这两部分信息。 也就是说 Cookies 和 authenticity_token 是在访问登录页的时候设置的。 一些临时变量的参数，肯定是在登陆之前附加的参数（这是个先有鸡后有蛋的故事） token 是类似令牌的东西，这种令牌要么是用户注册的时候分配给用户，要么就是在用户调用的时候才提供，可能是长期固定的值，也可能是频繁变化的，通过服务器对用户名和密码的组合处理后生成。 我们应该弄懂 token 是如何生成的，才能完成表单提交。 于是我们再次退出登陆，清除Cookies，寻找Cookie、 authenticity_token 参数是从哪来的。 发现 交互的报文中，Response Headers 有一个 Set-Cookie 字段。这就是设置 Cookies 的过程。 然后在网页的源码探寻，搜索相关字段，发现源代码里面隐藏着 authenticity_token信息，它是一个隐藏式表单元素。 那么我们就拿到了POST的请求参数了。 可以进行模拟登陆了~ 代码示例：12345678910111213141516171819202122232425262728293031323334353637import requestsfrom lxml import etreeclass Login: def __init__(self): self.headers = &#123; 'host': 'github.com', 'Referer': 'https://github.com/login', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; self.login_url = 'https://github.com/login' # 登陆之前的URL self.post_url = 'https://github.com/session' # 发送POST请求的URL self.session = requests.Session() # 由Session实例化的对象，Cookies会自动保存 def back_tokenI(self): response = self.session.get(self.login_url, headers=self.headers) selector = etree.HTML(response.text) token = selector.xpath('//div//input[2]/@value')[0] # 选中 authenticity_token 参数的值 return token def login(self, username, password): data = &#123; 'commit': 'Sign in', 'authenticity_token': self.back_tokenI(), 'utf8': '✓', 'login': username, 'password': password &#125; response = self.session.post(url=self.post_url, headers=self.headers, data=data) if response.status_code == 200: print('模拟请求成功了！！！') return response.text # 返回登陆成功之后的数据c1 = Login()c1.login('username','password') 注意我们这里使用的Session（）实例化的对象发送请求的，这个方法构建的对象，在访问页面的时候，会自动帮我们保存 Cookies值 。这样我们就不用特定设置Cookies 参数了。 写在最后其实这种携带参数请求的模拟登陆并不复杂，主要是它携带的参数有多个，你需要一个个去试，没有其他规律可循。当然，有时候会碰到 salt 加密或CSS反爬，这个就需要其他方面的知识点了。 这篇Github模拟登陆就说到这里啦，下期见，peace~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day7-事件驱动模型]]></title>
    <url>%2F2019%2F08%2F15%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay7-%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[事件驱动性思想事件驱动模型不是什么具体的东西，而是一种“思想”。事件驱动编程是一种编程范式，这里程序的执行流由外部事件来决定。它的特点是包含一个事件循环，当外部事件发生时使用回调机制来触发相应的处理。 关于编程范式，在这里要说说传统编程和事件驱动型编程的区别： 传统编程传统的编程是如下线性模式的： 1开始---&gt;代码块A---&gt;代码块B---&gt;代码块C......---&gt;结束 事件驱动型 对于事件驱动型程序模型，它的流程大致如下： 1开始---&gt;初始化---&gt;等待信号---&gt;特定信号---&gt;相应处理 我们看到了事件驱动型和传统编程的编程风格是完全不同的 事件驱动程序在启动之后，就在那等待，等待什么呢？等待被事件触发。传统编程下也有“等待”的时候，例如：input()，需要用户输入交互数据。但这种等待是有目的性的：期待着用户某些行为，或输入一个特定的字符串、数字等等。如果用户输入不规范，还需要提醒他，并做处理。 而事件驱动程序的等待则是没有目的性的，也不强制用户输入或者干什么。只要某一事件发生，那程序就会做出相应的“反应”。这些事件包括：输入信息、鼠标、敲击键盘上某个键还有系统内部定时器触发。 事件驱动模型介绍通常，我们写服务器处理模型的程序时，有以下几种模型： 每收到一个请求，创建一个新的进程，来处理该请求； 每收到一个请求，创建一个新的线程，来处理该请求； 每收到一个请求，放入一个事件列表，让主进程通过非阻塞I/O方式来处理请求 第一种第二种都挺消耗资源的，第三种就是协程、事件驱动的方式，一般普遍认为第3种方式是大多数网络服务器采用的方式。 下图就是典型的事件驱动模型： onclick 绑定着 一个 fun（）函数，当事件被触发时（这里是被点击），相应的处理模块，就会运行。 有了这种思想，我们就可以监测用户行为，从而发送响应内容了。 那么，如何去检测用户什么时候点击、点的哪里呢？ 检测点击的方法①创建一个线程循环 这个方式有以下几个缺点： CPU资源浪费，可能鼠标点击的频率非常小，但是扫描线程还是会一直循环检测，这会造成很多的CPU资源浪费 如果扫描鼠标点击的接口是阻塞的，又会出现下面这样的问题，我们无法检测键盘是否被按下，因为扫描鼠标时被堵塞了，那么可能永远不会去扫描键盘；（此时，一个线程无法解决问题） 如果一个循环需要扫描的设备非常多，这又会引来响应时间的问题 ②事件驱动模型思想 目前大部分的UI编程都是事件驱动模型，如很多UI平台都会提供onClick()事件，这个事件就代表鼠标按下事件。事件驱动模型大体思路如下： 有一个事件（消息）队列； 鼠标按下时，往这个队列中增加一个点击事件（消息）； 有个循环，不断从队列取出事件，根据不同的事件，调用不同的函数，如onClick()、onKeyDown()等； 事件（消息）一般都各自保存各自的处理函数指针，这样，每个消息都有独立的处理函数； 模型如图所示： okk，网络编程系列更新到此结束啦，跟计算机网络系列一样，都是 day7，挺巧的哈哈哈 以后新增的更新系列应该是操作系统方面吧（Linux），当然，爬虫系列是不会断更的。byebye~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day6-队列]]></title>
    <url>%2F2019%2F08%2F14%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay6-%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[前言上此我们讲到了线程： 网络编程Day3-子线程 ，那么多线程如何应用呢？ 就是我们今天要讲的主题了——队列。 队列，是用来解决线程安全的一种“利器”，讲述队列之前，我们需要对一个模型进行了解。 生产者消费者模型生产者消费者模式是指通过一个容器来解决生产者和消费者的强耦合问题。 耦合： 用python代码来做比喻，如果更改一处代码，其他代码会随之受到影响的情景，我们就称之为耦合。 生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这是一个解耦的过程。 PS：python 的协程，就涉及到了生产者消费者模型的相关知识。协程，又称微线程，纤程。英文名Coroutine。协程的作用，是在执行函数A时，可以随时中断，去执行函数B，然后中断继续执行函数A（可以自由切换）。但这一过程并不是函数调用（没有调用语句），这一整个过程看似像多线程，然而协程只有一个线程执行。(因此也被较为伪多线程) 队列队列是用于线程安全的，提供了一个适用于多线程编程的数据结构。queue是python标准库中的线程安全的队列，默认基本FIFO队列（FIFO即First in First Out,先进先出） Python Queue模块有三种队列及构造函数（返回一个数据容器）: Python Queue模块的FIFO队列先进先出。 LIFO类似于堆，即先进后出。 queue.LifoQueue(maxsize) 还有一种是优先级队列级别越低越先出来。 queue.PriorityQueue(maxsize) 队列的方法q.put()调用队列对象的put()方法在队尾插入一个项目。put()有两个参数，第一个item为必需的，为插入项目的值；第二个block为可选参数，默认为True。如果队列当前为空且block为True，put()方法就使调用线程暂停,直到空出一个数据单元（等待其他线程做get操作）。如果block为False，put方法将引发Full异常（设置当队列塞不进数据时直接报错）。 q.get()调用队列对象的get()方法从队头删除并返回一个项目。可选参数为block，默认为True。如果队列为空且block为True， get()就使调用线程暂停，直至有项目可用。如果队列为空且block为False，队列将引发Empty异常。 block 参数的作用：设想一下，如果一个线程运行着 put 或 get 方法，当队列为空时，是不是一直保持着，等待状态，这跟我们之前讲过的 socket 一样，类似于它的 recv（一直等待消息），这时候，就处于阻塞状态。 put和get同样可以卡住，也是依赖于同步状态进行操作的。所以我们可以通过设置block值引发错误，然后通过异常捕获来避免这种情况发生。 其他方法 q.task_done()： 在每一次完成put或get时，就会发送一个信号 q.join（）： 用于接收信号，只有全接收到信号时，才会执行下一步操作 这两者的配合就达到了同步效果 队列的使用假设现在有一个需求：开两个线程来删除某数据对象，这里我们使用一个列表来模拟。 运行之后发现报错了：原因很简单，remove是按值删除的，线程1删除掉的数据，列表已经没有这个值了，线程2想要删除这个值时，就报错。那么这个问题就是数据不安全导致的。 报错信息如下： 根据我们学到过的知识点，有这么两个解决方法： 方法一：加锁 加了锁之后，虽然能够解决问题，但对于这种 CPU密集型的任务，其并不能完美胜任这个工作。 方法二：使用队列123456789101112131415161718192021222324252627import queueimport threadingimport timethe_list = [1,2,3,4,5]q = queue.Queue()def dl(): while len(the_list) &gt;0: data = q.get() # 从队列中拿数据 q.task_done() # 发送信号，表明已拿到数据 the_list.remove(data) # 接着将拿到的数据从列表中删除 print("%s 已经被删除了"%data)def d2(): while len(the_list) &gt;0: a = the_list[-1] # 从列表中取出最后一个数据 q.put(a) # 将数据放入队列中 q.join() # 等待另一个线程的信号t1 =threading.Thread(target=dl)t2 =threading.Thread(target=d2)t1.start()t2.start() 我们使用一个线程循环来完成这个需求，循环的条件时：列表不为空 然后一个子线程不断的放、另一个子线程不断的取。最终达到删除列表数据的效果 通过队列来完成解耦操作，保证了数据的安全性 1队列的相关知识就讲到这儿了~ 下期见，各位。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发者的思维]]></title>
    <url>%2F2019%2F08%2F14%2F%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%84%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[前言之前我们提及过，一名计算机学生应有的必修课 其中提及到了： 努力、视野、职业规划 但这远远不够，怎么去努力？所谓的计算机思维是什么？ 这是今天我们要说的主题。 如何学习对于编程学习者，我决定最应该普及的，不是什么理论知识，而是思维的蜕变，这个至关重要。昨晚听了一个知乎大佬的live之后，我收益匪浅，决定将一些心得写下来。 那么对于思维的转变是这样的：不要把自己当学生，而是要把自己当开发者。 学生们刚从高中的学习进入到大学，潜意识的沿用了高中的学习方法，这很不利于计算机的学习 如果你懂计算机思维的话，就知道抱着书照啃是多么糟糕的一种事情，有些同学，就想着一本书从头到尾理解透彻，其实计算机的理论知识的递归性的东西，而不是线性的。高中的知识，学校已经为我们安排好了每一章，每一个知识点。而对于计算机，没有一种固定的学习路线，导致了学生们前期迷茫的感觉。 记笔记笔记的重要性不言而喻，当你学完一定的知识点之后，大部分所内化的东西就是你的笔记内容。但是，记笔记也是有方法可循的，不是我夸大，大部分人连技术型笔记都不会写。 我赞成使用电脑记笔记，因为纸质版并不方便，还有一点就是：你确定你记在书上的东西，会再去翻吗 下图是我记笔记的方式 大脑是有遗忘机制的，如果想着把所有东西都弄懂并完整的记下来，几乎是不可能的。 那么这时候，笔记就成了你的第三方库，你随时可以上面查阅你已经弄懂的内容，开发者的工作并不是考试，你完全可以利用网上的资源，只要你找得到。 当你学习一个东西的时候, 如果学完马上用语言讲给别人听, 你会学的更好, 而且会发现新问题，还有没发现过的知识点。 费曼学习理论都指出, 知识的学习, 输入固然重要, 输出更为重要. 几乎所有优秀的程序员, 都有攥写技术文章的习惯, 很多时候, 并不是他们什么都懂, 而是他们刚学会了什么, 然后就围绕着这个刚学会的东西, 用自己的语言讲出来, 久而久之, 就会被别人觉得是大牛, 但是他和你的区别, 有可能仅仅是是否输出的区别. 我们可以在笔记上写平时学习的东西，然后再使用博客之类的东西，将笔记的内容用自己的话写出来，这样一来，就是输入以及输出了。 1除了会记笔记之外，思维层面的东西，更为重要 开发者思维开发者思维是计算机思维典型的一种。 开发者思维是指, 从学习编程第一天起, 你的目标, 你所做的事, 永远不是以要学会XX为目标, 而是以开发出XX为目标, 深刻意识到你学习的一切, 最终是为了你的开发而服务的。 具有开发者思维的人，会这么学习： 先确定好要学习的理论知识，比如python，然后大体了解一下python语言在哪些领域比较强势，在网上搜索一番之后，他会下定一个我要做出 xx 的项目这样的目标。 目的性很强的人，学习能力往往也很强。 与开发者思维相反的是：学生思维 学生思维读教材, 听课, 记笔记, 追求把这个语言的每个知识点都记得很清楚, 追求一种”内功”的修炼, 在这个过程中, 从来不想着用它去”创造”什么，比如在学习的第一天起，给自己定下的目标是这样的： “我这个学期一定要把这个语言的基础打牢，并且为以后的学习提供更坚实的基础.” NO！！！ 这就是为什么很多学生虽然在校学习成绩很好，但毕业后找不到工作的根本原因。 拥有学生思维，总是会陷入焦虑：“我下一阶段应该学什么、学的东西以后忘掉了怎么办、学这些东西有用吗？”等等这些思想，然后学习曲线非常平缓，甚至待在舒适区久了，你就无法跨越自己了。 这两种思维所导致的结果往往是, 后者无论是知识熟练度还是实用性都会超过前者, 而且整个学习过程会有源源不断地动力。 如果你具备开发者精神, 你开发出的东西, 你做出的产品, 它就是永久存在在这个世界上的, 你的成就感来源于真实的, 具体的, 可持久延续的项目中, 而不是来源于”我学会了什么”. 一切不谈成就感, 不谈反馈的学习, 都是空谈。 一份优秀的简历当你面聘工作时，好的简历上面写着的，应该是 “用xxx语言开发出xxx的项目”，而不是苍白的“精通xx语言” 如果你有记博客的习惯，当你面聘时，拿给考官一个博客 或 github 链接，一目了然。 这就是开发者思维带来的，最直接的影响。 写在最后以上就是我的一些心得体会，在以后的日子里，望君共勉！ 这是最好的时代，这是最坏的时代，我们一无所有，我们巍然矗立]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day13-Mongodb存储]]></title>
    <url>%2F2019%2F08%2F13%2F%E7%88%AC%E8%99%ABDay13-Mongodb%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[前言之前我们讲过关于 mysql 的数据存储：爬虫Day7-Mysql的那些事 对于爬虫的数据存储来说，一条数据可能存在某些字段提取失败而缺失的情况，而且数据可能随时调整，另外数据之间能还存在嵌套关系。如果我们使用了关系型数据库存储，一是需要提前建表，二是如果存在数据嵌套关系的话需要进行序列化操作才可以存储，比较不方便。如果用了非关系数据库就可以避免一些麻烦，简单高效。 MongoDB 是一个基于分布式文件存储的开源数据库系统，其内容存储形式类似 Json 对象，它的字段值可以包含其他文档，数组及文档数组，非常灵活。在python中使用pyMongo模块进行交互。 Mongodb基础名词比较如果之前对 Mysql 有所了解，那么在 Mongodb中呢，对一些概念性东西就能一通百通了。下图是这两者的名词比较 在 Mongodb中，区别比较大的就是，他的表名词改为 collection （集合），且 所存的数据称为 document（文档） Document文档是一组键值(key-value)对(即 JSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点 集合集合就是 MongoDB 文档组，集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 pyMongo模块在对Mongodb有了初步的了解之后呢，我们就可以使用python 与 mongodb进行交互，进行操作了。 三步曲 连接 MongoDB 我们需要使用 PyMongo 库里面的 MongoClient，一般来说传入 MongoDB 的 IP 及端口即可，第一个参数为地址 host，第二个参数为端口 port，端口如果不传默认是 27017。 MongoDB 中还分为一个个数据库，我们接下来的一步就是指定要操作哪个数据库 MongoDB 的每个数据库又包含了许多集合 Collection，也就类似与关系型数据库中的表，我们需要指定要操作的集合 指定数据库或者指定集合可以使用client[‘test’] 和 db.[‘student’] 的形式进行指定（集合的结构造就了这种选择方式） 插入数据insert_one()： 插入单条数据 其返回一个 InsertOneResult 对象 ，表示插入的数据，是个 ObjectId 对象。 insert_many()：插入多条数据 Insert_many( )传入的参数需要以列表形式传递（二维数据） 这个方式挺常用的，我们把抓取的每个数据用字典存储，再append 到一个列表中，最终使用 inser_many 进行一次性存储，非常方便。 查询数据在爬虫中，数据存储最重要的是insert，但是数据查询也必不可少，比较我们需要查看数据是否插入成功嘛 find_one()： find_one（）里面传的参数可以为字典形式，其表明了限制条件，返回的result结果为 dict 类型 发现数据中多了个字段：“id”：“ObjectId”，MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象。ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes。 find(): 返回结果是 Cursor 类型，相当于一个生成器。 我们需要遍历取到所有的结果，每一个结果都是字典类型。 更新数据 Update_One 传入两个参数，第一个是查询条件，第二个是修改的数据（键为使用功能符号@进行选定的操作指令set，值为修改的数据） 如果调用 update_many() 方法，则会将所有符合条件的数据都更新。而update_one只是取第一条匹配的数据进行更新操作。 删除数据 删除数据 有 三个 API 供我们调用，操作与 update挺相像的 今天介绍了Mongodb这种NOSQL数据库的作用和特点，还有Mongodb的基本操作：增删改查。 ok，我们下期见，撒由那拉~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day5-全局解释器锁GIL]]></title>
    <url>%2F2019%2F08%2F11%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay5-%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81GIL%2F</url>
    <content type="text"><![CDATA[GIL （Global Interpreter Lock）Python代码执行由 Python 虚拟机 (又名解释器主循环) 进行控制。Python 在设计时是这样考虑的，在主循环中同时只能有一个控制线程在执行。对 Python 虚拟机的访问由全局解释器(GIL) 控制，这个锁用于确保当有多个线程时保证同一时刻只能有一个线程在运行。 GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念（这把GIL锁加在解释器层面的），而CPython是大部分环境下默认的Python执行环境（GIL并不是Python的特性，Python完全可以不依赖于GIL） GIL遵循的原则：“一个线程运行 Python ，而其他 N 个睡眠或者等待 I/O.”（即保证同一时刻只有一个线程对共享资源进行存取）。 GIL的影响 这个例子中，两个线程分别对一个全局变量total进行加和减1000000次，但是结果并不是0！而是每次运行结果都不相同。 造成这个结果的原因是GIL的释放，python中有两种多任务处理： 协同式多任务处理：一个线程无论何时开始睡眠或等待网络 I/O，就会释放GIL锁（协程） 抢占式多任务处理：如果一个线程不间断地在 Python 3 运行15 毫秒，那么它便会放弃 GIL，而其他线程可以运行（线程） 在单核CPU下没什么不一样（也有可能有性能损失），但是在多核CPU下问题就大了，不同核心上的线程同一时刻也只能执行一个，所以不能够利用多核CPU的优势，反而在不同核间切换时会造成资源浪费，反而比单核CPU更慢。 上面的 +- 操作就是触发了抢占式多任务处理机制，导致了这么一种情况：当一个函数运行着计算密集型程序时，由于GIL的原因，只会在一个单CPU上面运行。 两个函数对于共享变量 total 进行的修改，没有同步变量，导致数据不安全现象。 解决方法 可用 multiprocess 库替代 Thread 库，即使用多进程而不是多线程，每个进程有自己的独立的GIL，因此也不会出现进程之间的GIL争抢。但这样的话也会带来很多其他问题，比如进程间数据通讯和同步的困难，但是，多进程的开销也会增大。 多进程+协程（多用于解决IO密集型情景） 用其他解析器。像JPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。用了Java/C#用于解析器实现。 总结 在IO密集型型操作下，多线程还是可以的。比如在网络通信，time.sleep()延时的时候或者是 IO 操作时，有空余出来的时间，我们就可以对其他线程进行处理。 在CPU密集型/计算密集型操作下(没有空余时间)，多线程性能反而不如单线程，此时只能用多进程（从而打破GIL对多核的限制） 其实龟叔对这个GIL是有其考虑之处的，虽然python 有了这个缺陷，但是python社区的大佬们都在为了这个GIL 奋斗，相信不久的将来，这个缺陷会被解决。（对比python2.7，如今的python3.7已经达到了运行速度更加快的进展）]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day12-代理请求]]></title>
    <url>%2F2019%2F08%2F11%2F%E7%88%AC%E8%99%ABDay12-%E4%BB%A3%E7%90%86%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[前言我们在爬虫的过程中会遇到一些反爬手段，例如封IP、封用户账号等行为，我们可以通过使用代理IP或者代理池进行代理访问，预防被反爬。如何学会代理请求是爬虫工程师必须会的技能噢~ 必须点满 代理的原理我们常称呼的代理实际上指的就是代理服务器，英文叫做 ProxyServer，它的功能是代理网络用户去取得网络信息。也就是说，我们发送请求给代理服务器，再由他再次发出请求给服务端。这样就成功实现了 IP 伪装，也是代理的基本原理。 代理服务器这种机制，还有其他功能： 隐藏真实IP，对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。 提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。 Session和Cookies接着，我们应该了解这两个概念：Session和Cookies。 在浏览网站的过程中我们经常会遇到需要登录的情况，有些页面只有登录之后我们才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就会需要重新登录。还有一些网站有时在我们打开浏览器的时候就自动登录了，而且很长的时间都不会失效，这种情况又是为什么？其实这里面涉及到 Session 和 Cookies 的相关知识 无状态HTTPHTTP的无状态是指 HTTP协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送一个 Requset后，服务器解析此 Request，然后返回对应的 Response，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化。（这就导致了如果用户后续需要对已经发送的数据再做操作就会导致重传等等一些浪费资源的现象） 所以，这时候，两个用于保持HTTP 连接状态的技术就出现了，它们分别是 Session 和Cookies。（Session在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端） Cookie的作用有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。Cookie相当于一个凭证的东西，有了它，就可以完成“身份检验”。 注意，Cookie 是由服务器端通过 set-Cookie字段发送给客户端的，而不是客户端自动产生的。 Session原理在 Web 中 Session对象用来存储特定用户会话所需的属性及配置信息。当用户在应用程序的 Web 页之间跳转时，存储在Session 对象中的变量也不会丢失，会在整个用户会话中一直存在下去。（在登陆之后访问同网页下其他页面不需要重新登陆的原因） 当客户端再次访问时，服务器就拿着客户端携带的Cookies参数，检查该 Cookies 即可找到对应的Session是什么，然后再判断 Session来以此来辨认用户状态。这样，就实现了免登陆的效果。 神秘数字403当运行的爬虫速率太高，或者被检测出你是个爬虫工具发出的访问时，网页可能会弹出这样的403警告信息：“您的 IP 访问频率太高”。出现这样的现象的原因是网站采取了一些反爬虫的措施，比如服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，那么会直接拒绝服务，返回一些错误信息。 例如，这样的写的爬虫… 1234import requestsfor i in range(10000): requests.get('http://www.baidu.com') 是不是想人家服务器挂掉… 别人不封你封谁，估计掐死你的心都有了，哈哈 爬虫代理的实现requests的实现12345678910111213import requestsproxy = '123.114.203.61:8118' # 代理IP + 端口号proxies = &#123; 'http':'http://' + proxy, 'https:':'https://' + proxy&#125;try: response = requests.get('http://httpbin.org/get', proxies =proxies) # 代理需要通过 proxies 关键字指定，不然会以为是个data print(response.text)except requests.exceptions.ConnectionError as e: print('ERROR',e.args) proxy 变量接收的是我从网上找的免费代理，然后通过 IP +端口号的形式传给 get方法中的 proxies 变量，达到代理的效果 这么设置之后，访问页面时，对端服务器看到的就是我们设置的这个IP发出的请求。如果被封了，我们可以通过更换IP进行修改 selenium的实现123456789from selenium import webdriverproxy = '123.114.203.61:8118'chrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--proxy-server=http://' + proxy )browser = webdriver.Chrome(chrome_options=chrome_options)browser.get('http://httpbin.org/get')print(browser.page_source) selenium的实现是通过实例化一个带参数的 ChromeOptions 对象，然后这个对象就携带着我们添加进去的参数 后记实现代理的途径还有代理池、付费代理、ADSL拨号等等，不过原理都是相似的，都是拿到一个IP，然后传给 请求函数，这样携带的请求就是代理 IP。 不过最好还是控制一下记己，服务器挂了大家都没得玩。利用爬虫的时间，冲冲茶养养生还是蛮不错的，哈哈哈]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day4-并发并行与同步异步]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay4-%E5%B9%B6%E5%8F%91%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[并发与并行的区别 并发：是指系统具有处理多个任务的能力。 单个CPU可以通过高频率的切换达到并发效果。 并行：是指系统具有 同时 （唯一的时间节点）处理多个任务的能力。 多核CPU能实现并行效果。 并行是并发的一个子集，并行一定是并发，而并发不一定是并行。 虽然当任务量小时，并发给我们的直观感受好像也是同时完成的任务，但其实是利用CPU高速计算，频繁的切换进程（15毫秒）来实现的。 同步与异步的区别 同步： 你总是做完一件再去做另一件，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果（期间其他事情也没干）。例如socket中的recv，同步时，就一直等待用户输入，直到有结果返回。 异步则反之，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。（异步方法通常会在另外一个线程中，“真实”地执行着。整个过程，不会阻碍调用者的工作。） 同步异步是描述的是一种处理机制。 阻塞与非阻塞的区别阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。(进程或线程就阻塞在那了，即挂起状态，不能做其它事情) 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。(在等待的过程中可以做其它事情) 举一个现实生活的例子吧： 有一个人想要网购买东西，他打电话问店家老板：“我想要xxx，这个价格是多少”？ 如果是同步：问完之后，他就一直在电话那头等待着老板的回答；而如果是异步，他问完之后就挂断了电话，等待店家老板查询价格之后再回拨电话。 如果是阻塞，那个人就会在电话面前苦苦等待，什么事也不干；如果是非阻塞，他在挂断电话之后，可以干其他事情，期间会看看电话是否得到回复。]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day3-子线程]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay3-%E5%AD%90%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言之前我们讨论了进程与线程，网络编程Day1-进程线程概念，那么，在python中如何调用呢？今天这一篇，就是介绍如何创建子线程进行调用的。并且在学习如何创建子线程之后，子进程的创建也就会了，因为他们的 API 接口都是一样的。 创建子线程创建线程有两种方法： ① 使用python替我们封装好的模块：threading ② 基于Thread类，我们重写模块。通过重写run方法，调用这个模块 线程的运行规律我们开启了两个子线程，依次让这两个子线程执行一个hello函数，里面使用sleep维持函数的执行时间 观察发现： 我们知道，当单线程运行时，函数是从上到下按顺序执行的，那么 两个sleep函数，则要耗费 6s 以上的时间。但是我们采用了子进程之后，执行时间 只有3秒。 主线程程序按顺序执行，子线程的函数同时也执行（与主程序并驾齐驱，称为并发）三秒后，整个进程才真正结束 当主线程完成想退出时，会检验子线程是否完成，如果子线程未完成，则主线程会等待子线程完成后再退出。 Join方法概念：等待子线程运行完毕后再执行下一步操作 我们知道，子线程的运行默认是并发的，也就是说，不会因为子线程函数运行时，程序就卡在那。join方法可以实现等待子线程运行完后再进行下一步操作 setDaemon方法setDaemon又称为守护线程，被设置为守护进程的子线程，会随着主线程运行完毕而强行终止。（程序是否运行完的判断不再依据守护进程）。 同时，setDaemon方法必须置于 start（）之前 setDaemon（）默认是 Flase，表示未启动 其他实例方法：无论是start 还是 join ，亦或者是 setDarmon方法，都是我们通过Treading.Thread下实例的方法，同时还有： run（）是所有实例化的子线程都会执行的函数 start（）是将子线程设置为等待被CPU调用的状态]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day2-socket原理]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay2-socket%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言之前我们在 计算机网络系列中提到过，应用都是依靠传输层协议进行通信的，并且，不同的网络应用使用端口号进行区分。我们知道IP层的ip地址可以唯一标识主机，而TCP协议和端口号可以唯一标示主机的一个进程。本地进程通讯中我们可以使用PID来唯一标示一个进程，但PID只在本地唯一，网络中的两个进程PID冲突几率很大，这时候我们需要另辟它径了。那么，Socket（套接字）就诞生了。 socket套接字 把传输层以下的低层实现抽象化了，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 套接字介绍套接字的类型： 套接字有两种（或者称为有两个种族）,分别是基于文件型的UNIX和基于网络型的INET。 我们主要讲的是 AF_INET ，这种基于网络类型的套接字 收发消息的原理应用之间互相发送数据，并不是我们想象的那样，直接发送到对端的应用上，而是通过网卡发送到对方的缓存，再通过OS进行调用获得的。如下图…（画的丑不要介意） 客户端和服务端不是对接性、直接的往对方发送消息，而是有经过缓存的中转（应用程序产生的数据拷贝给缓存）。 操作系统如何控制发送数据是按照我们定义的传输层协议（UDP\TCP）进行管理的。 socket建立连接服务器端： 12345678910111213import socket# 实例化，以及注明是哪个套接字类型、基于TCP的流tele_phone =socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 实例化一个套接字对象，绑定、监听都是基于socket对象进行操作的tele_phone.bind(("127.0.10.1",8080)) # 绑定所监听的地址以及端口号（需要在本机网卡上存在的地址）tele_phone.listen(5) # 最多可以跟5个客户机保持连接print("执行到我啦")conn,addr =tele_phone.accept() # 等客户端的链接地址，当对面使用connect，就建立起连接了# 注意，收发消息是基于conn连接的conn.send("hello world".encode('utf-8')) # 发送数据，基于网络通信，须以二进制传输conn.recv().decode('utf-8') # 等待对方发送数据，编码格式要保持一致 函数 作用 s.bind() 绑定(主机,端口号)到套接字 s.listen() 开始TCP监听，listen对应backlog池，当有多个链接时，服务端对将收到的链接放到backlog池中，链接池里面的链接会等待着与服务器建立链接 s.accept() 被动接受TCP客户的连接,(阻塞式)等待连接的到来 s.recv() 接收TCP数据 s.send() 发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完) 客户端： 123456import socketphone = socket.socket() # 实例化一个套接字对象phone.connect(("127.0.0.1",8080)) # 连接服务器绑定的IP地址phone.recv().decode('utf-8') phone.send("OK".encode('utf-8')) 通过上面这些代码，我们就已经建立起了socket连接，然后就可以通过 send、recv 函数来完成收发信息的操作了。 粘包现象从上面我们知道了，数据的交互是通过缓存区的，那么粘包就很好理解了：上次执行的命令，没有全部显示完，还停留在缓存区，等下次执行别的命令时，这时候，缓存把残余的内容发送出来了。所谓粘包问题主要还是因为接收方不知道消息之间的界限，不知道一次性提取多少字节的数据所造成的。 粘包现象两种情况： ① 发送数据量太大，超出对端缓存区限制 ② 发送数据量非常小，TCP采用Nagle算法（为了消除网络延迟），Nagle算法简单来说就是：如果你连续发送几次数据，但这些数据并不大，通常TCP会根据优化算法把这些数据合成一个TCP段后一次性发送。而对端当作一个包处理（它并不能逆向解包，导致粘包现象的产生） 当然，这种粘包现象是有解决方案的——使用UDP，它是无连接的、面向消息的，数据包与包之间互相独立，一收对应一发，因此没有粘包现象的产生。但是会丢数据，不可靠。 或者，告诉对端消息边界——在实际数据交互之前，通知对方，“我”的数据有多少；或者将多次发送数据的情况分隔开。 这篇socket就介绍到这儿了，下回见~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day11-Selenium魔法师 Vs bilibili]]></title>
    <url>%2F2019%2F08%2F06%2F%E7%88%AC%E8%99%ABDay11-Selenium%E9%AD%94%E6%B3%95%E5%B8%88-Vs-bilibili%2F</url>
    <content type="text"><![CDATA[前言B站可谓是个资源丰富的地方，对我来说，它就是一个学习网站，里面啥都有。。。我们今天使用selenium 爬取B站信息，实现 关键词 抓取 的需求。 对于selenium 我们之前已经介绍过： 爬虫Day8-Selenium魔法师 实战url = https://www.bilibili.com/ 使用 selenium ，当然是先需要获取我们对应点击的节点，进行模拟操作了。 拿着之前的文章，这不是小意思嘛，欻欻的写出了下面这段代码： 代码v1.0 12345678910111213141516171819202122232425262728293031from selenium import webdriverfrom selenium.webdriver.support.wait import WebDriverWaitfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECimport timebrowser = webdriver.Chrome()wait = WebDriverWait(browser, 20)url = 'https://www.bilibili.com/'KEYWORD = '大仙'def get_page(): browser.get(url) # 点击首页刷新一下网页，防止有验证码登录 index = '#primary_menu &gt; ul &gt; li.home &gt; a' can_click = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,index))) can_click.click() # 找出文字搜索框 botton = '#banner_link &gt; div &gt; div &gt; form &gt; input' input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, botton))) # 找出提交按钮 search = '#banner_link &gt; div &gt; div &gt; form &gt; button' wait_search = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, search))) input.send_keys(KEYWORD) # 发送关键字 wait_search.click() # 点击按钮 ok…接着我们运行一下代码测试一下，然后就来到了这个页面 发现所有的视频内容都包括在一个 class = “all-contain”的 div 中 我们使用 find 方法将其匹配出来。 代码v1.1 1234567891011def get_page(): ... # 以上省略的代码 input.send_keys(KEYWORD) wait_search.click() # 页面跳转 browser.switch_to_window(browser.window_handles[1]) html = browser.page_source return html 为什么有一个页面跳转的步骤呢？ 因为啊，我们的浏览器发送关键词搜索之后，页面已经切换了，而我们后续需要使用page_source 属性获取响应信息，如果没有页面切换，那么得到的响应内容是原页面的内容，这不是我们想要的。 解析我们调用 爬虫Day6-Beautiful介绍 这个库 进行代码解析并写入文件 1234567891011121314def parser(html): soup = BeautifulSoup(html,'lxml') results = soup.find(class_='contain').find_all(class_='info') with open('daxian.txt', 'w', encoding='utf-8') as f: for result in results: title = result.find(class_='des hide').text.split() watch_frequency = result.find(class_='so-icon watch-num').text.split() put_time = result.find(class_='so-icon time').text.split() the_up = result.find(class_='up-name').text.split() f.write(str(title)+ '\n') f.write(str(watch_frequency) + '\n') f.write(str(put_time) + '\n') f.write(str(the_up) + '\n') f.write('='*50 + '\n') 运行结果如下： 后记这一次的selenium实战就介绍到这里了， 如果想实现多页爬取，同样道理，利用 EC.element_to_be_clickable（） 显示等待 “下一页”节点加载出来，然后模拟点击即可。 用没用感受到selenium的魅力呢？ 什么反爬什么加密统统不在话下！这可不是吹的~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程Day1-进程线程概念]]></title>
    <url>%2F2019%2F08%2F05%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8BDay1-%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[前言网络编程很重要，在实际开发环境中，很多地方会用到， 举个例子，当我们的爬虫速率太慢了，我们可以开多进程多线程进行爬取；遇到 I/O 密集型任务，我们可以采用多线程处理。 今天介绍的进程线程是理论基础，应该熟练掌握。不熟悉进程线程概念的同学，认真理清其中的区别噢~ 操作系统还记得我们之前说过的操作系统基础吗？今天排上用场了~ 忘记的童鞋可以点击下方链接看一下： 计算机基础知识之操作系统 操作系统是一个用来协调、管理和控制计算机硬件和软件资源的系统程序，其中包括：文件系统、内存管理、设备管理和进程管理。它位于硬件和应用程序之间。 简单来说，操作系统是存在于用户与程序之间的”中间商”，我们负责下令调度，它负责具体工作的实现。 程序？进程？我们知道，每一个软件都是由一串串代码构造出来的，而在我们电脑磁盘中，任何文件类型都只能以二进制存储。 那么，以一堆代码以文本形式存入一个文档，我们把这么一个二进制文件叫做程序。当我们运行这个程序时，就会读取到内存，再用相应的解释器编译执行。当程序执行起来时，我们称之为进程。在我们的电脑上，每一个运行着的软件，就是一个进程。 进程的定义进程就是一个程序在一个数据集上的一次动态执行过程（程序运行的一种状态）。 进程一般由程序、数据集、进程控制块三部分组成。 程序：我们编写的程序用来描述进程要完成哪些功能以及主逻辑； 数据集则是程序在执行过程中所需要使用的资源； 进程控制块用来记录进程的外部特征，描述进程的执行变化过程，系统可以利用它来控制和管理进程（很重要的概念），它是系统感知进程存在的唯一标志。 进程切换我们知道，一个CPU在同一时刻只能处理一件事情，但是呢，CPU运行速度很快，它可以实现多个进程之间来回切换来达到我们看到的多个进程同时运行的效果。 进程是一种程序运行的状态，会涉及到状态挂起的保存，状态恢复、程序的识别、CPU资源的调用等等，因此，进程之间的切换是很消费资源的。 因为，进程被暂时挂起后，在随后的某个时间里，该进程再次启动的状态必须和挂起之前一致。这就意味着该进程挂起时所有的信息都要记录下来（比如指针、值等等）。这样在该进程重新启动后，所执行的调用才能拿到正确的数据。 还有就是，进程的切换是怎么样的呢？ 两个条件可以触发进程切换： ① 时间轮循 ： 每隔一段时间，CPU就会切换到其他进程。 ② I/O操作 ： 当遇到 输入输出操作时，CPU不会傻傻的等着，直接会进行切换操作。 进程状态因此，我们得到了这么一幅图，这就是进程的状态变化： 转换2和3都是由进程调度程序引起的，进程调度就是操作系统的一部分，系统认为一个运行进程占用处理器时间已经过长，就会发生进程转换。 当遇到I/O状态时，就会发生转换1，然后CPU在没得到响应之前（收到有效输出），不会再运行阻塞状态的进程。 线程定义线程的概念：“一个进程的独立运行片段”。 我们知道了，进程虽然可以完成切换，但是十分的消耗资源。线程的出现是为了降低上下文切换的消耗，提高系统的并发性，并突破一个进程只能干一样事的缺陷，使到进程内并发成为可能。 上面这三个任务，我们不能放到同一个进程里面，不然就无法实现三个任务的切换操作。 但又不想放到三个进程里（这三个任务实现的是同一个功能，所以三个进程之间需要互相访问，同时频繁的切换很费资源）。 线程也叫轻量级进程，它是一个基本的CPU执行单元，也是程序执行过程中的最小单元。——相当于进程里面的进程 线程的引入减小了程序并发执行时的开销，提高了操作系统的并发性能。线程没有自己的系统资源。所有的线程共用进程里面的资源 进程线程的关系 一个程序至少有一个进程,一个进程至少有一个线程.(进程可以理解成线程的容器)。 每一个进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行。 进程是最小的资源单位，操作系统分资源只能分到进程（最小单位），而线程是最小的执行单位（程序真正在跑的，是线程） 乌拉！！ 进程线程介绍就到这里了。各位，下期见~]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day10-Ajax异步请求]]></title>
    <url>%2F2019%2F08%2F04%2F%E7%88%AC%E8%99%ABDay10-Ajax%E5%BC%82%E6%AD%A5%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[Ajax引言：有时候我们在用 Requests 抓取页面的时候，得到的结果可能和在浏览器中看到的是不一样的，在浏览器中可以看到正常显示的页面数据，但是得到的 Response 并没有相应的内容。 这其中的原因是 Requests 获取的都是原始的 HTML 文档，而浏览器中的页面则是页面又经过 JavaScript 处理数据后生成的结果，这些数据的来源有多种： 通过 Ajax 加载 包含在了 HTML 文档中的（我们之前爬取的网站都是这个类型） 经过 JavaScript 经过特定算法计算后生成的。 1今天我们就来谈谈 Ajax 加载类型的网页数据。 Ajax的概念Ajax，全称为Asynchronous JavaScript and XML，即异步的 JavaScript和 XML， 对于Ajax 渲染 的页面，数据的加载是一种异步加载方式，原始的页面最初不会包含某些数据，原始页面加载完后会会再向服务器请求某个接口获取数据，然后数据再被处理才呈现到网页上，这其实就是发送了一个 Ajax请求。 这也就是我们直接使用 requests 请求 不到页面实际内容的原理所在。 这时需要做的就是分析网页的后台向接口发送的 Ajax 请求，再用Requests 来模拟 Ajax 请求，那就可以成功抓取了。 1知道了A jax 原理之后，一切就变得有头绪了些~ Ajax 观察对于Ajax 请求， 我们在现实生活中一直碰到，我描述一下你就知道了：“当你在浏览网页、APP的时候，下拉进度条到最下面时，会看到进度条唰的一下子跳到中间，然后下方有了新的内容出现” 其实这就是Ajax 的运用了。 今天，我们使用 微博来作为爬取对象。 url = https://m.weibo.cn/u/2145291155 浏览器发起请求老规矩，进入开发者模式。 然后在Network 下，点击 XHR 进行筛选（过滤出只有Ajax请求的请求响应报文） 让我们看第一个条目信息，观察其Response 发现其代码只有五十行，结构也非常简单，只是执行了一些 JavaScript。这只是最原始的链接返回的结果。 所以说，我们所看到的微博页面的真实数据并不是最原始的页面返回的，而是后来执行 JavaScript 后再次向后台发送了 Ajax 请求，拿到数据后再进一步渲染出来的。 于是我们进行测试，拖拽进度条到最下面，发现浏览器与服务器偷偷的交互了报文，效果如下： 新刷新出来的响应报文就是网站下方一些新的内容了 Ajax 渲染网页的就是这样，当用户看到下方时，才会加载内容。这样在 Web 开发上可以做到前后端分离，而且降低服务器直接渲染页面带来的压力。（以后可能都是这样类型的网页噢，赶紧学起来吧~） 查看详细信息我们点开这条响应报文进行分析。 Ajax其实有其特殊的请求类型，它叫作xhr。我们可以发现一个名称以getlndex开头的请求，其Type为xhr，这就是一个Ajax请求。 其中RequestHeaders 中有一个字段为 X-Requested-With：XMLHttpRequest，这就标记了此请求是Ajax异步请求 Ajax 结果提取我们要能将Ajax内容提取出来，就是要用Python来模拟这些Ajax请求，达到自动加载页面信息的功能。（第一步就是观察参数的规律，这是解析网页的必修课） ① URL的构建 观察一下这些请求，发现它们的 type、value、containerid 始终如一。type 始终为 uid，value 的值就是页面的链接中的数字，其实这就是用户的 id，另外还有一个containerid，经过观察发现它就是 一个常量 然后加上用户id。 所以改变的值就是 page，很明显这个参数就是用来控制分页的，page=1 代表第一页，page=2 代表第二页，以此类推。我们使用一个变量来控制页码数 ② 头部的构造 并且，我们需要构造header头部，模拟这是由浏览器发出的Ajax请求，还有一些重要字段（最基本的反爬策略） ③ 观看响应内容 它是一个 Json 格式，浏览器开发者工具自动为做了解析方便我们查看，可以看到最关键的两部分信息就是 cardlistInfo 和 cards，将二者展开，cardlistInfo 里面包含了一个比较重要的信息就是 total，经过观察后发现其实它是微博的总数量，我们可以根据这个数字来估算出分页的数目。 其中cards 则是一个列表，它包含了 10个元素，这就是每一条微博帖子存放的地方所在了。 ④我们继续展开看看 发现它包含的正是微博的一些信息。比如 attitudes_count 赞数目、comments_count 评论数目、reposts_count 转发数目、created_at 发布时间、text 微博正文等等 Python 代码模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445import requestsimport urllib.parsefrom pyquery import PyQuery as pqbase_url = 'https://m.weibo.cn/api/container/getIndex?'# 抓包得到的Ajax请求的url前部headers = &#123; 'host':'m.weibo.cn', 'Referer':'https://m.weibo.cn/u/2145291155', # 此内容用来标识这个请求是从哪个页面发过来的 'User-Agent':'Mozilla/5.0', 'X-Requested-With':'XMLHttpRequest'&#125;def get_page(page): params = &#123; 'type':'uid', 'value':'2145291155', 'containerid': '1076032145291155', 'page':page &#125; # 这个字典是我们根据xhr连接请求分析构造的 url = base_url+ urllib.parse.urlencode(params) # 构建Ajax请求的 URL response = requests.get(url=url,headers=headers) return response.json() # 返回json 格式的数据def parse_page(json): result = json.get('data').get('cards') result =result.pop() weibo = &#123;&#125; item = result.get('mblog') print(item) weibo['text'] = item.get('text') weibo['attitudes'] = item.get('attitudes_count') weibo['comments'] = item.get('comments_count') weibo['reposts'] = item.get('reposts_count') print(weibo)if __name__ == '__main__': for page in range(1): json = get_page(page) result = parse_page(json) 运行结果如下： 先抓取一页进行测试，完成！ 这样，我们只需要增加 变量 page，即可将微博数据全部爬取下来了。由于篇幅原因，略过。 写在后面：这次我们主要介绍了Ajax 请求的原理，这样一来，我们就可以模拟Ajax请求，对任何使用异步加载的网页进行爬取了。 啊~ 溜了溜了，胖友们，我们下回见。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day9-requests实战]]></title>
    <url>%2F2019%2F08%2F04%2F%E7%88%AC%E8%99%ABDay9-requests%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[前言之前介绍requests 这个库的时候在文尾说过要做一篇实战的文章，今天突然想起，然后就写一下咯。 爬虫Day5-requests介绍 今天的主题是——使用 requests请求库 和 re 解析库进行爬取当当网热门top 500书籍。 url = http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-1 网页省察第一步：我们进入url 链接之后，按F12进入开发者模式 总结出以下规律：所有书籍都存放在一个 ul 标签下面 每一本书都是 一个li 节点 第二步：切换页码数，观察 url 变化 发现 url 链接最后面的数字发生改变—— 由”1“ 变成 ”2“，这正好对应上我们翻页的页码数 每一页的书籍量为20 ， Top 500 的书籍 爬取 25 个连接就可以爬取完毕 动手写代码12345678def get_page(page): kv = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; # 伪装浏览器标识 url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page) # 由 变量 page 来进行控制翻页 response = requests.get(url=url, headers=kv) # 发出请求 return response.text # 将响应内容返回 解析我们点开的每一个 li 节点，观察我们需要爬取的节点信息 发现每一段信息都有一个div 存储着，我们只需要re 正则匹配出来，就欧克了 解析代码如下： 123456789101112131415def parser_page(html): regix = re.compile(r'&lt;li.*?list_num.*?"&gt;(\d+).*?&lt;img src="(.*?)".*?class="name".*?title="(.*?)"&gt;.*?class="tuijian"&gt;(.*?)&lt;/span&gt;.*?class="publisher_info.*?title="(.*?)".*?class="biaosheng".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?class="price_n"&gt;&amp;yen;(.*?)&lt;/span&gt;',re.S) # 编写匹配规则 doc = regix.findall(html) for i in doc: yield &#123; '排名':i[0], '图片':i[1], '书名':i[2], '推荐指数':i[3], '作者':i[4], '五星好评人数':i[5], '价格':i[6] &#125; # 将内容变成一个生成器 return doc 我们先看看 这个 叫 “doc” 生成器 的内容，使用 for 循环遍历出来： perfect！ 这样我们就把信息完美的抓取下来了。 写入文件接下来，我们就将内容写入文件，以便后期的数据分析。 1234with open("book.txt","w",encoding="utf-8") as f: for writeline in doc: print(writeline) f.write(str(writeline) + '\n') 我们运行一下代码，效果如下： 爬虫完善最后，我们需要控制爬取速率，做一个有素质的爬虫，毕竟一直爬取别人网站，服务器也会受不了的呀。并且，加上异常处理，一个简单的小爬虫就写好了。 完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243import requestsimport timeimport redef get_page(page): kv = &#123; 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36' &#125; url = 'http://bang.dangdang.com/books/fivestars/01.00.00.00.00.00-recent30-0-0-1-' + str(page) response = requests.get(url=url, headers=kv) return response.textdef parser_page(html): regix = re.compile(r'&lt;li.*?list_num.*?"&gt;(\d+).*?&lt;img src="(.*?)".*?class="name".*?title="(.*?)"&gt;.*?class="tuijian"&gt;(.*?)&lt;/span&gt;.*?class="publisher_info.*?title="(.*?)".*?class="biaosheng".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?class="price_n"&gt;&amp;yen;(.*?)&lt;/span&gt;',re.S) doc = regix.findall(html) for i in doc: yield &#123; '排名':i[0], '图片':i[1], '书名':i[2], '推荐指数':i[3], '作者':i[4], '五星好评人数':i[5], '价格':i[6] &#125; return docdef main(): for i in range(1,26): html = get_page(i) doc = parser_page(html) time.sleep(4) with open("book.txt","w",encoding="utf-8") as f: for writeline in doc: print(writeline) f.write(str(writeline) + '\n')if __name__ == '__main__': main() 后记可以发现，使用 re 来解析库的难度还是有的，不过，我想说的是，使用什么模块并不重要，只有最适合当前需求的模块，取决于你如何去灵活运用它，编程语言也是这样。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吐槽下环境部署]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%90%90%E6%A7%BD%E4%B8%8B%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[前言今天，用了大半天的时间，部署了爬虫环境。 历经千辛万苦，终于全部搞定了。现在是晚上十一点，有些感概，对于这么一套流程，有一些思考，于是有了写下来的冲动。 鬼知道我经历了什么不夸大的说，今天遇到的BUG，已经超出了十几个，这么debug过来，一个又一个新的问题又爆出来。有时候，并不是那么顺风顺水。 当心情开始暴躁的时候，不妨将手头的事情放一放（进度不是很紧的话） 其次，要相信，在计算机中没有解决不了的问题。 在每一次遇到bug时，我都不会慌张，因为我知道，这是必然的、肯定会出现的一种现象，于是我面对bug的时候会很自信，因为最终它都会被我消灭掉。 真正的难题，是你如何去debug。 处理BUG的流程这套流程不一定适用于任何场景，但是作用于解决问题的参考还是行得通的。 出现报错一个BUG的出现往往是由ERROR这个字眼开始的，这时候，第一步要做的就是将它提示的内容，翻译为自己能理解的意思，再配合你当前所做的事情，联想一下，思考它为什么会报错。 一个谦卑的程序员总是先设想自己的代码是错的，排除之后再找其他方面的问题 谷歌、stackoverflow还有一点要相信的是，你遇到的BUG绝对不是历史第一次，什么意思呢？ 我们遇到的BUG，前辈们已经踏过无数次，我们可以利用互联网这个大资源，将自己遇到的东西，提取出关键字进行谷歌搜索，或者，直接将报错的代码直接复制粘贴。这是最简单粗暴的办法，但是没有前者精准。 所以，学会如何提问也是一门技术。 记录debug将问题解决了之后，如果你还是不能理解为什么，我们就可以将面临的问题、如何解决的，记下来。做好笔记，这是一个好的习惯。有很大几率，你会在某一段时间再次碰到你之前遇到的BUG，这时候，翻翻笔记，比盲目的谷歌好用多了，同时也促进了你对问题的理解。 为自己欢呼最终的最终，我们把BUG踩在脚下，这时候，请给自己鼓一下掌。因为这是不容易的，有句话是这么说的：“击不倒我的，必然使我更加强大” 既然选择了当程序猿，就不可能不遇到BUG，我们要学会坦然处之。这些debug的经验，都是你身为一名计算机人士的财富，你应该感谢一路走来的BUG，他成就了明天更强大的你。 写在最后今天被Docker 环境部署折腾的够呛，其中包含了 VirtualBox的配置、git 版本的更迭设置、 DockerToolbox 的配置 等等，其中遇到的难题之前都没接触过，历时半天，我终于将环境部署完成。等到看到内容完全输出到 git bash 的时候，真的挺开心。 最后，我想说的是，计算机是最公平的，对就是对，错就是错。只有耐心与它沟通，才能解决问题。 于 2019.8.02 晚 23.51 的深夜吐槽~]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>随便聊聊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day8-Selenium魔法师]]></title>
    <url>%2F2019%2F07%2F31%2F%E7%88%AC%E8%99%ABDay8-Selenium%E9%AD%94%E6%B3%95%E5%B8%88%2F</url>
    <content type="text"><![CDATA[前言久违的爬虫Day 系列！！ 自从计算机网络系列更新完之后，我又把目光投到爬虫系列来了hhh… 今天介绍的技术可厉害了，在外人眼中，这近乎是“魔法”，那就是——selenium 同时，这也是我目前最为喜欢的爬取工具。 Selenium 是一个自动化测试工具，利用它我们可以驱动浏览器执行特定的动作，如点击、下拉等等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。用 Selenium 来驱动浏览器加载网页的话，我们就可以直接拿到 JavaScript 渲染的结果了，不管是什么加密统统不用再需要担心。 环境准备： selenium Chrome浏览器 与 ChromeDriver 以上内容，请自行百度安装 selenium介绍：模拟登陆：声明浏览器对象：selenium 驱动浏览器进行操作的，于是我们要实例化一个浏览器对象，有以下： 接下来我们要做的就是调用 生成的 browser 对象，让其执行各个动作，就可以模拟浏览器操作了。 访问页面：基于browser对象， 我们可以用 get() 方法来请求一个网页，参数传入链接 URL 即可（get方法没有返回值，他只是模拟了浏览器的运行） 123456from selenium import webdriverbrowser = webdriver.Chrome() # 实例化一个浏览器对象，后续操作都是针对此对象进行操作的browser.get('http://www.baidu.com') # 模拟登陆网页print(browser.page_source) # page_source（）方法输出了页面的源代码browser.close() # 使用close（）对浏览器进行关闭。 让我们看看效果： 有没有看到嗖的一下子，就访问了百度这个网站，然后解析出了HTML。 注意： 这里我们拿到了一个 browser 对象，这个在后续操作中，是最关键的（节点选择，动作交互等等，都是基于 browser对象进行操作） 查找节点Selenium 可以驱动浏览器完成各种操作，比如填充表单、模拟点击等等，比如我们想要完成向某个输入框输入文字的操作，在这这前我们需要得知这个输入框的位置。 所以 Selenium 提供了一系列查找节点的方法，我们可以用这些方法来获取想要的节点，以便于下一步执行一些动作或者提取信息。 我们想要驱动浏览器完成自动搜索的任务，首先要先找到文本输入框。图中&lt;input&gt;节点就是我们要找的目标，熟悉前端的胖友，肯定倍感亲切。 可以发现它的 ID 是 q，Name 也是 q，还有许多其他属性，那我们获取它的方式就有多种形式了，第一种是通过节点名称查找，比如find_element_by_name() 是根据 Name 值获取，find_element_by_id()是根据 ID 获取，另外还有根据XPath、CSS Selector 等获取的方式。 以下代码可以查找出文本框的节点： 123456789101112131415from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get("https://www.taobao.com")input_first = browser.find_element_by_id('q') # 通过节点名称指定input_second = browser.find_element_by_css_selector('#q') # css选择器input_third = browser.find_element_by_xpath('//*[@id="q"]') # 使用xpath路径list = [input_first,input_second,input_third]for i in list: print(i)time.sleep(3)browser.close() 效果演示： 进入到淘宝页面中，通过selenium的节点选择，使我们的光标直接移动到了文本框中。 可以看到三个节点都是 WebElement 类型，是完全一致的。证明三种方法选择效果是一样的。 其他获取节点的方法： 这些API 都通过节点名称表明了它的作用。 我们就可以利用这些API，获取我们需要的节点了。 节点交互Selenium 可以驱动浏览器来执行一些操作（文本输入和提交），也就是说我们可以让浏览器模拟执行一些动作，比较常见的用法有：输入文字用 send_keys() 方法，清空文字用 clear() 方法，另外还有按钮点击，用 click() 方法。 12345678910111213from selenium import webdriverimport timebrowser = webdriver.Chrome()browser.get('https://www.taobao.com')input = browser.find_element_by_id('q')input.send_keys('iPhone') # 输入文字time.sleep(1)input.clear() # 清除文字input.send_keys('iPad')button = browser.find_element_by_class_name('btn-search') # 获取搜索框的节点属性button.click() # 点击，在这里是提交搜索内容的意思 节点交互的方法基于WebElement对象 我们发现，每个搜素框都有对应的属性名、id值，我们需先利用find_element_by_id（） 函数找到他（返回一个WebElement对象），然后传递交互方法。这样就实现了模拟浏览器的效果 使用send_keys 方法输入文字（这个方法挺常用的，例如selenium模拟登陆，我们可以输入用户名密码…） 其他交互动作也是由各个API封装好了，具体观看官方文档 : https://selenium-python-zh.readthedocs.io/en/latest/ 动作链在上面的实例中，一些交互动作都是针对某个节点执行的，比如输入框我们就调用它的输入文字和清空文字方法，按钮就调用它的点击方法，其实还有另外的一些操作它是没有特定的执行对象的，比如鼠标拖拽、键盘按键等操作。所以这些动作我们有另一种方式来执行，那就是动作链。 我们下面使用一个网站来测试 selenium 的动作链 123456789101112131415from selenium.webdriver import ActionChains # 动作链使用的类browser = webdriver.Chrome()url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'browser.get(url)browser.switch_to.frame('iframeResult')# selenium提供了switch_to.frame()方法来切换frame,括号内是传入的参数，# 用来定位frame，可以传入id、name、index以及selenium的WebElement对象source = browser.find_element_by_css_selector('#draggable') # 选择节点target = browser.find_element_by_css_selector('#droppable')actions = ActionChains(browser) # 实例化动作链对象actions.drag_and_drop(source, target) # 基于此对象，进行拖动并放入的操作actions.perform() # 必须的调用，执行动作 效果演示： 搜的一下子，selenium 就出色的完成了拖动释放的操作，是不是很棒！ 执行JavaScript语句对于某些操作，SeleniumAPI 是没有提供的，如下拉进度条等，可以直接模拟运行 JavaScript，使用 execute_script() 方法即可实现。 123456from selenium import webdriverbrowser = webdriver.Chrome()browser.get('https://www.zhihu.com/explore')browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')browser.execute_script('alert("To Bottom")') 效果演示： 不单单上面举例的JS语句，还能实现打开选项卡等其他操作 获取节点信息获取源代码 page_source 属性可以获取网页的源代码，获取源代码之后就可以使用解析库如正则、BeautifulSoup、PyQuery 等来提取信息了。不过既然 Selenium 已经提供了选择节点的方法，返回的是WebElement 类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等等。这样的话我们就可以不用通过解析源代码来提取信息了。 获取属性使用find_element获取节点，基于WebElement对象 ，使用 get_attribute() 方法，传入想要获取的属性名，就可以得到它对应的值了。 获取文本值原理同上，每个 WebEelement 节点都有 text 属性，我们可以通过直接调用这个属性就可以得到节点内部的文本信息了，就相当于 BeautifulSoup 的 get_text() 方法、PyQuery 的 text() 方法。 获取ID、相对位置、标签名、大小等信息 WebElement 节点还有一些其他的属性，比如 id 属性可以获取节点 id，location 可以获取该节点在页面中的相对位置，tag_name 可以获取标签名称，size 可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。 设置延时等待 动态加载的页面需要时间等待页面上的所有元素都渲染完成，如果在没有渲染完成之前我们就switch_to_或者是find_elements_by_，那么就可能出现元素定位困难而且会产生错误信息 在 Selenium 中，get() 方法会在网页框架加载结束之后就结束执行，此时如果获取 page_source ，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的 Ajax 请求（Ajax发出请求和服务器响应网页渲染需要一定时间），我们在网页源代码中也不一定能成功获取到。 所以这里我们需要延时等待一定时间确保节点已经加载出来。在这里等待的方式有两种，一种隐式等待，一种显式等待。 隐式等待当查找节点而节点并没有立即出现的时候，隐式等待将等待一段时间再查找 DOM，默认的时间是 0（固定的等待，如果已经加载出来，那么立即返回），如果超出设定时间后则抛出找不到节点的异常 显示等待实际情况中，显示等待用的比较多，因为隐式等待的效果其实并没有那么好，因为我们只是规定了一个固定时间，而页面的加载时间是受到网络条件影响的。 所以在这里还有一种更合适的显式等待方法，它直接指定好要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，那就返回查找的节点，如果到了规定时间依然没有加载出该节点，则会抛出超时异常。并且还有一个好处——当页面加载很慢时，使用显示等待，等到需要操作的那个元素加载成功之后就直接操作这个元素，不需要等待其他元素的加载。 123456789101112131415161718from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.get("https://www.zhihu.com/explore")wait = WebDriverWait(driver, 10) # 传入一个wait对象try: element = wait.until( EC.presence_of_element_located((By.CLASS_NAME, "zu-top-add-question"))) # 调用until方法 print(element) # 得到的依旧是Element对象finally: print("ok") driver.quit() 主要涉及到selenium.webdriver.support 模块下的expected_conditions类。 接着，我们首先引入了 Wait 这个对象，指定好最长等待时间，实例化出来一个 wait 等待对象。 然后调用它的 until() 方法。第一个参数传入要等待条件expected_conditions ，第二个参数传入对应的定位节点。比如在这里我们传入了 presence_of_element_located 这个条件，就代表当节点被加载出来时，条件被满足。其参数是节点的定位元组，也就是 class 为 “zu-top-add-question” 的节点搜索框。 所以这样可以做到的效果就是，在 10 秒内如果 ID 为 q 的节点即搜索框成功加载出来了，那就返回该节点，如果超过10 秒还没有加载出来，那就抛出异常。 设置Cookies使用 Selenium 还可以方便地对 Cookies 进行操作，例如获取、添加、删除 Cookies 等等。 我们使用了三次 get_cookies() 来显示对Cookies 的一些操作，可以看到 第三次调用之前，我们对cookies进行删除，因此打印此出空列表 异常处理在使用 Selenium 过程中，难免会遇到一些异常，例如超时、节点未找到等错误，一旦出现此类错误，程序便不会继续运行了，所以异常处理在程序中是十分重要的。我们可以使用 try except 语句来捕获各种异常。 1234567891011121314from selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionbrowser = webdriver.Chrome()try: browser.get('https://www.baidu.com')except TimeoutException: # 超时的异常处理 print('Time Out')try: browser.find_element_by_id('hello')except NoSuchElementException: # 没有找到节点的异常处理 print('No Element')finally: # 最终执行的语句 browser.close() 异常处理有利于我们后期的排错，也有利于爬取过程中一些错误的处理，一个完整的爬虫脚本应该含括异常处理。 我们把 selenium 大概的介绍完了！ 有没有像我一样，一接触selenium 就被他高明的抓取手段迷住了呢？哈哈]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day7-数据转发过程]]></title>
    <url>%2F2019%2F07%2F30%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay7-%E6%95%B0%E6%8D%AE%E8%BD%AC%E5%8F%91%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[LAN对于LAN最通俗易懂的了解就是：网段相同的一片区域。 计算机近距离构成的小型网络 叫局域网， 简称LAN。局域网能小到是同一个房间里的两台机器，或大到校园里的上千台机器。LAN技术中最为典型的就是“以太网”。 CSMA/CD多台电脑共享一个传输媒介， 这种方法叫 “载波侦听多路访问” 简称”CSMA”。而这种共享载体式网络有一个弊端：一个共享网段中，多个主机不能同时传输数据，其会发生冲突，导致丢包等现象。 解决办法是，每一台计算机都监听电线中的信号检测这些冲突，当检测到冲突时，就停止传输，等待网络空闲, 然后再试一遍。其等待的时间使用“指数退避”算法进行计算，这样，每台计算机都有机会传输数据了。 因此，CSMA/CD 全程就叫做 ： 载波侦听多路访问 / 冲突检测 虽然CSMA/CD能够一定程度上的解决问题，但是，当主机数量多起来之后，意味着传输速率大大减少，我们使用交换机进行分割为多个“冲突域”，身处不同冲突域的主机传输互不影响。 传输数据的方式：电路交换：连接两台相隔遥远的计算机或网路，最简单的办法是分配一条专用的通信线路，这就叫电路交换。这种方法虽然能用，但是不灵活而且价格昂贵，因为总有闲置的线路，好处是：如果有一条专属于自己的线路你可以最大限度地随意使用，无需共享。 报文交换：“报文交换” 就像邮政系统一样，每个站点都知道下一站发哪里， 因为站点有“表格”，记录到各个目的地，信件该怎么传等等信息。报文交换的好处是 可以用不同路由，灵活性、可靠性大大提高。 而这个过程中，每个站点就是一个路由器，消息沿着路由跳转的次数叫”跳数”，记录跳数很有用，因为可以分辨出路由问题，有利于网络工作人员的差错排查。 报文交换的缺点之一是有时候报文比较大，会堵塞网络，因为要把整个报文从一站传到下一站后才能继续传递其他报文，设想一下，如果你现在有一个非常大的文件在传输，整条路都堵塞了。即便你只有一个1KB的小文件要传输也只能等大文件传完，或是选另一条效率稍低的路线。 因此，分组交换出现了… 分组交换：将大报文分成很多小块，叫”数据包”，就像报文交换 每个数据包都有目标地址，因此路由器知道发到哪里。报文具体格式由”互联网协议”定义，简称“IP协议”。将数据拆分成多个小数据包，然后通过灵活的路由传递，非常高效且可容错，如今互联网就是这么运行的。 有了上面大概的介绍，让我们看看，我们是如何实现上网的~ 分布式网络计算机首先要连到局域网（例如WIFI，也可能是4G），局域网再连到 WAN。WAN 的路由器一般属于你的”互联网服务提供商”，简称 ISP。就这么层层叠加，最终到达了互联网主干（互联网主干由一群超大型、带宽超高路由器组成）。 例如 我们现在在网络上请求了一个资源，数据包（packet）要先到互联网主干，沿着主干到达有对应文件的 服务器，最后拿到资源并返回，其过程跨越了多个路由器交换机。 在路由器交换机中传递的数据包依据 IP头部中的“信息”进行传递，最终使用传输层协议控制，你的计算机内核就拿到了数据。 数据转发过程：接下来我们谈谈… 数据转发的底层实现（前面几篇的汇总） 假设两台主机已经建立了TCP连接。主机对应用数据（Data）压缩加密 ，然后基于传输层协议封装数据 填充源端口跟目的端口（源端口一般是随机端口号），初始序列号和确认序列号字段、标识位、窗口字段等等信息。 随后进行网络层封装 ，一般使用IP封装时，需要源目IP地址，还有其他元数据： 如果IP报文大于MTU （最大传输单元） 则会被分片。 接着，封装生命周期TTL值（每经过一个三层设备这个值将减一 默认值是255 如果三层设备发送报文时 TTL值减为0 将丢弃数据包 这样就形成了防环机制 ） 协议字段标识了传输层所使用的协议，例如上层是TCP，所以该字段的填充值为0x06 （若是UDP 则0x017 ICMP是0x01） 接下来，数据包被封装成数据帧，需填充源目MAC地址字段。主机首先会查询ARP缓存表，如果有相应MAC地址，则直接封装，如果没有，则发送ARP请求： 不同网段通信时，发送ARP查询 ：如果ARP回应不在同一个网段，主机则封装网关的mac地址，对网关进行ARP请求（源MAC是本机，目的MAC是广播地址）， 路由器网关设备收到广播数据帧后 ，将源Mac地址记录到本地Mac缓存表中，并进行ARP回复。 如果ARP回应在同一个网段，那么直接使用ARP对目标主机发出请求 这时， 主机便有了目的网段的物理地址 。主机在链路层封装数据帧时，会遵循一般为以太二型帧标准 （其中的帧头的Type字段为0x0800 表明上层是IP ，ARP是0x0806 ） 途经中途的设备时，他们会做以下操作： ① 进行FCS校验 如果不通过就丢弃 ②对于通过的帧 设备会检查帧中的目的MAC地址 ，若与本地MAC地址不同 ，将进行丢弃 。 ③经过上面筛选，应该是合格的数据帧 ，这时，将帧头帧尾剥去（解封装），剩下数据报文会根据帧头的Type字段来送到上层对应协议模块去处理。 网络层的处理 ： ① 检查IP头部的校验和字段 ，看ip数据报文头部是否完整 ，然后根据目的IP地址查看路由表 ，确定是否能够将数据包转发给目的端 ② 设备在转发出去之前，会对TTL值进行处理，另外报文大小也不能超过MTU值 否则将分片，网络层处理完成后 报文将被送到数据链路层进行重新封装 ，添加新的源目MAC地址，但是源目IP地址则不会改变。 通过路由交换的选路和跳转，目的端最终收到数据包，执行以上步骤，接着查看Protocol字段，送往传输层进行处理。传输层会查看数据段头部信息的端口号 ，将数据段头部剥离后，依据其传输层的特性，将剩下的应用数据发送socket这个API接口处理。 运行于应用层协议的程序（也就是进程）从操作系统的内核态提取或发送数据，最终实现软件的通信。 这个就是数据转发过程中发生的事情，其中省略了物理层、应用层等具体实现。不过计算机网络主要关注的是网络层、传输层，所以有所侧重。 计算机网络系列更新完结啦~ 撒花撒花！！ 这里贴出几篇之前写的计算机网络的文章： 计算机网络Day4-TCP协议 计算机网络Day3-网络层 计算机网络Day2-数据链路层]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day6-应用层协议]]></title>
    <url>%2F2019%2F07%2F29%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay6-%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[前言这一篇文章主要讲述的是应用层协议的几个协议 —— DNS 、 HTTP 和 DHCP DNS协议域名(domain name)是IP地址的代号。域名通常是由字符构成的。对于人类来说，字符构成的域名，比如www.yahoo.com，要比纯粹数字构成的IP地址(106.10.170.118)容易记忆。域名解析系统(DNS, domain name system)就负责将域名翻译为对应的IP地址。在DNS的帮助下，我们可以在浏览器的地址栏输入域名，而不是IP地址。 另一方面，处于维护和运营的原因，一些网站可能会变更IP地址。这些网站可以更改DNS中的对应关系，从而保持域名不变，而IP地址更新。由于大部分用户记录的都是域名，这样就可以降低IP变更带来的影响。 DNS服务器域名和IP地址的对应关系存储在DNS服务器(DNS server)中。所谓的DNS服务器，是指在网络中进行域名解析的一些服务器(计算机)。这些服务器都有自己的IP地址，并使用DNS协议(DNS protocol)进行通信。DNS协议主要基于UDP，是应用层协议 DNS服务器构成一个分级(hierarchical)的树状体系。一次DNS查询就是从树的顶端节点出发，最终找到相应末端记录的过程。 域名解析顺序：中间节点根据域名的构成，将DNS查询引导向下一级的服务器。比如说一个域名cs.berkeley.edu，DNS解析会将域名分割为cs, berkeley, edu，然后按照相反的顺序查询(edu, berkeley, cs)。出口DNS首先根据edu，将查询指向下一层的edu节点。然后edu节点根据berkeley，将查询指向下一层的berkeley节点。这台berkeley服务器上存储有cs.berkeley.edu的IP地址。所以，中间节点不断重新定向，并将我们引导到正确的记录。 在整个DNS查询过程中，无论是重新定向还是最终取得对应关系，都是用户计算机和DNS服务器使用DNS协议通信。用户计算机根据DNS服务器的反馈，依次与下一层的DNS服务器建立通信。用户计算机经过递归查询，最终和末端节点通信，并获得IP地址。 DNS缓存用户计算机的操作系统中的域名解析模块(DNS Resolver)负责域名解析的相关工作。任何一个应用程序(邮件，浏览器)都可以通过调用该模块来进行域名解析。 并不是每次域名解析都要完整的经历解析过程。DNS Resolver通常有DNS缓存(cache)，用来记录最近使用和查询的域名/IP关系。在进行DNS查询之前，计算机会先查询cache中是否有相关记录。这样，重复使用的域名就不用总要经过整个递归查询过程。 HTTP协议：我们在TCP流通信中说明了，TCP协议实现了数据流的传输。然而，在实践中发现，人们往往习惯以文件为单位传输资源，比如文本文件，图像文件，超文本文档(hypertext document)。超文本文档中包含有超链接，指向其他的资源。 HTTP协议是应用层协议，它随着万维网发展起来。其本质目的是，如何在万维网的网络环境下，更好的使用TCP协议(尽管HTTP协议也可以用UDP协议作为底层，但绝大部分都是基于TCP协议)，以实现超文本文件的传输。 也就是说，HTTP实现数据的传输。 HTTP交互过程HTTP交互是典型的C/S架构。 HTTP协议的通信是一次request-responce交流。客户端(guest)向服务器发出请求(request)，服务器(server)回复(response)客户端。 通过上图一问一答式的交互，就完成了资源请求与资源响应。HTTP协议规定了请求和回复需要遵循的格式。请求和回复需要满足下面的格式: 12345起始行(start line)头信息(headers)主体(entity body) 请求的起始行表示“想要什么”。回复的起始行表示”响应的简要信息”。 头信息可以有多行。每一行是一对键值对(key-value pair) 主体部分包含了具体的资源，这是个GET请求 (请求是可以有主体内容的，但前期是POST请求)。回复中包含的主体是一段文本文字(Hello World!)。 请求报文：我们深入一些细节。先来看一下请求报文的详细情况： 12GET /index.html HTTP/1.1 # 起始行Host:www.example.com # 头信息 起始行 ： GET /index.html HTTP/1.1 声明是GET方法 头信息 ：Host:www.example.com 该头信息的名字是Host。HTTP的请求必须有Host头信息，用于说明服务器的地址和端口。HTTP协议的默认端口是80，如果在HOST中没有说明端口，那么将默认采取该端口。 响应报文：服务器在接收到请求之后，会根据程序，生成对应于该请求的回复，比如： 123HTTP/1.1 200 OK # 起始行Content-type: text/plain # 头部字段标识Hello World! # 响应内容 一、起始行： HTTP/1.1 协议版本 200 状态码(status code)。 OK 状态描述 OK是对状态码200的文字描述，电脑只关心三位的状态码(status code)，200 表示访问正常 二、头部字段标识：Content-type：说明了主体所包含的资源的类型。 三、响应内容：服务器响应的内容主要信息都存放于此。如果请求一个网页，它的响应体就是网页的HTML代码，或者是Json格式的数据；请求一张图片，它的响应体就是图片的二进制数据。 HTTP无状态根据早期的HTTP协议，每次request-reponse时，都要重新建立TCP连接。TCP连接每次都重新建立，所以服务器无法知道上次请求和本次请求是否来自于同一个客户端。因此，HTTP通信是无状态(stateless)的。服务器认为每次请求都是一个全新的请求，无论该请求是否来自同一地址。 详细的 HTTP 协议介绍 可以阅读 我之前的爬虫文章： DHCP协议：DHCP协议用于动态的配置电脑的网络相关参数，如主机的IP地址，路由器出口地址、DNS域名服务器地址等。一台电脑只要接上网，就可以通过DHCP协议获得相关配置（例如IP地址等配置信息） DHCP协议全称为“动态主机设置协议”（Dynamic Host Configuration Protocol）。 通常来说，普通电脑中都内置有DHCP客户端模块。电脑接上网络后，DHCP客户端发现新连通的网络，会在该网络上找DHCP服务器。DHCP服务器将给电脑提供合理的网络配置，并把设置信息传回本机。本机和DHCP服务器之间的通信，都是通过DHCP协议进行的。 地址分配DHCP服务器的首要任务是分配IP地址。分配的IP地址要符合以下原则： 地址合理，即对应该局域网的IP地址和子网掩码。 地址空闲，同一网络下没有其他设备使用该地址。 地址池DHCP服务器上存有一个地址池，里面是可用的IP地址，当有一台主机接入该局域网时，DHCP服务器就会从地址池中取出一个IP地址分配给主机。此外，服务器还会说明IP地址的占用时间，也就是“租期” 当然，主机使用网络的时间可能超过租期。如果主机在租期到时都没有联系DHCP服务器，那么DHCP服务器会收回IP地址，再分配给其他主机。可如果主机想继续使用IP地址，就要在中途申请延长租期。收到申请的DHCP服务器通常会答应主机的请求，允许它继续使用现有IP地址。 DHCP通信过程DHCP协议的底层是UDP协议。使用UDP的广播，把UDP数据包发送到网络的广播地址，网络上的每个设备都能收到。因此，DHCP通信主要靠这种广播的形式进行。 DHCP通信分为四步： 客户机发广播，搜寻DHCP服务器（一个网段不一定只有一台DHCP服务器）。 DHCP服务器发出Offer报文，提供一个可用的IP地址。 客户机携带该IP地址发出Request报文请求。 DHCP服务器回复ACK报文进行确认，并提供其他配置参数。 应用层通信应用层通信可以理解为两种情景： ①C/S架构（又称客户端/服务端） 有一个总打开的主机称为服务器，他服务于来自多方的客户端，服务器有个最大的特征，就是IP大多数情况下是固定的，客户机通过域名、IP进行访问服务（具有C/S架构特别典型的例子就是FTP、Telnet） 我们上面讨论的协议都是基于这个架构。 ②P2P架构（又称客户机/客户机） 这种通信不必通过专门的服务器，应用程序在间断连接的主机之间使用直接通信，该体系称为对等方到对等方的（典型的P2P架构的例子是文件共享）。 软件都是在应用层协议工作，多数应用程序是由通信进程对组成，每对中的两个进程互发报文。两个进程之间通过套接字（传输层和应用层的接口）的软件接口向网络发送报文和网络接收报文。 PS：更多对socket的了解可以移步我之前写的网络编程系列：网络编程Day2-socket原理]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day5-UDP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay5-UDP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[UDP协议简介UDP(User Datagram Protocol)传输与IP传输非常类似。你可以将UDP协议看作IP协议暴露在传输层的一个接口。UDP协议同样以数据包(datagram)的方式传输。 当应用程序对传输的可靠性要求不高，但是对传输速度和延迟要求较高时，可以用UDP协议来替代TCP协议在传输层控制数据的转发。UDP将数据从源端发送到目的端时，无需事先建立连接。UDP采用了简单、易操作的机制在应用程序间传输数据，没有使用TCP中的确认技术或滑动窗口机制，因此UDP不能保证数据传输的可靠性，也无法避免接收到重复数据的情况。 UPD的诞生：尽管UDP协议非常简单，但它的产生晚于更加复杂的TCP协议。早期的网络开发者开发出IP协议和TCP协议分别位于网络层和传输层，所有的通信都要先经过TCP封装，再经过IP封装(应用层-&gt;TCP-&gt;IP)。开发者将TCP/IP视为相互合作的套装。但很快，网络开发者发现，IP协议的功能和TCP协议的功能是相互独立的。对于一些简单的通信，我们只需要简单的IP传输就可以了，而不需要TCP协议复杂的建立连接的方式(如果过多的建立TCP连接，会造成很大的网络负担，而UDP协议可以相对快速的处理这些简单通信)。UDP协议随之被开发出来。 UDP运用场景：UDP适合传输对延迟敏感的流量，如语音和视频。在使用TCP协议传输数据时，如果一个数据段丢失或者接收端对某个数据段没有确认，发送端会重新发送该数据段。 让我们设想一下，平时打电话过程中，偶尔出现的信号不好的情况，此时，如果对方在说话，那么我们肯定是听不清楚的，而这些遗失的语音是不会重新出现的（除非你叫对方重新讲一遍） 上面的打电话案例其传输层协议就是使用的UDP，而不是TCP。UDP采用实时传输机制和时间戳来传输语音和视频数据。 使用UDP传输数据时，由应用程序根据需要提供报文到达确认、排序、流量控制等功能。 UDP说的东西不多，只要我们IP协议学的好，UDP并不是很大难题，重点还是TCP的熟练掌握。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day4-TCP协议]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay4-TCP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[传输层协议之TCP协议传输层最重要的协议为TCP协议和UDP协议。TCP协议复杂且面向连接，但传输可靠。UDP协议简单且不面向连接，传输不可靠。 TCP比较出众的一点就是提供一个可靠的，流控的数据传输。而传输层的出现有个重要的原因：端口。 IP协议进行的是IP地址到IP地址的传输，这意味者两台计算机之间的对话。但每台计算机中需要有多个通信通道，并将多个通信通道分配给不同的进程使用。一个端口就代表了这样的一个通信通道。网络层在逻辑上提供了端口的概念。一个IP地址可以有多个端口。一个具体的端口需要IP地址和端口号共同确定(我们记为IP:port的形式)。 TCP协议的两个特性：一、双向连接： TCP连接是双工(duplex)的。双向连接实际上就是建立两个方向的TCP传输。因此，主机与服务器的交互可能是双向的~ 二、TCP端口号 TCP允许一个主机同时运行多个应用进程。每台主机可以拥有多个应用端口，每对端口号、源和目标IP地址的组合唯一地标识了一个会话。端口分为知名端口和动态端口。 有些网络服务会使用固定的端口，这类端口称为知名端口，端口号范围为0-1023。动态端口号范围从1024到65535 TCP 头部： TCP通常使用IP作为网络层协议,这时TCP数据段被封装在IP数据包内。TCP数据段由TCP Header（头部）和TCP Data（数据）组成。TCP最多可以有60个字节的头部，如果没有Options字段，正常的长度是20字节。 TCP Header是由上图标识一些字段组成，下面是对字段的介绍： 16位源端口号：源主机的应用程序使用的端口号。 16位目的端口号：目的主机的应用程序使用的端口号。每个TCP头部都包含源和目的端的端口号，这两个值加上IP头部中的源IP地址和目的IP地址可以唯一确定一个TCP连接。 32位序列号：用于标识从发送端发出的不同的TCP数据段的序号。数据段在网络中传输时，它们的顺序可能会发生变化；接收端依据此序列号，便可按照正确的顺序重组数据。 32位确认序列号：用于标识接收端确认收到的数据段。确认序列号为成功收到的数据序列号加1。 4位头部长度：表示头部占32bit字的数目，它能表达的TCP头部最大长度为60字节。 16位窗口大小：表示接收端期望通过单次确认而收到的数据的大小。由于该字段为16位，所以窗口大小的最大值为65535字节，该机制通常用来进行流量控制。 16位校验和：校验整个TCP报文段，包括TCP头部和TCP数据。该值由发送端计算和记录并由接收端进行验证。 TCP 三次握手建立起一个TCP连接需要经过“三次握手”： 第一次握手：客户端发送请求建立连接报文（seq = x，同时SYN置1，表示第一次建立连接）到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到SYN包，回复客户的请求报文（seq =y，ack=x+1），同时回包，即SYN+ACK包（SYN、ACK置1，ACK置1表示同意建立连接），此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。 握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手” TCP四次挥手： 我们可以看到，连接终结的过程中，连接双方也交换了四片信息(两个FIN和两个ACK)。TCP并没有合并FIN与ACK片段。原因是TCP连接允许单向关闭(half-close)。 假设Client端发起中断请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说“我client端要发给你了”，但是如果你还没有数据要发送完成，则不必急着关闭Socket，可以继续发送数据。所以所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。 当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，”就知道可以断开连接了”。Client端等待了2MSL(最大报文段生存时间)后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。 Ok，TCP连接就这样关闭了！ “流”通信TCP协议是传输层协议，实现的是端口到端口(port)的通信。更进一步，TCP协议虚拟了文本流(byte stream)的通信。在TCP协议与”流”通信中讨论的TCP传输需要一个前提：TCP连接已经建立。 IP协议和UDP协议采用的是数据包的方式传送，后发出的数据包可能早到，我们并不能保证数据到达的次序。 TCP协议确保了数据到达的顺序与文本流顺序相符。当计算机从TCP协议的接口读取数据时，这些数据已经是排列好顺序的“流”了。比如我们有一个大文件要从本地主机发送到远程主机，如果是按照“流”接收到的话，我们可以一边接收，一边将文本流存入文件系统。这样，等到“流”接收完了，硬盘写入操作也已经完成。如果采取UDP的传输方式，我们需要等到所有的数据到达后，进行排序，才能组装成大的文件。 片段与编号我们知道了TCP是通过报文流进行通信的，那么同时发出的多个报文流，对端是如何按正确的顺序进行组装呢？ TCP片段的头部(header)会存有该片段的序列号(sequence number)。这样，接收的计算机就可以知道接收到的片段在原文本流中的顺序了，也可以知道自己下一步需要接收哪个片段以形成流。比如已经接收到了片段1，片段2，片段3，那么接收主机就开始期待片段4。如果接收到不符合顺序的数据包(比如片段5)，接收方的TCP模块可以拒绝接收，从而保证呈现给接收主机的信息是符合次序的“流”。 TCP 可靠性 TCP的可靠性是这么体现的：在每收到一个正确的、符合次序的片段之后，就向发送方(也就是连接的另一段)发送一个特殊的TCP片段，回复一个ACK给发送方(ACK，acknowledge)：“我已经收到那个片段了。” 这个特殊的TCP片段叫做ACK回复。如果一个片段序号为L，对应ACK回复有回复号L+1，也就是接收方期待接收的下一个发送片段的序号。如果发送方在一定时间等待之后，还是没有收到ACK回复，那么它推断之前发送的片段一定发生了异常。发送方会重复发送(retransmit)那个出现异常的片段，等待ACK回复，如果还没有收到，那么再重复发送原片段… 直到收到该片段对应的ACK回复(回复号为L+1的ACK)。 当发送方收到ACK回复时，它看到里面的回复号为L+1，也就是发送方下一个应该发送的TCP片段序号。发送方推断出之前的片段已经被正确的接收，随后发出L+1号片段。ACK回复也有可能丢失。对于发送方来说，这和接收方拒绝发送ACK回复是一样的。发送方会重复发送，而接收方接收到已接收过的片段，推断出ACK回复丢失，会重新发送ACK回复。 通过ACK回复和重新发送机制，TCP协议将片段传输变得可靠。但我们也看出来了—— 这也太麻烦了吧！ 而TCP 的连接是“可靠的”，这一点还体现在 滑动窗口机制和重传机制上。 TCP 滑动窗口机制：滑窗：上面的工作方式中，发送方保持发送-&gt;等待ACK-&gt;发送-&gt;等待ACK…的单线工作方式，这样的工作方式叫做stop-and-wait。stop-and-wait虽然实现了TCP通信的可靠性，但同时牺牲了网络通信的效率。在等待ACK的时间段内，我们的网络都处于闲置(idle)状态。我们希望有一种方式，可以同时发送出多个片段。然而如果同时发出多个片段，那么由于IP包传送是无次序的，有可能会生成乱序片段。 在stop-and-wait的工作方式下，乱序片段完全被拒绝（序列号对不上，会等待重发），这也很不效率。毕竟，乱序片段只是提前到达的片段。我们可以在缓存中先存放它，等到它之前的片段补充完毕，再将追加在后面。 然而，如果一个乱序片段实在是太过提前(太“乱”了)，该片段将长时间占用缓存。我们需要一种折中的方法来解决该问题：利用缓存保留一些“不那么乱”的片段，期望能在段时间内补充上之前的片段(暂不处理，但发送相应的ACK)；对于“乱”的比较厉害的片段，则将它们拒绝(不处理，也不发送对应的ACK)。 这个机制就是——窗口机制 窗口机制TCP滑动窗口技术可以通过动态改变窗口大小来实现对端到端设备之间的数据传输进行流量控制。 滑窗(sliding window)被同时应用于接收方和发送方，以解决以上问题。发送方和接收方各有一个滑窗。当片段位于滑窗中时，表示TCP正在处理该片段。滑窗中可以有多个片段，也就是可以同时处理多个片段。滑窗越大，越大的滑窗同时处理的片段数目越多(当然，计算机也必须分配出更多的缓存供滑窗使用)。 流量控制主机和服务器之间可以通过滑动窗口来实现流量控制。 TCP协议会根据情况自动改变滑窗大小，以实现流量控制。流量控制(flow control)是指接收方将 window的大小通知给发送方，从而指导发送方修改发送 window的大小。接收方将该信息放在TCP头部的window size区域：发送方在收到window size的通知时，会调整自己滑窗的大小。这样，发送窗口变小，文本流发送速率降低，从而减少了接收方的负担。 TCP重传机制：超时重传机制超时重传机制是指：当发送方送出一个TCP片段后，将开始计时，等待该TCP片段的ACK回复。如果接收方正确接收到符合次序的片段，接收方会利用ACK片段回复发送方。发送方得到ACK回复后，继续移动窗口，发送接下来的TCP片段。如果直到计时完成，发送方还是没有收到ACK回复，那么发送方推断之前发送的TCP片段丢失，因此重新发送之前的TCP片段。这个计时等待的时间叫做重新发送超时时间(RTO, retransmission timeout)。 快速重传机制快速发送机制如果被启动，将打断计时器的等待，直接重新发送TCP片段。 快速重传机制：实现了另外的一种丢包评定标准，即如果连续收到3次 ACK，发送方就认为这个seq的包丢失了，立刻进行重传，这样如果接收端回复及时的话，基本就是在重传定时器到期之前，提高了重传的效率。 TCP的可靠机制虽然保证了数据安全，但是往往也带来某些方面的缺陷：例如TCP慢启动和TCP全局同步现象。 TCP的缺陷：一个概念——慢启动：TCP连接刚建立时，不是一下子就能把握TCP窗口值大小的，会从一个小的窗口值慢慢往上加，最后协商成功，达到稳定的windows大小。 TCP全局同步：当网络发生堵塞时，报文就不能及时送达目的端。对于TCP报文，如果大量的报文被丢弃，将造成TCP超时，从而引发TCP慢启动，使得TCP减少报文的发送。当队列同时丢弃多个TCP连接的报文时，将造成多个TCP连接同时进入拥塞避免和慢启动状态以调整并降低流量，这就被称为TCP全局同步现象。这样多个TCP连接发往队列的报文将同时减少，而后又会在某个时间同时出现流量高峰，如此反复，使网络资源利用率低。 资源消耗：TCP 最大的缺点是那些”确认码”数据包把数量翻了一倍，但并没有传输更多信息。有时候这种代价是不值得的，于是有了UDP这个协议来代替一些用户的其他需求（下一篇我们会讲到） 这一章节我不知不觉就写了四千字（捂脸），可见TCP之重要性… 刚开始学TCP的时候，我还没有意识到问题的严重性…hhh 这几天写的博客有点多，因为热爱，所以无惧！ 当回过头来看这些文章时，更多的是一种自豪感，我会继续坚持下去的。Fighting！]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day3-网络层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay3-%E7%BD%91%E7%BB%9C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[网络层：网络层(network layer)是实现互联网最重要的一层。正是在网络层面上，各个局域网根据IP协议相互连接，最终构成覆盖全球的Internet。更高层的协议，无论是TCP还是UDP，必须通过网络层的IP数据包(datagram)来传递信息。操作系统也会提供该层面的套接字(socket)，从而允许用户直接操作IP包。 IP数据包： IP数据包简称为IP包。它是符合IP协议的0/1序列。信息包含在这一序列中。IP包分为头部(header)和数据(Data)两部分。数据部分是要传送的信息，头部是为了能够实现传输而附加的信息。 与帧类似，IP包的头部也有多个区域。我们将注意力放在源地址(source address)和目的地(destination address)。它们都是IP地址。IPv4的地址为4 bytes的长度(也就是32位)。我们通常将IPv4的地址分为四个十进制的数，每个数的范围为0-255,比如192.0.0.1就是一个IP地址。填写在IP包头部的是该地址的二进制形式。 IP地址是全球地址，它可以识别局域网和主机。这是通过将IP地址分类实现的。下图则为 IP 地址分类： 每个IP地址的32位分为前后两部分，第一部分用来区分局域网，第二个部分用来区分该局域网的主机。 子网掩码(Subnet Mask)告诉我们这两部分的分界线，比如255.0.0.0(也就是8个1和24个0)表示前8位用于区分局域网，后24位用于区分主机。由于A、B、C分类是已经规定好的，所以当一个IP地址属于B类范围时，我们就知道它的前16位和后16位分别表示局域网和主机。 网卡与路由器网卡（NIC）： IP地址是分配给每个计算机的。但这个说法并不精确。IP地址实际上识别的是网卡(NIC, Network Interface Card)。 网卡是计算机的一个硬件，它在接收到网路信息之后，将信息交给计算机的CPU处理。当计算机需要发送信息的时，也要通过网卡发送。一台计算机可以有不只一个网卡，比如笔记本就有一个以太网卡和一个WiFi网卡。计算机在接收或者发送信息的时候，要先决定想要通过哪个网卡。 路由器： 路由器(router)实际上就是一台配备有多个网卡的专用电脑。它让网卡接入到不同的网络中。他专门负责与多个网络之间的寻址与通信。跨网段的数据通信一般都会经过路由器 路由寻址：IP包的传输要通过路由器的接力。每一个主机和路由中都存有一个路由表(routing table)。路由表根据目的地的IP地址，规定了等待发送的IP包所应该走的路线。 始发：比如我们从主机145.17生成发送到146.21的IP数据包，剩下数据部分可以是TCP包，可以是UDP包，我们暂时不关心。注明目的地IP地址(199.165.146.21)和发出地IP地址(199.165.145.17)。主机145.17随后参照自己的路由表，看到路由表中的记录： 145.17 routing table (Genmask为子网掩码,Iface用于说明使用哪个网卡接口) 这里有两行记录： 第一行，如果IP目的地是199.165.145.0这个网络的主机，那么只需要自己在eth0上的网卡直接传送(“本地社区”：直接送达)，不需要前往路由器(Gateway 0.0.0.0 = “本地送信”)。 第二行，所有不符合第一行的IP目的地，都应该送往Gateway 199.165.145.17，也就是中间路由器接入在eth0的网卡IP地址。因为 目的地址和子网掩码全为 0.0.0.0 是缺省路由（当匹配不到路由明细时，就会往这个接口发出） 中转我们的IP包目的地为199.165.146.21，不符合第一行，所以按照第二行，发送到中间的路由器。主机145.17会将IP包放入帧的payload，并在帧的头部写上199.165.145.17对应的MAC地址，这样，就可以在局域网中传送了。 中间的路由器在收到IP包之后(实际上是收到以太协议的帧，然后从帧中的payload读取IP包)，提取目的地IP地址，然后对照自己的路由表： 送达：数据包的目的地符合第二行，所以将IP放入一个新的帧中， 在帧的头部写上199.165.146.21的MAC地址，直接发往主机146.21。 总结： 这样，我们就完成了“路由”的过程。整个过程中，IP包不断被主机和路由封装入帧并拆开，然后借助连接层，在局域网的各个NIC之间传送帧。整个过程中，我们的IP包的内容保持完整，没有发生变化（有变化的是TTL值、FCS校验码等字段）。最终的效果是一个IP包从一个主机传送到另一个主机。利用IP包，我们不需要去操心底层(比如连接层)发生了什么。 路由协议：之前的信息传递基于一个假设：每个人手里都有份准确的地图。用计算机的话来说，就是每个主机和路由上都已经有一张路由表。这个路由表描述了网络上的路径信息。如果你了解自己的网络连接，可以手写自己主机的路由表。但是，一个路由器可能有多个出口，所以路由表可能会很长。更重要的是，周围连接的其他路由器可能发生变动(比如新增路由器或者拓扑变更)，我们就需要路由表能及时将交通导向其他的出口。我们需要一种更加智能的探测周围的网络拓扑结构，并自动生成路由表—— 路由协议 一些比较出名的路由协议有： OSPF、BGP、ISIS 等 这里不过多介绍，不然可以讲一天 =.=]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day2-数据链路层]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay2-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82%2F</url>
    <content type="text"><![CDATA[以太网的帧格式帧本身是一段有限的0/1序列。它可以分为以下几部分： Preameble 和 SFD： 帧的最初7个字节被称为前导码，前导码是包括 七个字节的二进制0 1交替的代码 共56位。 定界符是由一个字节的01 交替的 二进制序列 ，他的作用是使接收端对数据帧的第一位进行定位 通常，我们都会预定好以一定的频率发送0/1序列(比如每秒10bit)。如果接收设备以其他频率接收(比如每秒5bit)，那么就会错漏掉应该接收的0/1信息。但是，由于网卡的不同，发送方和接收方即使预订的频率相同，两者也可能由于物理原因发生偏差。这就好像两个人约好的10点见，结果一个人表快，一个人表慢一样。前导码是为了让接收设备调整接收频率，以便与发送设备的频率一致。 DST 和 src 紧跟着后面的是6 字节的目的地（DST）和6 字节的源地址（src）。要注意，这里写的是MAC地址。MAC地址是物理设备自带的序号，只能在同一个以太网中被识别。而不同局域网的通信，则要在网络层协议中解决。 Data 数据一般包含有符合更高层协议的数据，比如IP包。连接层协议本身并不在乎数据是什么，它只负责传输。注意，数据尾部可能填充有一串0(PAD区域)。填充0的原因是，一个帧需要超过一定的最小长度。 数据的字节长限制为 64～1500字节： 最大包长1518字节，其中数据1500字节为MTU，18字节为字段值 最小包长64字节，为了限制冲突，有足够时间检测到冲突（CSMA/CD） Type字段： type字段两个字节，它用于区分两种帧类型，当type字段值小于1500，则是IEEE802.3格式。当type字段值大于或等于1536，帧使用的是以太Ethernet格式。0×0800标识IP，0×0806标识ARP) FCS效验码 接着是校验序列(FCS, Frame Check Sequence)。校验序列是为了检验数据的传输是否发生错误。在物理层，我们通过一些物理信号来表示0/1序列(比如高压/低压，高频率/低频率等)，但这些物理信号可能在传输过程中受到影响，以致于发生错误。 FCS采用了循环冗余校验(CRC, Cyclic Redundancy Check)算法。利用特定的算法得到的余数与实际拿到数据包的FCS进行比较， 如果两者不相符，我们就知道数据在传输的过程中出现错误，不能使用。 链路层的通信依据就是通过以上字段，它控制我们如何在局域网中的转发通信。 以上的数据包封装需要MAC 地址，本端的MAC地址我们知道（由厂商分配写好），那么对端的MAC地址我们如何得知呢？ ARP协议ARP协议介于连接层和网络层之间，ARP包需要包裹在一个帧中。它的工作方式如下：主机会发出一个ARP包，该ARP包中包含有自己的IP地址和MAC地址。通过ARP包，主机以广播的形式询问局域网上所有的主机和路由：我是IP地址xxxx，我的MAC地址是xxxx，有人知道199.165.146.4（目的端IP地址）的MAC地址吗？拥有该IP地址的主机会回复发出请求的主机：哦，我知道，这个IP地址属于我的一个NIC，它的MAC地址是xxxxxx。由于发送ARP请求的主机采取的是广播形式，并附带有自己的IP地址和MAC地址，其他的主机和路由会同时检查自己的ARP cache，如果不符合，则更新自己的ARP cache。 这样，经过几次ARP请求之后，ARP cache会达到稳定。如果局域网上设备发生变动，ARP重复上面过程。 总的来说： APR协议就是通过IP 地址请求对端的MAC地址的过程。 免费arp ： 主机被分配IP地址或者IP地址发生变更之后，检测该网段是否唯一，避免冲突。主机用过发送一个arp request 报文进行检测，将广播报文中目的ip地址设置为自己的IP地址。当收到其他主机的回复时，则表示该网段被占用。 ICMP协议：ICMP(Internet control message protocol)是网络层的的协议，用来在网络设备间传递各种差错和控制信息，他对于收集各种网络信息，诊断和排除各种网络故障具有至关重要的作用。 ICMP重定向，ICMP差错检查:用于诊断源和目的之前的连通性(ICMP echo request查询和ICMP echo reply响应)，ICMP错误报告(各种传输数据失败的原因) ICMP基于IP协议。也就是说，一个ICMP包需要封装在IP包中，然后在互联网传送。ICMP是IP套装的必须部分，也就是说，任何一个支持IP协议的计算机，都要同时实现ICMP。 常见的ICMP包类型目的端不可达目的地无法到达(Destination Unreachable)属于错误信息。如果一个路由器接收到一个没办法进一步接力的IP包，它会向出发主机发送该类型的ICMP包。比如当IP包到达最后一个路由器，路由器发现目的地主机down机，就会向出发主机发送目的地无法到达(Destination Unreachable)类型的ICMP包。目的地无法到达还可能有其他的原因，比如不存在路由表项，比如不被接收的端口号等等。 重定向重新定向(redirect)属于错误信息。当一个路由器收到一个IP包，对照其routing table，发现自己不应该收到该IP包，它会向出发主机发送重新定向类型的ICMP，提醒出发主机修改自己的routing table（“我”不是你的目标，请更换目的地址） ICMP 的两个运用： Ping 和 Ttracertping可以检测网络的连通性，这一点运用很广。ping命令就是利用了该类型的ICMP包。当使用ping命令的时候，将向目标主机发送Echo-询问类型的ICMP包，而目标主机在接收到该ICMP包之后，会回复Echo-回答类型的ICMP包，并将询问ICMP包包含在数据部分。ping命令是我们进行网络排查的一个重要工具。如果一个IP地址可以通过ping命令收到回复，那么其他的网络协议通信方式也很有可能成功。 tracert 用于测试链路的跳数：IPv4中的Time to Live(TTL)会随着经过的路由器而递减，当这个区域值减为0时，就认为该IP包超时(Time Exceeded)。Time Exceeded就是TTL减为0时的路由器发给出发主机的ICMP包，通知它发生了超时错误。traceroute命令用来发现IP接力路径(route)上的各个路由器。它向目的地发送IP包，第一次的时候，将TTL设置为1，引发第一个路由器的Time Exceeded错误。这样，第一个路由器回复ICMP包，从而让出发主机知道途径的第一个路由器的信息。随后TTL被设置为2、3、4，…，直到到达目的主机。这样，沿途的每个路由器都会向出发主机发送ICMP包来汇报错误。traceroute将ICMP包的信息打印在屏幕上，就是路径的信息了。 集线器(Hub) vs. 交换器(Switch)二层设备集线器： 以太网使用集线器或者交换器将帧从发出地传送到目的地。一台集线器或交换器上有多个端口，每个端口都可以连接一台计算机(或其他设备)。 集线器像一个广播电台。一台电脑将帧发送到集线器，集线器会将帧转发到所有其他的端口。每台计算机检查自己的MAC地址是不是符合DST。如果不是，则保持沉默。集线器是比较早期的以太网设备。它有明显的缺陷： 任意两台电脑的通信在同一个以太网上是公开的。所有连接在同一个集线器上的设备都能收听到别人在传输什么，这样很不安全。可以通过对信息加密提高安全性。 不允许多路同时通信。如果两台电脑同时向集线器发信，集线器会向所有设备发出“冲突”信息，提醒发生冲突。可以在设备上增加冲突检测算法：一旦设备发现有冲突，则随机等待一段时间再重新发送（CSMA/CD） 交换机： 交换器克服集线器的缺陷。交换器记录有各个设备的MAC地址。当帧发送到交换器时，交换器会检查DST，然后将帧只发送到对应端口。交换器允许多路同时通信。由于交换器的优越性，交换器基本上取代了集线器。 多数分组交换机在链路的输入端使用存储转发传输机制。向目的端发送报文时，源将报文分为多个较小的数据块，称为分组，提高了灵活性和转发速率。到达目的端后，在根据偏移量等字段控制其组合成一个完整的报文。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识之操作系统]]></title>
    <url>%2F2019%2F07%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8B%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[操作系统的诞生现代计算机系统是由一个或者多个处理器，主存，磁盘，打印机，键盘，鼠标显示器，网络接口以及各种其他输入输出设备组成的复杂系统，每位程序员不可能掌握所有系统实现细节，并且管理优化这些部件是一件挑战性极强的工作。所以，我们需要为计算机安装一层软件，也就是操作系统。操作系统的任务就是用户程序提供一个简单清晰的计算机模型，并管理以上所有设备。 操作系统所处位置图例： 我们将整个计算机分为硬件和软件，操作系统充当软件和硬件之间的媒介 操作系统使得程序员可以用标准化机制 和 输入输出硬件（I/O）交互。还有就是说，程序员只负责编写程序，如何调用、处理程序的任务就交给了操作系统 总结：操作系统是一个用来协调、管理和控制计算机硬件和软件资源的系统程序，其中包括：文件系统、内存管理、设备管理和进程管理。它位于硬件和应用程序之间。需要注意的是，操作系统本质上也是一个软件，它由硬件上的程序进行调用运行。 CPU的资源利用我们知道，CPU的运算是很快的，而 I/O、打印机之类的设备运行很慢，这导致了一个后果，CPU闲置（程序阻塞在I/O上，这很浪费资源） 我们想实现的是：当cpu遇到阻塞时，它会把程序休眠，切换到下一个程序，等到阻塞的任务完成了，再回过头来进行运行处理。 所以我们需要有一种方式来最大限度的利用它，这样使得多个程序可以同时运行，在单个CPU上共享时间——“多任务处理” 内存地址的分配：每个程序都会占用一些内存，当切换到另一个程序时，我们不能丢失数据，需要记录当前状态，以方便回过头来处理程序时不会丢失数据。 解决办法就是： 给每个程序分配专属内存块 把程序A分配到内存地址0~999 把程序B分配到内存地址1000~1999 分配内存地址面临的一个问题： 如果一个程序请求更多内存，操作系统会决定是否同意，以及决定分配具体哪些内存块。这样虽然灵活性很好，但是会遇到个问题，由于更多的内存块是后续申请的，同个程序的内存块可能分配到非连续的情况。 虚拟内存其实真正的程序可能会分配到内存中数十个地方，这对于程序员来说很难跟踪，为了隐藏这种复杂性，操作系统会把内存进行“虚拟化”—— 称为虚拟内存，程序可以假定内存总是从地址0开始，而实际物理地址，被操作系统隐藏和抽象了。 如上图所示，此时程序B的虚拟内存地址为0~999，而实际的物理地址，却是1000~1999。 什么意思呢？或者说，这么做的好处是什么？ 有了虚拟内存之后，程序员不用关心实际的内存地址，操作系统会自动处理虚拟内存和物理内存之间的映射。 这种功能对于程序A的情况更为有用： 程序A的物理内存是不连续的，而从程序A的角度上看，它是连续的（虚拟内存） 这种机制使程序的内存大小可以灵活增减，叫“动态内存分配”，它的简化，为操作系统同时运行多个程序提供了更大的灵活性 内存保护给程序分配专用的内存范围，还有另外一个好处： “程序之间的隔离” 程序与程序直接互相独立，互不影响。内存保护是操作系统对计算机上的内存进行访问权限管理的一个机制。内存保护的主要目的是限制某个进程访问的只是操作系统配置给它的地址空间。这个机制可以防止某个行程，因为某些程序错误或问题，而有意或无意地影响到其他进程或是操作系统本身的运行状态和数据。 这也是程序与程序之间不能直接通信的原因。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络Day1-TCP/IP协议]]></title>
    <url>%2F2019%2F07%2F27%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9CDay1-TCP-IP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[引言：感谢！计算机网络可以说是一门计算机的必修课吧，私以为，计算机网络和计算机组成原理是一名计算机专业学生最应该先学习的知识~ 于是我决定展开对于这门学科一些知识总结与感悟。 在这里要感谢一名老师，在刚踏上大学这段旅途的时候，余老师像一盏明灯一样，使得我面临计算机这个钢铁巨兽面前不会感到手足无措，带我踏进了计算机这个门槛、揭开了计算机的奥妙。 实例引入：计算机之所以能够通信，是因为其遵守了相约定好的协议(protocol)。就像人与人之间对话所使用的语言一样，语言不同，自然也就不能实现“通信”。于是，协议这种约定俗成的东西就诞生了。 协议控制因特网中信息的接收和发送。将复杂的计算机通信过程分为多层，提供了结构化的形式。每层之间互相独立，只需实现其所在层的功能即可，层与层之间提供接口与服务。 说到协议，就不得不提大名鼎鼎的TCP/IP了。 TCP/IP 模型： 下面的 OSI 七层模型是最早的协议，但由于某些原因，他更多的是一种标准的形式存在，没有实际推广（有兴趣的同学可以自行百度~） TCP/IP 四层模型的网络接口层又被人拆分为 数据链路层和 物理层。（又叫TCP 五层模型） 下面我们针对TCP/IP的各层进行简单介绍 物理层所谓的物理层，是指光纤、电缆或者电磁波等真实存在的物理媒介。这些媒介可以传送物理信号，比如电信号、电压。对于数字应用来说，我们只需要两种物理信号来分别表示0和1，比如用高电压表示1，低电压表示0，就构成了简单的物理层协议。针对某种媒介，电脑可以有相应的接口，用来接收物理信号，并解读成为0/1序列。 数据链路层息以帧(frame)为单位传输。信息是一段有序的0/1序列，而帧，是这个序列中符合特定格式的一小段。连接层协议的功能就是识别0/1序列中所包含的帧。并且负责 物理层和网络层之间的通信。 帧中，有源地址和目的地址，还有能够探测错误的校验序列(FCS)。当然，帧中最重要的最重要是所要传输的数据 通过连接层协议，我们可以建立局域的以太网或者WiFi局域网，并让同一局域网中的两台计算机通信。 网络层数据链路层使得同一个局域网的人可以互相通信，那么不处于同一个局域网如何通信呢？我们需要一个“中间人”。这个“中间人”必须有以下功能: 能从物理层上在两个网络的接收和发送0/1序列， 能同时理解两种网络的帧格式。 通过网络层的交换和选路，最终使得两个远端设备能够通信，这就是网络层的用处了。 路由网关： 路由器(router)就是为此而产生的“中间人”，也称为“网关”。一个路由器有多个网卡(NIC，Network Interface Controller)。每个网卡可以接入到一个网络，并理解相应的连接层协议。在帧经过路由到达另一个网络的时候，路由会读取帧的信息，并改写以发送到另一个网络。 两个不同局域网的计算机是这么通信的： 主机发出数据 -&gt; 发现与对端主机处在不同个局域网 -&gt; 数据交给网关进行转发 -&gt; 送达目的端计算机 而实际情况比这更加复杂，上述内容省略了ARP、ICMP、路由等步骤 在数据链路层，一个帧中只能记录目的地址和源地址两个地址。而完成上面的过程需要经过四个地址(计算机1，网关地址，以太网接口，计算机2)，因此，这也是 IP协议的主要功能——寻址。 传输层上面的三层协议让不同的计算机之间可以通信。但计算机中可能运行了许多个进程，每个进程都可能有自己的通信需求。我们使用网络接口层、网络层实现了主机与主机之间的通信，可是， 软件与软件之间的通信该如何解决呢？ 没错，就是传输层要做的事情了。 端口号： 计算机有多个进程，那么我们如何辨别是某个进程发出的信息请求呢？ 比如我们打开firefox浏览网页，与此同时，又用QQ来接收邮件。我们需要有一个标志来识别，也就是端口号啦。 我们知道IP层的ip地址可以唯一标识主机，而端口号可以唯一标示主机的一个进程。 TCP 和 UDP Internet 在传输层有两种主要的协议：一种是面向连接的协议 TCP ，一种是无连接的协议 UDP。利用端口号，一台主机上多个进程可以同时使用 TCP/UDP 提供的传输服务，并且这种通信是端到端的，它的数据由 IP 传递，但与 IP 数据报的传递路径无关。 应用层应用层协议定义了运行在不同端系统上的应用程序进程如何相互传递报文。向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。 运行在一个端系统的应用程序，怎样才能指令因特网向运行在另一个端系统上的软件发送数据呢？ 与因特网相连的端系统提供了一个应用程序接口（API），API往往规定了运行在一个端系统上的软件向另一个端系统上特定目的地软件交互数据的方式。可以说，因特网API是一套数据发送软件必须遵循的规则集合。 应用层常用服务： 常用服务 协议 端口号 HTTP TCP 80 FTP TCP 20、21 Telnet TCP 23 ​ 以上就是这篇文章的全部内容，我们只是介绍了个大概的框架，具体内容会在以后进行详细介绍。 还有一点要说的是，计算机网络是基础，大多数内容都是理论。这些知识点都是需要牢牢记住的~ 因为听说面试大多数都会问到 TCP 如何建立连接、HTTP 的内部实现 等等一些理论性知识点。]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day7-Mysql的那些事]]></title>
    <url>%2F2019%2F07%2F26%2F%E7%88%AC%E8%99%ABDay7-Mysql%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[前言：爬虫爬取到的数据最终是要保存下来滴~ 别以为爬下来就完事了，具体流程还有数据去重，数据分析，最后数据可视化等等操作呢… 这一章节，我们聊聊 “数据存储” 在 Python2 中，连接 MySQL 的库大多是使用 MySQLDB，但是此库官方并不支持 Python3，所以在这里推荐使用的库是 PyMySQL。 环境准备： 安装好PyMysql库 本地mysql环境的部署 PyMysql使用介绍：获取连接对象我们要连接到 mysql 这个库，首先应该获取一个connect 对象（输入本地部署的Mysql参数） 1234import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)# 通过 PyMySQL 的 connect() 方法声明了一个 MySQL 连接对象 获取操作游标紧接着，需要实例化一个操作游标对象。 12345import pymysqldb = pymysql.connect(host ='localhost',user ='root',password ='root',port=3306)cursor=db.cursor()# 注意：cursor 对象是基于db 这个connection对象实例化出来的 cursor 则为mysql 里面的 操作游标，这个概念和光标有些类似。语句的执行操作，都是依靠操作游标的。 我们拿到操作游标后，就很容易了。因为python执行的 sql 语句跟 mysql 里面是一样的…但还是有一丢丢不同的，让我们接着往下看。 插入数据 注意我们这里使用了异常处理，因为在PyMysql 里面是采用事务这个概念进行操作的 插入数据需要使用 commit 进行数据提交；当捕捉到错误时，db对象进行 rollback 事件回滚（只针对插入、更新、删除这些对数据库进行更改的操作，事件回滚就是：要么执行成功，要么不执行） 还有一点要注意的是：里面的参数，不管什么类型，统一使用%s作为占位符 动态插入数据：有时候我们面临的实际情况，你不知道有多少字段插入（上面的情况是我们”写死“的），可以说，插入的字段是不确定因素的，我们应该学会根据实际情况去构造 sql 语句。 在很多情况下，我们要达到的效果是插入方法无需改动，做成一个通用方法，只需要传入一个动态变化的字典给就好了。比如我们构造这样一个字典： 上面这条执行语句， 就替代之前那条 很 low 的写法，这么一来，无论字典有多少个键值对，我们都能进行插入。 数据查询：是的，对于数据库的增删改查，什么最重要？ （大声告诉我！）—— 没错！ 就是查询！！ 旁白君吐槽：“自嗨型作者…” 于是，旁白君大手一挥，说：“这个还不简单，我最拿手了。”于是抢走了作者的键盘，刷刷的写下这么一串代码 12345678910111213import pymysqldb = pymysql.connect(host='localhost',port=3306,user='root',password='root',db='spiders')cursor = db.cursor()table = 'students'sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: results = cursor.execute(sql) print(results)except Exception as e: print(e) 但是，结果却是这样的… what？ 这是什么鬼东西，我的数据呢？？ 作者邪魅一笑，眼中三分不屑三分冷漠四分讥笑，慢吞吞的拿走旁白君的键盘，自信的啪啪啪添加上了这么几条代码: 12345678910sql = 'select * from &#123;table&#125; WHERE age &gt;= 20'.format(table=table)try: cursor.execute(sql) results = cursor.fetchall() # fetchall（）方法，获取查询数据的返回值 for row in results: print(row)except Exception as e: print(e) 看到输出的结果，作者在旁白君羡慕的眼神中满意的点了点头 旁白君对着图仔细看了看说：“刚好四条！原来我刚才输出的4是执行成功的条目数！” “没错，悟性还挺高！” 作者留下潇洒离去的背影，并甩给旁白君一本秘籍——“数据查询的方法” 数据查询的方法： 使用select语句查询后不会直接返回查询的具体数据，而是返回执行成功的条目数，我们需要使用fetchone或者fetchall方法可以将数据读取出来。 fetchone() 方法，这个方法可以获取结果的第一条数据，返回结果是元组形式，元组的元素顺序跟字段一一对应，也就是第一个元素就是第一个字段 id，第二个元素就是第二个字段 name，以此类推。 fetchall() 方法，它可以得到结果的所有数据，然后将其结果和类型打印出来，它是二重元组，每个元素都是一条记录。我们将其遍历输出，将其逐个输出出来。 我们还可以用 while 循环加 fetchone() 的方法来获取所有数据，而不是用 fetchall() 全部一起获取出来，fetchall() 会将结果以元组形式全部返回，如果数据量很大，那么占用的开销会非常高。所以推荐使用如下的方法来逐条取数据 总结这一篇主要是理解如何连接数据库、事务的概念（默认开启事务），以及使用对数据库对字典形式的数据进行操作。然后 使用sql语句进行增删改查（主要是动态数据、插入更新数据部分） 在这一期皮了一下，希望大家喜欢，啊哈哈。 各位，我们下期再见！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day6-Beautiful介绍]]></title>
    <url>%2F2019%2F07%2F24%2F%E7%88%AC%E8%99%ABDay6-BeautifulSoup%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[之前的urllib实战中我们谈到了BeautifulSoup这个解析库，那么…是不是应该来一篇详细介绍呢？ 今天，他来了！！ BeautifulSoup的基本使用：拿到网页数据：一般来说，我们使用解析库的前提，已经是通过请求库拿到了网页的数据，假设现在由下图的 html 变量存储的数据正是我们请求的文本内容，下面我们将使用BeautifulSoup进行加工解析。 获取Soup对象：使用BeautifulSoup方法，第一个参数传入 HTML字符串文本，第二个参数传入的是解析器的类型，在这里我们使用lxml。不指定会使用默认的解析器，但是会发出警告 123from bs4 import Beautifulsoupsoup = BeautifulSoup(html,"lxml")print(soup) 运行后，发现内容如下（发现不完整的HTML字符串被补齐了）： 属性选择调用 soup 的各个方法和属性（选择器）对这串HTML代码解析。这个soup对象支持 “.” 获取节点操作，也可以通过”.”表达嵌套关系。 我们直接拿到的soup打印不美观，使用prettify（）方法可以打印出树形结构的 html 文本 这里我们打印 title 的文本内容，以及 head 节点信息 上面就是BeautifulSoup的使用流程，有时候，我们需要的内容存放于某些节点中，这些节点有它的特征，我们可以通过选择器获取这些节点，最后拿到我们需要的结果。 1对于某些节点，我们使用嵌套选择来选中它非常麻烦，那么，就有了节点选择器，根据其CSS编写的 类、id 这样的属性进行选择。Beautifulsoup 很快就能找出匹配的标签。 节点选择器：选择节点：直接通过调用节点的名称就可以选择节点元素了，这种选择方式速度非常快，如果单个节点结构话层次非常清晰，可以选用这种方式来解析。 注意，我们的选择器是基于soup这个对象的 输出了soup.head的类型，是 bs4.element.Tag 类型.这是 BeautifulSoup 中的一个重要的数据结构，经过选择器选择之后，选择结果都是这种 Tag 类型，它具有着一些属性，比如 string 属性（输出文本信息），可以进行获取属性、嵌套等操作。 不过这种选择方式只会选择到第一个匹配的节点，其他的后面的节点都会忽略——比如我们打印的p节点，只呈现一个信息（html字符串中有三个p节点信息） 获取属性值：每个节点可能有多个属性值，比如id，class 等等，我们选择到这个节点元素之后，可以调用 attrs 获取所有属性。 可以看到 attrs 的返回结果是字典形式，把选择的节点的所有属性和属性值组合成一个字典。因此，可以通过键值形式进行取值传入节点的属性，获取该节点的属性值 contents 属性： 选取到了一个节点元素之后，如果想要获取它的直接子节点的信息 （不包含子孙节点），可以调用contents 属性（和直接选择不同的是，contents返回的列表包含了所有子节点和所有文本信息） 关联选择：我们在做选择的时候有时候不能做到一步就可以选择到想要的节点元素，有时候在选择的时候需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等等。这里列出了获取节点的各个方法： Children 查询某个节点的直接子节点（返回一个迭代器） Descendants 查询某个节点的所有子孙节点（返回生成器） parent 查询某个节点的元素的父节点 Parents 查询某个节点的元素的祖父节点（返回生成器） soup.a.next_sibling 查询某个节点的下一个节点 previous_sibling 查询某个节点的上一个节点 next_siblings 和 previous_siblings 分别返回所有前面和后面的兄弟节点的生成器。 需要注意的是：上面这些节点选择，但凡涉及到多个节点内容，就会返回一个生成器类型。 关于这些节点选择，我们要注意 html 树型结构的排列关系，例如，我们不能跨越父级节点，去拿到另一个子孙节点的内容。 调用 API前面我们所讲的选择方法都是通过属性来选择元素的，这种选择方法非常快，但是如果要进行比较复杂的选择的话则会比较繁琐，不够灵活。所以 BeautifulSoup 还为我们提供了一些查询的方法，这里介绍非常常用的 find_all()、find() 等方法，我们可以调用方法然后传入相应等参数就可以灵活地进行查询。 find_all( )查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，并使用一个列表进行返回。 12# 它的API 如下：find_all (name, attrs, recursicve, text, **kwargs) 我们接下来针对它的 API 下的属性进行介绍 ①name：根据标签名选择节点 方法选择器会返回一个Tag标签的类型，也就是说，其支持嵌套获取元素、层层迭代使用find_all等等操作。 当name=True时，匹配所有标签。 string 属性获取文本： 如果我们想拿到节点中的文本，该怎么做呢？ 使用列表切片方式取值，再调用.string属性，即可获取文本。 ② attrs / id ：根据属性值匹配 可以通过这些attrs、id 这两个关键字对我们需要的节点传入进行匹配 ③ text：用来匹配节点的文本，文本参数 text是用标签的文本内容去匹配，而不是用标签的属性。 传入的形式可以是文本字符串，也可以是正则表达式对象（compile方法生成的对象） 举例：text = “Hello，this is a link” 这样的形式进行匹配节点。 find( ) find() 方法返回的是单个元素，也就是第一个匹配的元素。类型依然是 Tag 类型。（当某个标签只有一个是使用find比较好，因为find返回一个Tag类型，利于我们后续操作。 而find_all返回一个列表，这是最大的区别了，有时候我们需要拿到Tag类型的节点，这样对我们的后续操作更为便利），跟find_all很相似，API都是相同. CSS选择器BeautifulSoup 还提供了另外一种选择器，那就是 CSS 选择器。如果学过前端，那么这个选择器简直是为你量身定做的。 我们只需要调用 select() 方法，传入相应的 CSS 选择器即可（个人比较喜欢这个选择器） CSS选择器是 BeautifulSoup 最大的特性，比较简便，且好理解 例如： id =“xx” 我们就可以写成 ‘#xx’，传给select 方法匹配。 获取文本经过节点选择，我们已经匹配到我们需要的节点了，接下来应该是获取文本内容了，通过Tag 类型的string 属性，我们可以拿到文本内容，还有一个方法那就是 get_text（），同样可以获取文本值。 只需基于 Tag类型，调用 get_text（） 方法即可。不过需要注意的是： .get_text（）会把你正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如你正在处理一个包含许多超链接、段落和标签的大段源代码，那么 .get_text() 会把这些超链接、段落和标签都清除掉，只剩下一串不带标签的文字。 所以，get_text（）方法一般留到节点匹配之后使用，不然会影响我们获取节点 获取属性 get 属性还是比较常用的，例如获取 a 标签的href 属性，我们就能得到链接了： get（”href“） 写在最后： 说一下节点选择的建议：页面布局总是不断变化的。一个标签这次是在表格中第一行的位置，没准儿哪天就在第二行或第三行了。如果想让你的爬虫更稳定，最好还是让标签的选择更加具体。如果有属性，就利用标签的属性。 解析库呢，就像一把锤子，使用的是我们，我们学会如何使用一把锤子的同时呢，也要关注什么时候去使用它，不要手上握着一把锤子，看任何问题都是钉子。使用适当的解析库去拿到我们需要的内容才是我们应该做的，使用什么样的锤子并不重要。 不经意间，爬虫专栏已经走到的Day6了，过阵子应该会更新 爬取网站的实战。还是那句话，学理论归理论，但只有实战才能使我们将其内化，成为我们真正的技能。]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day5-requests介绍]]></title>
    <url>%2F2019%2F07%2F19%2F%E7%88%AC%E8%99%ABDay5-requests%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Requests的基本使用我们知道在 Urllib 库中有 urlopen() 的方法，实际上它是以 GET 方式请求了一个网页。那么在 Requests 中，相应的方法就是 get() 方法，是不是很方便（使用什么请求方法就用对应的方法名封装的API） GET 请求下面是使用requests库发出的简单get请求，是不是相对于urllib，步骤变简单了很多呢？嘿嘿~ 响应内容的类型是type，我们可以基于这个对象，使用text方法获取文本，也可以调整编码格式等等。 响应状态码封装在status_code这个属性中，200表示正常访问。 模拟其他请求接下来，在这里分别用post()、put()、delete() 等方法实现了 POST、PUT、DELETE 等请求。requests这些方法时是直接使用的（不像urllib那么隐晦） 附加信息我们知道，使用百度这样的搜素引擎搜索关键字时，会在url生成类似这样的url，比如我们现在搜索的关键词是python，会生成下面这条url——“http://www.baidu.com/s?wd=python” 那么，在requests中，如何模拟这样的关键词查询呢？ 回答：将信息数据存储到字典中，然后利用params这个参数进行传递。 我们可以看到，字典的键值被传递到url中。 注意字典里值为None 的键都不会被添加到 URL 的查询字符串里。 定制请求头 注意：定制 header 的优先级低于某些特定的信息源，例如： 如果在 .netrc 中设置了用户认证信息，使用 headers= 设置的授权就不会生效。而如果设置了 auth= 参数，.netrc 的设置就无效了。 如果被重定向到别的主机，授权 header 就会被删除。 代理授权 header 会被 URL 中提供的代理身份覆盖掉。 在我们能判断内容长度的情况下，header 的 Content-Length 会被改写。 更进一步讲，Requests 不会基于定制 header 的具体情况改变自己的行为。只不过在最后的请求中，所有的 header 信息都会被传递进去。 二进制流数据上面例子中，实际上它返回的是一个 HTML 文档，那么如果我们想抓去图片、音频、视频等文件的话应该怎么办呢？我们都知道，图片、音频、视频这些文件都是本质上由二进制码组成的，由于有特定的保存格式和对应的解析方式，所以就需要拿到他们的二进制码。 需要记住的是：用什么保存格式编码就要用对应的解析方式解码 前两行便是 r.text 的结果，最后一行是 r.content 的结果。 前者出现了乱码，由于图片是二进制数据，所以前者在打印时转化为 str 类型（.text属性会猜测 其编码格式，然后自动给你转码），因此这里就是把图片直接转化为字符串，理所当然会出现乱码 而.content会到HTML纯文档中查找Meta标签下的编码，然后进行指定编码。可以发现，打印的结果前面带有一个 b，代表这是 bytes 类型的数据。 得出结论： r.test返回的是字符串类型，如果返回结果是文本文件，那么用这种方式直接获取其内容即可。 如果返回结果是图片、音频、视频等文件，应该使用 r.content，Requests 会为我们自动解码成 bytes 类型，即获取字节流数据。此时，我们就可以通过写入文件（注意编码格式是wb），将其保存为照片格式。 我建议使用 .content ， 然后将二进制数据 编码为 utf-8 关于编码：编码可谓是蛰伏在黑暗中的精灵，如果你不搞懂他，它就会偶尔出来耍一下脾气… Requests 会自动解码来自服务器的内容 。 大多数 unicode 字符集都能被无缝地解码 。 请求发出后 ， Requests 会基于 HTTP 头部对响应的编码作出有根据的推测 。 当你访问 r.text 时 ， Requests 会使用其推测的文本编码 。 不过你也可以找出 Requests 使 用 了 什 么 编 码 ， 并 且 能 够 使 用 r.encoding来更改编码方式。如果你改变了编码格式，每当你访问r.text，Request都会使用r.encoding的新值。 不过每一次设置r.encoding都需要去浏览器中查看相应字符集，这很麻烦。我们可以这样写： 1r.encoding = r.apprent_encoding # apprent_encoding是在响应报文的头部获取字符集，这样设置比较灵活 post 请求通常，你想要发送一些编码为表单形式的数据——非常像一个 HTML 表单。要实现这个，只需简单地传递一个字典（也可以是个元组）给 data 参数。你的数据字典在发出请求时会自动编码为表单形式 是不是觉得好简单？哈哈~ 这就是requests向我们提供的简单接口，然后我们直接拿来调用就行了！ Request的高级用法文件上传我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传。使用files关键字传递文件，就可以达到我们想要的效果。 content-type的值是multipart/from-data，表示是以表单数据提交的，证明请求方法为post 发送到网页，需要以字节流形式传输，所以我们打开文件时必须指定为“wb” 最后使用files关键字指定传入文件，文件需以字典形式传入（键名是什么不重要） 设置Cookies在前面我们使用了 Urllib 处理过 Cookies，写法比较复杂，而有了 Requests，获取和设置Cookies 就比较方便快捷了。而且requests有两种设置方法，我们一起来看看吧~ ① 在请求报文头中设置 ② 通过cookies参数进行传递 代理设置对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会提出登录验证、验证码，甚至直接把IP给封禁掉。（我就曾使用selenium对淘宝网进行爬取，结果没爬几页，发现被淘宝反爬虫给识别出来了，这就很尴尬了…） 那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在 Requests 中需要用到 proxies 这个参数。 超时设置在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能会收到一个响应，甚至到最后收不到响应而报错。为了防止服务器不能及时响应，我们应该设置一个超时时间，即超过了这个时间还没有得到响应，那就报错，或者谁用异常处理来处理这类事情。 设置超时时间需要用到 timeout 参数（默认是None）。这个时间的计算是发出 Request 到服务器返回Response 的时间。 上面就是requests的简单介绍了，是不是发现功能都被封装称为各个API中，我们只需要进行调用就好了，有没有像我当初一样，被requests这个库的魅力所感染到了呢？]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机基础知识扫盲篇]]></title>
    <url>%2F2019%2F07%2F16%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2%E7%AF%87%2F</url>
    <content type="text"><![CDATA[计算机组成基础谈谈汇编语言：机器语言——“计算机运行的基础” 机器语言是机器指令的集合，机器指令展开来讲就是一台机器可以正确执行的命令。电子计算机的机器指令是一列二进制数字。计算机将之转变为一列高低电平，以使计算机的电子器件受到驱动，进行运算。 汇编语言的诞生： 汇编语言是直接在硬件上工作的编程语言，用来替代机器语言麻烦的编程问题（难以辨别和记忆）。汇编语言的主题是汇编指令，汇编指令和机器指令的差别在于指令的表示方法上：汇编指令是机器指令便于记忆的书写格式 编译器： 计算机能读懂的只有机器指令，那么如何让计算机执行程序员用汇编语言编写的程序呢？ 这时，一种能够将汇编语言转成机器指令的翻译程序——编译器 产生了。 计算机组成的各个部分存储器：CPU是计算机的核心部件，它控制整个计算机的运作，要想让CPU工作，就必须向它提供指令和数据。指令和数据在存储器中存放，也就是平时说的内存。离开了内存，CPU也无法工作。（就像空有计算能力，却没有记忆力一般） 磁盘是计算机永久性存储数据的空间所在，磁盘上的数据要想被CPU执行调用，必须先读到内存当中。 指令和数据：指令和数据是应用上的概念，在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。 CPU在工作时，把有的信息看做指令，有的信息看做数据。（有特殊意义的二进制信息视为指令） 存储单元：存储器又被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127。 微型机存储器的每一个存储单元可以存储一个字节，即8个bit（电子计算机的最小信息单位）。 也就是说，一个存储器有128个存储单元，它可以存储128个字节。 微机存储器的容量是以字节为最小单位来计算的。 CPU对存储器的读写：存储器被划分成多个存储单元，存储单元从零开始顺序编号，这些编号可以看作存储单元在存储器中的地址，就像每个房子的门牌号 CPU要从内容中读取数据，首先要指定存储单元的地址，也就是说它要先确定读取哪一个存储单元中的数据。 另外，在微机中，不当当只有存储器这一种器件，CPU在进行读写数据操作时，需要指明它要对哪一个器件进行操作、进行哪种操作、是从中读出数据，还是写进数据。 可见，CPU要想进行数据的读写，必须和外部器件（可以说成芯片）进行三类信息的交互： 存储单元的地址（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据） 那么CPU是通过什么介质将这些信息传到存储器芯片中呢？ 电子计算机能处理、传输的信息都是电信号，电信号当然要用导线传送。在计算机中，专门连接CPU和其他芯片的导线，称为总线。 从物理上来讲，总线是一根根导线的集合，根据传送信息的不同，又从逻辑上分为三类、即地址总线、控制总线和数据总线。 总线 地址总线： CPU是通过地址总线来指定存储器单元的。 地址总线上能传送多少个不同的信息，CPU就可以对多少个存储单元进行寻址。 在电子计算机中，一根导线可以传送的稳定状态有两种，高电平或是低电平。用二进制表示就是1或者0。 有 n 根导线就能表示 2的n次方种信息。假设一个CPU有N跟地址线，则可以说这个CPU的地址总线的宽度为N，这样的CPU最多可以寻找2的N次方个内存单元。 数据总线： CPU与内存或其他器件之间的数据交互是通过数据总线进行传递的。 数据总线的宽度决定了CPU与外界的数据传输速度。8跟数据总线一次可传送一个8位二进制数据（即一个字节）。 控制总线： CPU对外部器件的控制是通过控制总线来进行的。 控制总线是一些不同控制线的集合，有多少根控制总线，就意味着CPU提供了对外部器件的多少种控制，所以，控制总线的宽度决定了CPU对外部器件的控制能力。 内存地址空间：举例来说，一个CPU的地址宽度为10，那么可寻址1024个内存单元，这1024个可寻址的内存单元就构成了这个CPU的内存地址空间。CPU在操纵和控制各类存储器时，都把他们当作内存来对待，把它们总的看做一个由若干存储单元组成的逻辑存储器，这个逻辑存储器就是我们所说的内存地址空间。 主板：每一台PC机中，都会有一个主板，主板上的器件通过总线相连。 这些器件有：CPU、内存、外围芯片组、扩展插槽等。扩展插槽一般插有RAM内存条和各类接口卡. 接口卡：计算机系统中，所有可用程序控制其工作的设备，都归CPU控制。 CPU对于外部设备如显示器、音箱等，是通过控制这些设备直接相连的接口卡，从而实现间接的控制这些外部设备的。简单来说，CPU通过总线向接口卡发送命令，接口卡根据CPU的命令控制外设进行工作。 各类存储器芯片：一台PC机中，装有多个存储器芯片。不同的器件，从读写属性上分为两大类： 随机存储器（RAM）和只读存储器（ROM） 随机存储器可读可写，但必须带电存储，关机后存储的内容丢失； 只读存储器只能读取不能写入，关机后内容也不会消失。 这些存储器从功能和连接上又可分为以下几类： 随机存储器：用于存放供CPU使用的绝大部分程序和数据，主随机存储器一般由两个位置上的RAM组成，装在主板上的RAM和插在插槽上的RAM 装有BIOS（基本输入输出系统）的ROM：BIOS是由主板和给某些接口卡（如：显卡、网卡等）厂商提供的软件系统。 接口卡上的RAM：某些接口卡需要大批量输入输出数据进行暂时存储，因此会在其上装有RAM。最为经典的是显示卡上的RAM，一般称为显存。我们将需要显示的内容写入显存，就会出现在显示器上。 **** 以上就是总结的部分计算机基础知识。我们下篇见，see you ~]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程新手的必修课]]></title>
    <url>%2F2019%2F07%2F16%2F%E7%BC%96%E7%A8%8B%E6%96%B0%E6%89%8B%E7%9A%84%E5%BF%85%E4%BF%AE%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[论一名计算机专业学生的修养从事计算机行业，私以为，是最为公平且投入风险小的一件事。我们听过不少这样的话——“只要你们用心学，不要放弃，就一定能学会计算机”。但我认为这句话是片面的，任何没有回报的努力，都是耍流氓。 刚开学，大家学习着学校安排的课程，抱着几本所谓计算机专业的教材一顿猛啃，你也许看懂了里面讲什么，但不明所以，不知道这些有什么用；有的学生学的一头雾水开始怀疑自己准备转专业放弃计算机；有的学生学懂写程序一直报错开始怀疑自己。 以上这几种情况，若没有领悟到学计算机的真正奥妙，最终都要凉凉。 为什么会这样呢？ 我们有没有反省过，究竟是环境问题，还是自驱动力的不足，亦是学习方法一开始就是错的？ 私以为，学计算机的成功离不开下面三点。 努力：计算机虽然是目前最火的专业，但若人人都想分一杯羹，不付出实际行动，一切都是空谈。 视野：视野决定高度，有句话说，站在风口上，猪都能飞来。 职业规划： 计算机虽然被统称为计算机专业，但其包含的方向数不胜数，方向选择是很重要的。拥有一份明确的职业规划，绝对让你披荆斩棘，一路高歌。 1接下来我们针对以上三点，进行描述 投入努力：在计算机专业，有一点是肯定的，那就是 “投入时间占比 = 回报率占比”。课堂所学，是计算机最基础的东西，就算你课堂上听懂了，全弄懂了，那些知识也无法支撑你拿到满意的工作。（这是真的！） 你之所以觉得听不懂，你要弄清楚是哪方面原因，当夜深人静的时候，好好想想，是不是真的对计算机没有兴趣。当遇到的BUG终于解决的时候，心情是激动的，还是仍然一副恼怒的样子。当DEBUG成功时，应该感到开心，这才是兴趣驱动学习。 这是一门全靠自学为主的学科，投入时间最为重要。 视野的打开网上资源一抓一大把，学会如何搜索，等于坐拥了这些资源。对于计算机专业的人来说，更是一笔最为宝贵的财富，编程新手的必修课，就是会用搜索引擎。 走进世界编程的大社区，例如Github、stackoverflow。哪怕什么都不懂，你也能保证所在的社区，是世界程序员的优秀社区，里面接触的东西，更是当下流行的。相信我，你的视野就会在此为起点，快速打开并不断增长，进入一个良性循环。 有多少学生，连stackoverflow、github是什么都不知道，如果我是老师，我教编程的第一节课绝对是教同学们如何搜索、如何利用网上资源、还有社区。 视野是很重要的一点，视野打不开，一身本领就会被扼杀在摇篮中。 职业规划：现互联网处于蓬勃发展的阶段，更是等着我们这一代人去带动他，我推测，互联网的红火还能至少维持二十年。且现在的红利虽不如几年前那么丰富，但现在也是不容小觑的。我们经常听到过，穷人家的孩子学计算机，富人家的孩子学金融就是这么来的。如果已经就读计算机专业，那么恭喜你，已经处于风口上，接下来就看你能不能把握住了。 这里引用一张知乎上k点赞的图片，里面所讲述的，描述了计算机行业中多个方向的选择，我们可以从中挑选出适合我们自己的。 四大理论最为重要，分别是： 数据结构与算法、操作系统、计算机组成原理、计算机网络。就算大学生涯什么都不会，这四大基础一定要学好。 写在最后到了大学，请不要再有“学生思维”，碰到问题时不要立马请教他人，解决问题的过程就是你进步的一种。而且这种解决问题的能力，更是会伴随我们整个职业发展。可以说，这种技能胜过于任何一个知识点，也是自学过程中必须掌握的。 上面这些，是我的这一年来的心得体会，希望能帮到有需要的人。]]></content>
      <categories>
        <category>蓝水星</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day4-urllib实战]]></title>
    <url>%2F2019%2F07%2F15%2F%E7%88%AC%E8%99%ABDay4-urllib%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[urllib爬虫实战之爬取笔趣阁小说前言上一节我们介绍了urllib这个基本库，它的内容还是蛮多的，要想熟练掌握爬虫，必须配合实战进行巩固。一开始小伙伴们可能会觉得无从下手，特别是解析响应内容这一步。这一次，作者将会从小白的角度，讲述如何对网页进行爬取、解析。 环境运行平台： WindowsPython版本： Python 3.xIDE： Pycharm BeautifulSoup简介 BeautifulSoup 就是 Python 的一个 HTML 或 XML 的解析库，我们可以用它来方便地从网页中提取数据。BeautifulSoup提供一些简单的、Python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据。 解析器 BeautifulSoup 在解析的时候实际上是依赖于解析器的，它除了支持 Python 标准库中的 HTML 解析器，还支持一些第三方的解析器比如 LXML 。 Beautiful方法有两个参数，第一个参数传入待解析的文本，第二个参数指定解析器，如果我们想要使用 LXML 这个解析器，在初始化 BeautifulSoup 的时候指定 lxml 即可，如下： 好了，简单介绍了BeautifulSoup，我们快点进入今天的实战环节吧~ 实战 背景介绍：笔趣看是一个盗版小说网站，这里有很多起点中文网的小说，该网站小说的更新速度稍滞后于起点中文网正版小说的更新速度。并且该网站只支持在线浏览，不支持小说打包下载。因此，本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说。 PS：本实例仅为交流学习，支持耳根大大，请上起点中文网订阅。 URL: ‘https://www.biqukan.com/1_1094/5403177.html&#39; 预备知识： 本文提及的代码并不复杂，如果对BeautifulSoup想要更加了解，可以去官方文档进行翻阅 URL：‘http://beautifulsoup.readthedocs.io/zh_CN/latest/‘ BeautifulSoup环境搭建： pip install bs4 导入使用 – from bs4 import BeautifulSoup 获取内容① 打开url链接，按F12或者右键- 检查 进入开发者工具 ② 在开发者工具中，捕获我们要找到的请求条目信息 将正文中部分内容进行复制 在开发者工具中，使用 ctrl + f 进入全局搜索框，将复制内容进行粘贴 然后会在下方得到条目信息，点击，页面会跳转到加载正文的请求响应条目中。 分析发现，正文部分存储在 id 为 content 和 class 为 showtxt 的 div 中。这就是我们解析网页得到的重要信息啦 ③ 构造url请求 有了上面的信息还不够，现在的网站都有了反爬能力，我们需要模拟一条正常从浏览器中发出的url请求链接。 例如： User-Agent（浏览器标识）、Cookies（标识客户端身份的“钥匙”）、referer等等字段 那么，这些包含在请求头的字段信息，我们应该如何构造这些字段信息呢? 还是开发者工具，点击Headers，就可以看到Request-Response条目明细。 ④ 发出请求： 有了字段的详细内容，我们就可以编写出请求网页的代码… 12345678import urllib.requestdef get_page(url): headers = &#123;'User-Agent':'Mozilla/5.0','Cookie':'UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333'&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() print(html) return html ⑤ 获得相应内容：运行，得到内容如下: 发现是一个二进制流的数据，因此得知，我们在解析时，需要编码 解析响应数据接下来，我们就可以使用BeautifulSoup进行解析 12345678910def parser_page(html): soup = BeautifulSoup(html,'lxml',from_encoding='utf-8') # 传入编码方式，为utf-8 text = soup.find('div', class_='showtxt') # 使用BeautifulSoup的find方法，匹配符合的节点，该节点为我们上文得知的属性 cont = text.get_text() # 输出文本内容 cont = str(cont).split() # 将空白符进行分割，这里的作用是消除 \xa0、\u3000空白符 print(cont) 运行….代码结果如图： 拿到小说数据！ 到这里，不少同学露出这样的微笑，哈哈哈 永久存储数据：我们这里使用最简单的方式，存储写入本地文件 打开result.txt文件，内容如下： 写在最后本次我们实战项目就告一段落啦，最后再为大家做一次梳理总结。 观察网页信息，获取相应内容 构造get请求，拿到响应内容 用解析库对相应内容进行解析 写入文件 全文代码如下： 12345678910111213141516171819202122232425262728import urllib.requestfrom bs4 import BeautifulSoupurl = 'https://www.biqukan.com/1_1094/5403177.html'def get_page(url): headers = &#123;'User-Agent':'Mozilla/5.0','Cookie':'UM_distinctid=16b8523768d317-020cad425c2672-e343166-144000-16b8523768e115; bcolor=; font=; size=; fontcolor=; width=; CNZZDATA1260938422=1245222118-1561302037-https%253A%252F%252Fblog.csdn.net%252F%7C1563150333'&#125; Request = urllib.request.Request(url,headers=headers) response = urllib.request.urlopen(Request) html = response.read() return htmldef parser_page(html): soup = BeautifulSoup(html,'lxml',from_encoding='utf-8') text = soup.find('div', class_='showtxt') cont = text.get_text() cont = str(cont).split() return contif __name__ == '__main__': html = get_page(url) results = parser_page(html) with open('result.txt','w',encoding='utf-8') as f : for result in results: f.write(result) f.write('\n') print("爬取完毕") 耗费一上午时间，终于把这篇文章给赶出来了~ 爬虫Day系列会持续更新，让我们共同期待吧！]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day3-urllib介绍]]></title>
    <url>%2F2019%2F07%2F10%2F%E7%88%AC%E8%99%ABDay3-urllib%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Urllib介绍在 Python2 版本中，有 Urllib 和 Urlib2 两个库可以用来实现Request的发送。而在 Python3 中，已经不存在Urllib2 这个库了，统一为 Urllib。 urllib分为四个模块： 第一个模块是request，它是最基本的HTTP请求模块，我们可以用它模拟发送请求，就像在浏览器手动输入网址然后敲击回车一样，只需要给库方法传入URL，以及额外的参数，就可以模拟实现整个过程了。 第二个模块是error，即异常处理，如果出现请求错误，我们可以捕获这个异常，然后进行异常的处理，保证程序的稳定性 第三个模块是paser，它是一个工具模块，提供了许多URL处理方法，比如拆分、解析、合并等操作 第四个模块是robotparser，主要是用来识别网站的robots.txt的（反爬虫的文本说明） 下面我就对以上模块进行重点讲解~ request模块1.urlopen方法： 我们从上面知道，request是HTTP请求模块，那么具体的请求方法是什么呢？没错，正是urlopen！ 12urlopen的API：urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) 我们通过urllib.request.urlopen（“url链接”）就可以请求到网页，如下图所示 这个请求的返回结果response，就是服务器响应内容。紧接着，我们可以调用这个对象包含的方法对返回的内容进行处理。 response.read（）读取响应内容，它是HTML纯文本，它以字节流的形式返回的，于是要解码成utf-8，得到字符串。 respon.status 返回网页的状态码，如果网页的状态码是200，则表示正常。 利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取。 2.data参数 data 参数是可选的，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。我们就可以使传递data参数，完成模拟登陆等类似功能… 具体如何模拟from表单，后文的urllib.parser会讲解 3. timeout参数 timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。 我们可以通过设置timeout值，从而缩短超时时间。或者是让程序不会一直处于阻塞状态（我们知道，客户端和服务器的交互是一个同步过程，我们可以利用timeout参数，以及异常处理来完成非阻塞的效果，以及让程序不会异常终止，让程序更加稳定） 4.构建Request对象 我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。比如可以构建请求链接的浏览器标识等等 下面是正常的urlopen请求和使用Request构建URL的代码举例： 可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，这样的好处是： 一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活（将参数全部加到Ruquest对象中作为一个整体发送） Request配置举例 之前我们说到过，可以构建url链接，比如关键字查询、模拟登陆。还可以模拟浏览器标识，让爬虫更像是一个人为发出的请求信息。具体内容如下： headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。 data就是我们发出的数据了，他会以表单形式向服务器发出 method是方法的意思，我们可以指定method来声明这是一个什么类型的请求方法。 Error模块前文我们了解了 Request 的发送过程，但是在网络情况不好、被反爬的情况下，出现了异常怎么办呢？这时如果我们不处理这些异常，程序很可能报错而终止运行，所以异常处理还是十分有必要的。 Urllib 的 error 模块定义了由request 模块产生的异常。如果出现了问题，request 模块便会抛出error 模块中定义的异常 1.URLError URLError 类来自 Urllib 库的 error 模块，它继承自 OSError 类，是 error 异常模块的基类，由 request 模块生的异常都可以通过捕获这个类来处理。它具有一个属性 reason，即返回错误的原因。 2.HTTPError 它是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败等等。 它有三个属性。 code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。 reason，同父类URLError一样，返回错误的原因。(有时候 reason 属性返回的不一定是字符串，也可能是一个对象—可以用 isinstance() 方法来判断它的类型，做出更详细的异常判断。) headers，返回 Request Headers。 因为 URLError是HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，这样捕获的代码更加精准，且有利于程序猿处理。 请求到响应信息不足为奇，也不是什么难事，最主要的，是让程序可以持久的”跑起来”，所以，异常处理是爬虫工程师的必修课，需好好掌握。 parse模块这是一个工具模块，用的地方挺多的，一些常用操作都封装在里面，如下面介绍的几个函数方法。 1.urlparse（） urlparse() 方法可以实现 URL 的识别和分段,返回六个部分的内容，字段分别是 scheme、netloc、path、params、query、fragment。 2.urlencode（） 常用的 urlencode() 方法，它在构造 GET 请求参数的时候非常有用——字典转序列化 我们知道，使用GET发送请求信息，它的参数是 写在URL中的，那么字典一样的数据（爬取数据大多数为字典）如何传递到URL中呢—— 正是上面讲的序列化了。 3.parse_qs（） 有了序列化必然就有反序列化，如果我们有一串 GET 请求参数，我们利用 parse_qs() 方法就可以将它转回字典 4.quote（） quote() 方法可以将内容转化为 URL 编码的格式，有时候 URL 中带有中文参数的时候可能导致乱码的问题，所以我们可以用这个方法将中文字符转化为 URL 编码 python3 默认编码格式是 Unicode，URL默认不接收这种编码格式，因此我们发送中文字符时，就应该使用相应编码。 5.unquote（） quote() 方法的逆操作，它可以进行 URL 解码（默认是以utf-8编码） robotparser模块在介绍rebotparser模块之前呢，我们需要对robots协议进行了解 Robots 协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下。 robotparse：用来解析rebots文件的类。 robotparser 模块提供了一个类，叫做 RobotFileParser。它可以根据某网站的 robots.txt 文件来判断一个爬取爬虫是否有权限来爬取这个网页。 123接口API：import urllib.robotparserurllib.robotparser.RobotFileParser(url='') 使用这个类的时候非常简单，只需要在构造方法里传入 robots.txt的链接即可。这样我们就拿到了一个 robot 对象，再调用can_fetch() 就可以返回是否可用爬取网页的布尔值了。 好了，对urllib库的介绍就到这儿了，下次作者会对第三方请求库requests讲解，这是一个当前的流行库，让我们一起期待吧~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day2_爬虫基础]]></title>
    <url>%2F2019%2F07%2F09%2F%E7%88%AC%E8%99%ABDay2-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[爬虫基础：简单来说，爬虫就是请求网页并提取数据和保存信息的自动化程序。 人们常常把爬虫比做蜘蛛爬网，把网的节点比做一个个网页，爬虫爬到这就相当于访问了该页面获取了其信息，节点间的连线可以比做网页与网页之间的链接关系，这样蜘蛛通过一个节点后可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页，这样整个网的节点便可以被蜘蛛全部爬行到，这样网站的数据就可以被抓取下来了。 简单流程介绍： 下面我们说说爬虫的具体流程，一般分为四步： 发起请求， 通过请求库对目标站点发起请求，我们可以模拟携带浏览器信息、cookie值、关键字查询等信息，从而访问服务器端，然后等待服务器的响应 解析内容： 服务器响应的内容有可能是HTML纯文本，也有可能是json格式的数据、或者是二进制（如图片视频），我们可以利用相关的解析库进行解析、保存或进一步处理。 处理信息：一般来说，我们使用解析库解析出来的内容，是冗杂晦涩的，我们通过用format或者切片等技术，使得内容清晰明了，这时，才算得上的有用的信息。 接下来，我们可以将信息存储到本地、数据库或者远程服务器等等。 爬虫优化：爬虫优化的方面有很多，例如，控制爬虫速率、使用高匿代理服务器、使用分布式爬取、处理异常… 接下来作者针对爬虫最重要的部分做出详细介绍： 获取网页爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。 在前面我们讲到了 Request 和 Response 的概念，我们向网站的服务器发送一个 Request，返回的 Response 的 Body 便是网页源代码。 所以最关键的部分就是构造一个 Request 并发送给服务器，然后接收到 Response 并将其解析出来。在python3 中，相关的库有：urllib（python自带的库）、requests（优雅的第三方库） 提取信息我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。 另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。 提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。 能抓取怎样的数据： 常规网页， 也就是html源代码。有些网页返回的不是不是 HTML 代码，而是返回一个 Json 字符串（使用Ajax渲染的网页），API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。 二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。 我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。 JavaScript 渲染页面：有时候我们在用 Urllib 或 Requests 抓取网页时，得到的源代码实际和浏览器中看到的是不一样的。 这个问题是一个非常常见的问题，现在网页越来越多地采用 Ajax、前端模块化工具来构建网页，整个网页可能都是由 JavaScript 渲染出来的，意思就是说原始的 HTML 代码就是一个空壳。 因此在用 Urllib 或 Requests 等库来请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中看到的内容了。 这也解释了为什么有时我们得到的源代码和浏览器中看到的是不一样的。 所以使用基本 HTTP 请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台的Ajax接口，通过模拟Ajax请求得到渲染的页面，或者也可以使用Selenium这样的库来实现模拟JavaScript。 本期对爬虫基础原理的讲述就到这里了~ 下期预告：“urllib请求库的使用”]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫Day1-HTML基础]]></title>
    <url>%2F2019%2F07%2F06%2F%E7%88%AC%E8%99%ABDay1-HTML%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[HTTP基础HTTP协议是Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于从万维网（WWW:World Wide Web ）服务器传输超文本到本地浏览器的传送协议。 HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。 不了解TCP/IP的童鞋，请穿越到这里：计算机网络Day1-TCP/IP协议 主要特点： 1、简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 2、灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 3.无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 4.无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。5、支持B/S及C/S模式。（俗称浏览器/服务器及客户端/服务端） HTTP之URLURL：用来传输数据和建立连接，URL全称为”统一资源定位符”。我们在网页上所干的事情，本质上就是找到特定的资源进行访问，然后获取我们需要的东西 通俗的讲，它是一个链接，它能使我们从互联网上找到我们访问的资源。 一个丰富详细的URL可以包含以下部分： 协议部分：该URL的协议部分为“http：”，这代表网页使用的是HTTP协议。在Internet中可以使用多种协议，如HTTP，FTP等等本例中使用的是HTTP协议。在”HTTP”后面的“//”为分隔符 域名部分：该URL的域名部分为 “www.4399.com”。一个URL中，也可以使用IP地址作为域名使用 端口部分：跟在域名后面的是端口，域名和端口之间使用“:”作为分隔符。端口不是一个URL必须的部分，如果省略端口部分，将采用默认端口 虚拟目录部分：从域名后的第一个“/”开始到最后一个“/”为止，是虚拟目录部分。虚拟目录也不是一个URL必须的部分。本例中的虚拟目录是“/flash/58508.html” 文件名部分：从域名后的最后一个“/”开始到“？”为止，是文件名部分，如果没有“?”,则是从域名后的最后一个“/”开始到“#”为止，是文件部分，如果没有“？”和“#”，那么从域名后的最后一个“/”开始到结束，都是文件名部分。本例中的文件名是“index.asp”。文件名部分也不是一个URL必须的部分，如果省略该部分，则使用默认的文件名 锚部分：从“#”开始到最后，都是锚部分。 参数部分：从“？”开始到“#”为止之间的部分为参数部分，又称搜索部分、查询部分。参数可以允许有多个参数，参数与参数之间用“&amp;”作为分隔符。 网页交互过程：我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。实际上这就是和服务器做交互行为， 通过发送请求，再到服务器响应服务，这就完成了我们获取资源的目的。 客户端请求–Request：Request，即请求，由客户端向服务端发出。可以将Request分为四部分： 请求方法（Request Method） 请求链接（Request URL） 请求头（Request Headers） 请求体（Request Body） 如图： Request Method请求方式，请求方式常见的有两种类型，GET 和 POST。 Get： 我们在浏览器中直接输入一个URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里（GET 方式请求提交的数据最多只有 1024 字节） Post： Post 请求大多为表单提交发起，如一个登录表单，输入用户名密码，点击登录按钮，这通常会发起一个 POST 请求，其数据通常以 Form Data 即表单的形式传输，不会体现在 URL 中。（POST 方式请求提交的数据没有限制） 而且Post请求在Request Headers中的Content-Type标识——只有设置为application/x-www-form-urlencoded 才是Post请求。 其他请求方式 我们平常遇到的绝大部分请求都是GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下： Request Headers：请求头，用来说明客户端要使用的附加信息，比较重要的字段有：Cookie，Referer、User-Agent 以及 Content-Type等等。 Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。 因为python爬虫脚本，默认的请求头参数会表明 这是由 python 发出的请求，而不是浏览器。（服务器一看，这不是明着来搞我吗，一条ACL写下去，请求头是脚本发起的请求就访问不了我了） Request Body：请求体，一般承载的内容是Post请求的Form Data，即表单数据，其一般承载着发送往服务器的信息。同时，服务端需要支持post请求方法，才能够接收我们发送的数据，同时，它会对着请求数据进行回复。 例如注册，我们把账号密码在表单中填充完整，然后点击注册，这时候就已经发送数据往服务器了，服务器验证规则成功后，会在数据库记录，然后我们就可以登录了。 而对于Get请求中，Request Body的内容为空。 所有信息存储在 URL中。 服务器响应–ResponseResponse，即响应，由服务端返回给客户端。 Response 可以划分为三部分： Response Status Code Response Headers Response Body Response Status Code：响应状态码，此状态码表示了服务器的响应状态，如 200则代表服务器正常响应，404 则代表页面未找到，500则代表服务器内部发生错误。 状态代码有三位数字组成，第一个数字定义了响应的类别，共分五种类别： 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 Response Headers：响应头，其中包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等，下面对一些常用的头信息说明： Date：日期，标识 Response 产生的时间。 Last-Modified，指定资源的最后修改时间。 Content-Encoding，指定 Response 内容的编码。 Server，包含了服务器的信息，名称，版本号等。 Content-Type，文档类型，指定了返回的数据类型是什么，如text/html 则代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回了图片。 Set-Cookie，设置Cookie，Response Headers 中的 Set-Cookie即告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。 Response Body：响应体，响应的正文数据都存放于此，如果请求一个网页，它的响应体就是网页的HTML代码，或者是Json格式的数据；请求一张图片，它的响应体就是图片的二进制数据。 爬虫的解析都是根据响应体进行操作的。 HTTP工作原理HTTP协议定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。HTTP协议采用了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。 在浏览器地址栏键入URL，按下回车之后会经历以下流程： 在浏览器键入URL后，向DNS服务器发出地址解析，拿到对应IP 检查是否携带端口号，如果有则携带端口号参数进行访问；如果没有，则添加上默认端口号进行访问，发起TCP请求连接。 建立连接后，浏览器根据文件（URL中表明的资源）发出HTTP请求，该请求报文作为TCP 三次握手的第三个报文的数据发送给服务器; 服务器对浏览器请求进行响应，返回HTML格式的代码 浏览器断开TCP连接，根据HTML代码进行渲染并返回。 好了，以上就是简单的讲解了HTTP基础内容，下期我会讲解爬虫的原理基础，以及实现过程。Bye~]]></content>
      <categories>
        <category>Python爬虫</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
